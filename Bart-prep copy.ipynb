{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import glob\n",
    "import random\n",
    "import cutlet\n",
    "import jiwer\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from convert_romaji import Romaji2Kana\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sacrebleu.metrics import BLEU, CHRF, TER\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "from transformers import (\n",
    "    T5Config,\n",
    "    T5Tokenizer,\n",
    "    TFT5ForConditionalGeneration,\n",
    "    GradientAccumulator,\n",
    "    logging)\n",
    "\n",
    "# policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "# tf.keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "def seed_everything(SEED):\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    print(\"Random seed set.\")\n",
    "\n",
    "seed_everything(42)\n",
    "tf.get_logger().setLevel('FATAL')\n",
    "logging.set_verbosity_error()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# katsu = cutlet.Cutlet()\n",
    "# katsu.use_foreign_spelling = False\n",
    "\n",
    "# def clean_kanji(sentence):\n",
    "#     symbols = r\"\\（.*\\）|\\(.*\\)|\\「.*\\」|\\『.*\\』\"\n",
    "#     sentence = re.sub(symbols, \"\", sentence.strip())\n",
    "#     return sentence\n",
    "\n",
    "# def clean_romaji(sentence):\n",
    "#     sentence = sentence.strip().lower()\n",
    "#     sentence = re.sub(r\"[^a-zA-Z0-9\\ ]\", \"\", sentence)\n",
    "#     sentence = sentence.split()\n",
    "#     for i, mora in enumerate(sentence):\n",
    "#         if (mora == \"n\") | (mora == \"u\") & (i < len(sentence) - 1):\n",
    "#             prev_mora = sentence.pop(i-1)\n",
    "#             sentence[i-1] = \"\".join([prev_mora, mora])\n",
    "#     sentence = \" \".join(sentence)\n",
    "#     return sentence\n",
    "\n",
    "# def kanji2romaji(text):\n",
    "#     try:\n",
    "#         new_line = katsu.romaji(text)\n",
    "#         new_line = clean_romaji(new_line)\n",
    "#     except:\n",
    "#         new_line = None\n",
    "#     return new_line\n",
    "\n",
    "# def clean_en(sentence):\n",
    "#     sentence = re.sub(r\"\\(.*\\)|\\[.*\\]|\\{.*\\}\", \"\", sentence.strip().lower()) # Parenthesis\n",
    "#     sentence = re.sub(r\"\\.{3,}\", \",\", sentence) # Ellipsis\n",
    "#     sentence = re.sub(r\"[^A-Za-z0-9\\ \\?\\.\\!\\,\\'\\\"]\", \"\", sentence) # Non alphanumeric\n",
    "#     return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_dir = \"D:\\School-stuff\\Sem-2\\PR-Project\\HoloASR\\Datasets\"\n",
    "# opus_ja_paths = glob.glob(f\"{main_dir}\\OPUS100-dataset\\*.ja\")\n",
    "# tatoeba_ja_paths = glob.glob(f\"{main_dir}\\Tatoeba-dataset\\*.ja\")\n",
    "\n",
    "# ja_paths = opus_ja_paths + tatoeba_ja_paths + coursera_ja_paths\n",
    "\n",
    "# ja_lines, en_lines = [], []\n",
    "# for ja_path in ja_paths:\n",
    "#     if ja_path.endswith(\".ja\"):\n",
    "#         en_path = ja_path.rsplit(\".\", 1)[0] + \".en\"\n",
    "#     else:\n",
    "#         en_path = ja_path.replace(\"ja\", \"en\")\n",
    "#     with open(ja_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         lines = [line.strip(\"\\n\") for line in f.readlines()]\n",
    "#         ja_lines.extend(lines)\n",
    "#     with open(en_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         lines = [line.strip(\"\\n\") for line in f.readlines()]\n",
    "#         en_lines.extend(lines)\n",
    "\n",
    "# tqdm.pandas()\n",
    "# data = pd.DataFrame({'ja_raw': ja_lines, 'en': en_lines})\n",
    "# data['ja_raw'] = data['ja_raw'].progress_apply(clean_kanji)\n",
    "# data['ja_ro'] = data['ja_raw'].progress_apply(kanji2romaji)\n",
    "# data = data.dropna().reset_index(drop=True)\n",
    "# data['ja_hira'] = data['ja_ro'].progress_apply(Romaji2Kana)\n",
    "# data['en'] = data['en'].progress_apply(clean_en)\n",
    "# data = data.dropna().reset_index(drop=True)\n",
    "# data.to_csv(\n",
    "#     r\"E:\\Datasets\\Language_model\\tokenizer_text.csv\",\n",
    "#     index=False, encoding=\"utf-8\")\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(r\"E:\\Datasets\\Language_model\\tokenizer_text.csv\")\n",
    "# data = data.dropna().reset_index(drop=True)[['en', 'ja_hira']]\n",
    "\n",
    "# with open(r\"E:\\Datasets\\Language_model\\tokenizer_text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for row in tqdm(data.iterrows(), total=len(data)):\n",
    "#         idx, (en, ja) = row\n",
    "#         row =  \" \".join([en, ja]) + \"\\n\"\n",
    "#         f.write(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_file = r\"E:\\Datasets\\Language_model\\tokenizer_text.txt\"\n",
    "# model_prefix = r\"E:\\Datasets\\Language_model\\t5_spm\"\n",
    "\n",
    "# spm.SentencePieceTrainer.train(\n",
    "#     f\"--input={text_file} --model_prefix={model_prefix} --vocab_size={32128} --model_type=unigram --pad_id=0 --unk_id=2 --bos_id=-1 --eos_id=1 --pad_piece=<pad> --unk_piece=<unk> --eos_piece=</s>\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFRWriter():\n",
    "    def __init__(self):\n",
    "        self.main_dir = \"E://Datasets/Language_model\"\n",
    "        self.n_shards = 10\n",
    "        self.tokenizer = T5Tokenizer(\n",
    "            vocab_file=f\"{self.main_dir}/t5_spm.model\",\n",
    "            eos_token=\"</s>\",\n",
    "            unk_token=\"<unk>\",\n",
    "            pad_token=\"<pad>\")\n",
    "        self.data = self.get_data()\n",
    "\n",
    "    def get_data(self):\n",
    "        tqdm.pandas()\n",
    "        data = pd.read_csv(\n",
    "            f\"{self.main_dir}/tokenizer_text.csv\", \n",
    "            encoding=\"utf-8\")\n",
    "        data = data.dropna().reset_index(drop=True)[['en', 'ja_hira']]\n",
    "        data['ja_token'] = data['ja_hira'].progress_apply(\n",
    "            lambda x: self.tokenizer(x).input_ids)\n",
    "        data['en_token'] = data['en'].progress_apply(\n",
    "            lambda x: self.tokenizer(x).input_ids)\n",
    "        data['ja_len'] = data['ja_token'].apply(len)\n",
    "        data = data.query(f\"ja_len >= 4 & ja_len <= 21\")\n",
    "        data = data.sort_values(by=\"ja_len\", ignore_index=True, ascending=True)\n",
    "        data.to_csv(\n",
    "            f\"{self.main_dir}/tokenizer_text2.csv\",\n",
    "            index=False)\n",
    "        return data\n",
    "\n",
    "    def _bytes_feature(self, value):\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    def serialize_example(self, *args):\n",
    "        feature = {\n",
    "            'input_ids': self._bytes_feature(args[0]),\n",
    "            'attention_mask': self._bytes_feature(args[1]),\n",
    "            'labels': self._bytes_feature(args[2])}\n",
    "\n",
    "        example_proto = tf.train.Example(\n",
    "            features=tf.train.Features(feature=feature))\n",
    "        return example_proto.SerializeToString()\n",
    "\n",
    "    def get_shards(self):\n",
    "        skf = KFold(n_splits=self.n_shards, shuffle=False)\n",
    "        return [j for i,j in skf.split(self.data)]\n",
    "\n",
    "    def get_shard_data(self, samples):\n",
    "        for sample in samples:\n",
    "            input_ids = tf.convert_to_tensor(\n",
    "                self.data['ja_token'][sample], dtype=tf.int32)\n",
    "            attention_mask = tf.where(input_ids != 0, x=1, y=0)\n",
    "            labels = tf.convert_to_tensor(\n",
    "                self.data['en_token'][sample], dtype=tf.int32)\n",
    "            yield {\n",
    "                \"input_ids\": tf.io.serialize_tensor(input_ids),\n",
    "                \"attention_mask\": tf.io.serialize_tensor(attention_mask),\n",
    "                \"labels\": tf.io.serialize_tensor(labels)}\n",
    "\n",
    "    def write(self):\n",
    "        for shard, samples in tqdm(enumerate(self.get_shards()), total=self.n_shards):\n",
    "            with tf.io.TFRecordWriter(f\"{self.main_dir}/t5_tfrec/shard_{shard+1}.tfrec\") as f:\n",
    "                for sample in self.get_shard_data(samples):\n",
    "                    example = self.serialize_example(\n",
    "                        sample['input_ids'],\n",
    "                        sample['attention_mask'],\n",
    "                        sample['labels'])\n",
    "                    f.write(example)\n",
    "\n",
    "# TFRWriter().write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(accum_steps=1, batch_size=32, buffer_size=512, epochs=15, learning_rate=0.0003, lr_max=0.0003, lr_min=1e-08, lr_start=1e-08, main_dir='E://Datasets/Language_model', n_cycles=0.5, n_samples=1034384, n_shards=10, n_train=930945, n_val=103438, random_state=42, sustain_epochs=2, test_size=0.1, train_steps=29093, val_steps=3233, warmup_epochs=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ArgParser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--random_state\", default=42)\n",
    "    parser.add_argument(\"--main_dir\", default=\"E://Datasets/Language_model\")\n",
    "    parser.add_argument(\"--n_shards\", default=10)\n",
    "    parser.add_argument(\"--test_size\", default=0.1)\n",
    "    parser.add_argument(\"--batch_size\", default=32)\n",
    "    parser.add_argument(\"--buffer_size\", default=512)\n",
    "\n",
    "    # Trainer\n",
    "    parser.add_argument(\"--accum_steps\", default=1)\n",
    "\n",
    "    # Scheduler\n",
    "    parser.add_argument(\"--epochs\", default=15)\n",
    "    parser.add_argument(\"--learning_rate\", default=3e-4)\n",
    "    parser.add_argument(\"--lr_start\", default=1e-8)\n",
    "    parser.add_argument(\"--lr_min\", default=1e-8)\n",
    "    parser.add_argument(\"--lr_max\", default=3e-4)\n",
    "    parser.add_argument(\"--n_cycles\", default=0.5)\n",
    "    parser.add_argument(\"--warmup_epochs\", default=0)\n",
    "    parser.add_argument(\"--sustain_epochs\", default=2)\n",
    "\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    n_samples = len(pd.read_csv(\n",
    "        f\"{args.main_dir}/tokenizer_text2.csv\"))   \n",
    "\n",
    "    n_train = int(n_samples * (1 - args.test_size))\n",
    "    n_val = int(n_samples * args.test_size)\n",
    "    train_steps = int(np.ceil(n_train / args.batch_size))\n",
    "    val_steps = int(np.ceil(n_val / args.batch_size))\n",
    "        \n",
    "    parser.add_argument(\"--n_samples\", default=n_samples)\n",
    "    parser.add_argument(\"--n_train\", default=n_train)\n",
    "    parser.add_argument(\"--n_val\", default=n_val)\n",
    "    parser.add_argument(\"--train_steps\", default=train_steps)  \n",
    "    parser.add_argument(\"--val_steps\", default=val_steps)\n",
    "\n",
    "    return parser.parse_known_args()[0]\n",
    "\n",
    "args = ArgParser()\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, args):\n",
    "        self.files = glob.glob(args.main_dir + \"/t5_tfrec/*.tfrec\")\n",
    "        self.args = args\n",
    "        self.AUTOTUNE = tf.data.AUTOTUNE\n",
    "        self.train_files, self.val_files = train_test_split(\n",
    "            self.files, test_size=args.test_size, shuffle=True, \n",
    "            random_state=args.random_state)\n",
    "        self.train = self.get_train()\n",
    "        self.val = self.get_val()\n",
    "\n",
    "    def read_tfrecord(self, example):\n",
    "        feature_description = {\n",
    "            'input_ids': tf.io.FixedLenFeature([], tf.string),\n",
    "            'attention_mask': tf.io.FixedLenFeature([], tf.string),\n",
    "            'labels': tf.io.FixedLenFeature([], tf.string)\n",
    "            }\n",
    "        \n",
    "        example = tf.io.parse_single_example(example, feature_description)\n",
    "        example['input_ids'] = tf.io.parse_tensor(\n",
    "            example['input_ids'], out_type=tf.int32)\n",
    "        example['attention_mask'] = tf.io.parse_tensor(\n",
    "            example['attention_mask'], out_type=tf.int32) \n",
    "        example['labels'] = tf.io.parse_tensor(\n",
    "            example['labels'], out_type=tf.int32)\n",
    "        return example\n",
    "\n",
    "    def load_dataset(self, files):\n",
    "        ignore_order = tf.data.Options()\n",
    "        ignore_order.experimental_deterministic = False\n",
    "        dataset = tf.data.TFRecordDataset(files)\n",
    "        dataset = dataset.with_options(ignore_order)\n",
    "        dataset = dataset.map(self.read_tfrecord, num_parallel_calls=self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_train(self):\n",
    "        dataset = self.load_dataset(self.train_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_ids': [None],\n",
    "                'attention_mask': [None],\n",
    "                'labels': [None]\n",
    "            },\n",
    "            padding_values={\n",
    "                'input_ids': tf.constant(0, dtype=tf.int32),\n",
    "                'attention_mask': tf.constant(0, dtype=tf.int32),\n",
    "                'labels': tf.constant(-100, dtype=tf.int32)\n",
    "            })        \n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_val(self):\n",
    "        dataset = self.load_dataset(self.val_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_ids': [None],\n",
    "                'attention_mask': [None],\n",
    "                'labels': [None]\n",
    "            },\n",
    "            padding_values={\n",
    "                'input_ids': tf.constant(0, dtype=tf.int32),\n",
    "                'attention_mask': tf.constant(0, dtype=tf.int32),\n",
    "                'labels': tf.constant(-100, dtype=tf.int32)\n",
    "            })\n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "# train = DataLoader(args).train\n",
    "\n",
    "# inputs = next(iter(train))\n",
    "# input_values = inputs['input_ids']\n",
    "# labels = inputs['labels']\n",
    "# attention_mask = inputs['attention_mask']\n",
    "# print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class BLEUMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"BLEU\", **kwargs):\n",
    "        super(BLEUMetric, self).__init__(name=name, **kwargs)\n",
    "        self.bleu = BLEU()\n",
    "        self.accumulator = self.add_weight(name=\"total_bleu\", initializer=\"zeros\")\n",
    "        self.counter = self.add_weight(name=\"counter\", initializer=\"zeros\")\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        try:\n",
    "            bleu_score = self.bleu.corpus_score(hypotheses=y_true, references=y_pred).score\n",
    "        except:\n",
    "            y_pred = [\".\" if x == \"\" else x for x in y_pred]\n",
    "            bleu_score = self.bleu.corpus_score(hypotheses=y_true, references=y_pred).score\n",
    "        self.accumulator.assign_add(bleu_score)\n",
    "        self.counter.assign_add(1)\n",
    "\n",
    "    def result(self):\n",
    "        return tf.math.divide_no_nan(self.accumulator, self.counter)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.accumulator.assign(0.0)\n",
    "        self.counter.assign(0.0)\n",
    "\n",
    "class WERMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"WER\", **kwargs):\n",
    "        super(WERMetric, self).__init__(name=name,  **kwargs)\n",
    "        self.accumulator = self.add_weight(name=\"total_wer\", initializer=\"zeros\")\n",
    "        self.counter = self.add_weight(name=\"wer_count\", initializer=\"zeros\")    \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        wer = jiwer.wer(y_true, y_pred)\n",
    "\n",
    "        # Add distance and number of batches to variables\n",
    "        self.accumulator.assign_add(wer)\n",
    "        self.counter.assign_add(1)\n",
    "\n",
    "    def result(self):\n",
    "        # Divides accumulated distance scores against number of batches passed,\n",
    "        # mimics mean reduction over batch\n",
    "        return tf.math.divide_no_nan(self.accumulator, self.counter)   \n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.accumulator.assign(0.0)\n",
    "        self.counter.assign(0.0)\n",
    "\n",
    "class CosineDecayWithWarmup(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "    def __call__(self, epoch):  \n",
    "        if epoch < self.args.warmup_epochs:\n",
    "            lr = ((self.args.lr_max - self.args.lr_start) / self.args.warmup_epochs) * epoch + self.args.lr_start\n",
    "        elif epoch < (self.args.warmup_epochs + self.args.sustain_epochs):\n",
    "            lr = self.args.lr_max\n",
    "        else:\n",
    "            progress = ((epoch - self.args.warmup_epochs - self.args.sustain_epochs) / \n",
    "            (self.args.epochs - self.args.warmup_epochs - self.args.sustain_epochs))\n",
    "            lr = (self.args.lr_max-self.args.lr_min) * (0.5 * (1.0 + tf.math.cos((22/7) * \n",
    "                self.args.n_cycles * 2.0 * progress)))\n",
    "            if self.args.lr_min is not None:\n",
    "                lr = tf.math.maximum(self.args.lr_min, lr)\n",
    "        return lr\n",
    "\n",
    "    def plot(self):\n",
    "        epochs = range(self.args.epochs+1)\n",
    "        lr = [self(epoch) for epoch in epochs]\n",
    "        plt.plot(epochs, lr)\n",
    "        plt.xlabel(\"learning_rate\")\n",
    "        plt.ylabel(\"epochs\")\n",
    "        plt.show()\n",
    "\n",
    "CosineDecayWithWarmup(args).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from epoch 1...\n",
      "Epoch 1/15: Learning rate @ 3.00e-04\n",
      "  823/29093 [..............................] - ETA: 16:14:44 - loss: 6.7312 - bleu: 0.3136 - wer: 1.0017"
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.tokenizer = T5Tokenizer(\n",
    "            vocab_file=f\"{self.args.main_dir}/t5_spm.model\",\n",
    "            extra_ids=0,\n",
    "            eos_token=\"</s>\",\n",
    "            unk_token=\"<unk>\",\n",
    "            pad_token=\"<pad>\")\n",
    "        self.train_dataset = DataLoader(args).train\n",
    "        self.val_dataset = DataLoader(args).val\n",
    "        schedule = CosineDecayWithWarmup(args)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(schedule)\n",
    "        self.bleu_metric = BLEUMetric()\n",
    "        self.wer_metric = WERMetric()\n",
    "        self.gradient_accumulator = GradientAccumulator()\n",
    "        self.gradient_accumulator.accum_steps = args.accum_steps\n",
    "        \n",
    "        # self.model = TFT5ForConditionalGeneration.from_pretrained(\n",
    "        #     \"t5-small\",\n",
    "        #     pad_token_id=0,\n",
    "        #     eos_token_id=1,\n",
    "        #     vocab_size=len(self.tokenizer),\n",
    "        #     decoder_start_token_id=0,\n",
    "        #     output_hidden_states=False,\n",
    "        #     output_attentions=False,\n",
    "        #     use_cache=False)\n",
    "        \n",
    "        self.model = TFT5ForConditionalGeneration(config=T5Config(\n",
    "            pad_token_id=0,\n",
    "            eos_token_id=1,\n",
    "            decoder_start_token_id=0,\n",
    "            vocab_size=len(self.tokenizer),\n",
    "            output_hidden_states=False,\n",
    "            output_attentions=False,\n",
    "            use_cache=False\n",
    "        ))\n",
    "\n",
    "        self.model_name = f\"model_{int(self.args.n_samples/1000)}k\"\n",
    "        self.log_path = f\"{self.args.main_dir}/model_weights/{self.model_name}.csv\"\n",
    "        if not os.path.exists(self.log_path):\n",
    "            print(\"Log file created.\")\n",
    "            columns = \"epoch,loss,bleu,wer,val_loss,val_bleu,val_wer\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(columns)\n",
    "\n",
    "    def decoder(self, labels, logits):\n",
    "        labels = tf.where(labels < 0, x=0, y=labels)\n",
    "        labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)  \n",
    "        logits = tf.argmax(logits, axis=-1)\n",
    "        logits = self.tokenizer.batch_decode(logits, skip_special_tokens=True)\n",
    "        return labels, logits\n",
    "\n",
    "    def display(self, epoch, t_labels, t_logits, v_labels, v_logits):\n",
    "        print(\"-\" * 129)\n",
    "        print(\"Training\")\n",
    "        for y_true, y_pred in zip(t_labels, t_logits):\n",
    "            print(f\"Target:    {y_true}\")\n",
    "            print(f\"Predicted: {y_pred}\") \n",
    "\n",
    "        print(\"\\nValidation\")\n",
    "        for y_true, y_pred in zip(v_labels, v_logits):\n",
    "            print(f\"Target:    {y_true}\")\n",
    "            print(f\"Predicted: {y_pred}\")\n",
    "        print(\"-\" * 129)\n",
    "        \n",
    "    def fit(self):\n",
    "        # Checkpointing\n",
    "        self.ckpt_dir = f\"{self.args.main_dir}/checkpoints\"\n",
    "        self.ckpt = tf.train.Checkpoint(self.model)\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(\n",
    "            checkpoint=self.ckpt, directory=self.ckpt_dir, max_to_keep=5)\n",
    "\n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            self.start_epoch = int(self.ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "            self.ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "            print(f\"Resuming from epoch {self.start_epoch + 1}...\")\n",
    "        else:\n",
    "            self.start_epoch = 0\n",
    "            print(\"Starting from epoch 1...\")\n",
    "\n",
    "        for epoch in range(self.args.epochs):\n",
    "            print(f\"Epoch {epoch+1}/{self.args.epochs}: Learning rate @ {self.optimizer.lr(epoch):.2e}\")\n",
    "            stateful_metrics = [\"loss\", \"bleu\", \"wer\", \"val_loss\", \"val_bleu\", \"val_wer\"]\n",
    "            progbar = tf.keras.utils.Progbar(\n",
    "                self.args.train_steps, interval=0.05,\n",
    "                stateful_metrics=stateful_metrics)\n",
    "\n",
    "            # Training loop\n",
    "            for step, t_batch in enumerate(self.train_dataset):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    t_loss, t_logits = self.model(\n",
    "                        input_ids=t_batch['input_ids'],\n",
    "                        attention_mask=t_batch['attention_mask'],\n",
    "                        labels=t_batch['labels'],\n",
    "                        training=True)[:2]              \n",
    "                self.gradient_accumulator(tape.gradient(t_loss, self.model.trainable_weights))\n",
    "                self.optimizer.apply_gradients(zip(\n",
    "                    self.gradient_accumulator.gradients, \n",
    "                    self.model.trainable_weights))\n",
    "                t_labels, t_logits = self.decoder(t_batch['labels'], t_logits)\n",
    "                self.bleu_metric.update_state(t_labels, t_logits)\n",
    "                self.wer_metric.update_state(t_labels, t_logits)\n",
    "                t_bleu = self.bleu_metric.result()\n",
    "                t_wer = self.wer_metric.result()\n",
    "                t_values = [\n",
    "                    (\"loss\", tf.reduce_mean(t_loss)),\n",
    "                    (\"bleu\", t_bleu),\n",
    "                    (\"wer\", t_wer)]\n",
    "                progbar.update(step, values=t_values, finalize=False)\n",
    "            self.bleu_metric.reset_states()\n",
    "            self.wer_metric.reset_states()\n",
    "\n",
    "            # Validation loop\n",
    "            for v_batch in self.val_dataset:\n",
    "                v_loss, v_logits = self.model(\n",
    "                    input_ids=v_batch['input_ids'],\n",
    "                    attention_mask=v_batch['attention_mask'],\n",
    "                    labels=v_batch['labels'],\n",
    "                    training=False)[:2]\n",
    "                v_labels, v_logits = self.decoder(v_batch['labels'], v_logits)\n",
    "                self.bleu_metric.update_state(v_labels, v_logits)\n",
    "                self.wer_metric.update_state(v_labels, v_logits)\n",
    "            \n",
    "            v_bleu = self.bleu_metric.result()\n",
    "            v_wer = self.wer_metric.result()\n",
    "            v_values = [\n",
    "                (\"loss\", tf.reduce_mean(t_loss)), \n",
    "                (\"bleu\", t_bleu),\n",
    "                (\"wer\", t_wer),\n",
    "                (\"val_loss\", tf.reduce_mean(v_loss)),\n",
    "                (\"val_bleu\", v_bleu),\n",
    "                (\"val_wer\", v_wer)]\n",
    "            progbar.update(self.args.train_steps, values=v_values, finalize=True)\n",
    "            self.bleu_metric.reset_states()\n",
    "            self.wer_metric.reset_states()\n",
    "\n",
    "            # Print sample transcriptions for both loops\n",
    "            self.display(epoch, t_labels, t_logits, v_labels, v_logits)\n",
    "\n",
    "            # Checkpointing\n",
    "            self.ckpt.save(file_prefix=f\"{self.ckpt_dir}/{self.model_name}\")\n",
    "\n",
    "            # Logging\n",
    "            log = f\"{epoch+1},{t_loss},{t_bleu},{t_wer},{v_loss},{v_bleu},{v_wer}\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(log)\n",
    "\n",
    "            save_path = f\"{self.args.main_dir}/model_weights\"\n",
    "            self.model.save_weights(f\"{save_path}/{self.model_name}_{epoch+1}.h5\")\n",
    "\n",
    "Trainer(args).fit()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b851d2923cfa3a2562599062e05fd9893d86a7c009c64d8ad3756552e4dd5f41"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('tf-gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
