{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import cutlet\n",
    "import jiwer\n",
    "import argparse\n",
    "import MeCab\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from convert_romaji import Romaji2Kana\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sacrebleu.metrics import BLEU, CHRF\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "from transformers import (\n",
    "    MarianConfig,\n",
    "    MarianTokenizer,\n",
    "    TFMarianMTModel,\n",
    "    GradientAccumulator,\n",
    "    logging)\n",
    "\n",
    "# policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "# tf.keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "def seed_everything(SEED):\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    print(\"Random seed set.\")\n",
    "\n",
    "seed_everything(42)\n",
    "tf.get_logger().setLevel('FATAL')\n",
    "logging.set_verbosity_error()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# katsu = cutlet.Cutlet()\n",
    "# katsu.use_foreign_spelling = False\n",
    "\n",
    "# def clean_kanji(sentence):\n",
    "#     symbols = r\"\\（.*\\）|\\(.*\\)|\\「.*\\」|\\『.*\\』\"\n",
    "#     sentence = re.sub(symbols, \"\", sentence.strip())\n",
    "#     return sentence\n",
    "\n",
    "# def clean_romaji(sentence):\n",
    "#     sentence = sentence.strip().lower()\n",
    "#     sentence = re.sub(r\"[^a-zA-Z0-9\\ ]\", \"\", sentence)\n",
    "#     sentence = sentence.split()\n",
    "#     for i, mora in enumerate(sentence):\n",
    "#         if (mora == \"n\") | (mora == \"u\") & (i < len(sentence) - 1):\n",
    "#             prev_mora = sentence.pop(i-1)\n",
    "#             sentence[i-1] = \"\".join([prev_mora, mora])\n",
    "#     sentence = \" \".join(sentence)\n",
    "#     return sentence.strip()\n",
    "\n",
    "# def kanji2romaji(text):\n",
    "#     try:\n",
    "#         new_line = katsu.romaji(text)\n",
    "#         new_line = clean_romaji(new_line)\n",
    "#     except:\n",
    "#         new_line = None\n",
    "#     return new_line\n",
    "\n",
    "# def clean_en(sentence):\n",
    "#     sentence = re.sub(r\"\\(.*\\)|\\[.*\\]|\\{.*\\}\", \"\", sentence.strip().lower()) # Parenthesis\n",
    "#     sentence = re.sub(r\"\\.{3,}\", \",\", sentence) # Ellipsis\n",
    "#     sentence = re.sub(r\"[^A-Za-z0-9\\ \\?\\.\\!\\,\\'\\\"]\", \"\", sentence) # Non alphanumeric\n",
    "#     return sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef09910dcfc94263ad38a24ff32d35cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2801388 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d0eebbec1748539c2c87b243ab8e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4014254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d43b43275f4b2caf4b78ccca24f920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4014254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4765ea2960fd4abbb13fea004b77ead9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4013985 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa37126bd6244d89e943775153cfe37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4013985 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ja_raw</th>\n",
       "      <th>en</th>\n",
       "      <th>ja_ro</th>\n",
       "      <th>ja_hira</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>アレックス・ダンバースを殺すか お前が殺されるか。</td>\n",
       "      <td>kill alex danvers or let her kill you.</td>\n",
       "      <td>arekkusudan baasu wo korosu ka omae ga korosar...</td>\n",
       "      <td>あれっくすだん ばあす を ころす か おまえ が ころされる か</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>あいつに聞こえるわ</td>\n",
       "      <td>shut up. he can hear you, he can hear you.</td>\n",
       "      <td>aitsu ni kikoeru wa</td>\n",
       "      <td>あいつ に きこえる わ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vは人間のDNAを解析して 皮膚を生成 人間に疑われないように</td>\n",
       "      <td>the v's used the human d.n.a. to replicate it,...</td>\n",
       "      <td>v wa ningen no dna wo kaiseki shite hifu wo se...</td>\n",
       "      <td>は にんげん の な を かいせき して ひふ を せいせい にんげん に うたがわれない ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>スーパーマンが登場以来、超最悪の男の 戦争はその都市で繰り広げられている。</td>\n",
       "      <td>since superman's arrival, a veritable super ba...</td>\n",
       "      <td>suupaaman ga toujou irai chousaiaku no otoko n...</td>\n",
       "      <td>すうぱあまん が とうじょう いらい ちょうさいあく の おとこ の せんそう は その と...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>こんにちは？</td>\n",
       "      <td>hello?</td>\n",
       "      <td>konnichiha</td>\n",
       "      <td>こんにちは</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4013980</th>\n",
       "      <td>知って何の意味がある?</td>\n",
       "      <td>what good would that have done?</td>\n",
       "      <td>shitte nan no imi ga aru</td>\n",
       "      <td>しって なん の いみ が ある</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4013981</th>\n",
       "      <td>二之助さんに駆け寄った 一之助さんは</td>\n",
       "      <td>when ichinosukesan rushed over to ninosukesan..</td>\n",
       "      <td>ni no sukesan ni kakeyotta ichinosukesan wa</td>\n",
       "      <td>に の すけさん に かけよった いちのすけさん わ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4013982</th>\n",
       "      <td>そういった お言葉を力に...。</td>\n",
       "      <td>your words are so encouraging.</td>\n",
       "      <td>sou itta okotoba wo chikara ni</td>\n",
       "      <td>そう いった おことば を ちから に</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4013983</th>\n",
       "      <td>切るぞ</td>\n",
       "      <td>goodbye, george.</td>\n",
       "      <td>kiru zo</td>\n",
       "      <td>きる ぞ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4013984</th>\n",
       "      <td>聞きたくなければ言わないけれど。</td>\n",
       "      <td>do you want to hear it?</td>\n",
       "      <td>kikitaku nakereba iwanaikeredo</td>\n",
       "      <td>ききたく なければ いわないけれど</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4013985 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        ja_raw  \\\n",
       "0                    アレックス・ダンバースを殺すか お前が殺されるか。   \n",
       "1                                    あいつに聞こえるわ   \n",
       "2              Vは人間のDNAを解析して 皮膚を生成 人間に疑われないように   \n",
       "3        スーパーマンが登場以来、超最悪の男の 戦争はその都市で繰り広げられている。   \n",
       "4                                       こんにちは？   \n",
       "...                                        ...   \n",
       "4013980                            知って何の意味がある?   \n",
       "4013981                     二之助さんに駆け寄った 一之助さんは   \n",
       "4013982                       そういった お言葉を力に...。   \n",
       "4013983                                    切るぞ   \n",
       "4013984                       聞きたくなければ言わないけれど。   \n",
       "\n",
       "                                                        en  \\\n",
       "0                   kill alex danvers or let her kill you.   \n",
       "1               shut up. he can hear you, he can hear you.   \n",
       "2        the v's used the human d.n.a. to replicate it,...   \n",
       "3        since superman's arrival, a veritable super ba...   \n",
       "4                                                   hello?   \n",
       "...                                                    ...   \n",
       "4013980                    what good would that have done?   \n",
       "4013981    when ichinosukesan rushed over to ninosukesan..   \n",
       "4013982                     your words are so encouraging.   \n",
       "4013983                                   goodbye, george.   \n",
       "4013984                            do you want to hear it?   \n",
       "\n",
       "                                                     ja_ro  \\\n",
       "0        arekkusudan baasu wo korosu ka omae ga korosar...   \n",
       "1                                      aitsu ni kikoeru wa   \n",
       "2        v wa ningen no dna wo kaiseki shite hifu wo se...   \n",
       "3        suupaaman ga toujou irai chousaiaku no otoko n...   \n",
       "4                                               konnichiha   \n",
       "...                                                    ...   \n",
       "4013980                           shitte nan no imi ga aru   \n",
       "4013981        ni no sukesan ni kakeyotta ichinosukesan wa   \n",
       "4013982                     sou itta okotoba wo chikara ni   \n",
       "4013983                                            kiru zo   \n",
       "4013984                     kikitaku nakereba iwanaikeredo   \n",
       "\n",
       "                                                   ja_hira  \n",
       "0                        あれっくすだん ばあす を ころす か おまえ が ころされる か  \n",
       "1                                             あいつ に きこえる わ  \n",
       "2        は にんげん の な を かいせき して ひふ を せいせい にんげん に うたがわれない ...  \n",
       "3        すうぱあまん が とうじょう いらい ちょうさいあく の おとこ の せんそう は その と...  \n",
       "4                                                    こんにちは  \n",
       "...                                                    ...  \n",
       "4013980                                   しって なん の いみ が ある  \n",
       "4013981                         に の すけさん に かけよった いちのすけさん わ  \n",
       "4013982                                そう いった おことば を ちから に  \n",
       "4013983                                               きる ぞ  \n",
       "4013984                                  ききたく なければ いわないけれど  \n",
       "\n",
       "[4013985 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# main_dir = \"D:\\School-stuff\\Sem-2\\PR-Project\\HoloASR\\Datasets\"\n",
    "# opus_ja_paths = glob.glob(f\"{main_dir}\\OPUS100-dataset\\*.ja\")\n",
    "# tatoeba_ja_paths = glob.glob(f\"{main_dir}\\Tatoeba-dataset\\*.ja\")\n",
    "# jesc_path = f\"{main_dir}/JESC-dataset/raw\"\n",
    "# ja_paths = opus_ja_paths + tatoeba_ja_paths\n",
    "\n",
    "# ja_lines, en_lines = [], []\n",
    "# for ja_path in ja_paths:\n",
    "#     if ja_path.endswith(\".ja\"):\n",
    "#         en_path = ja_path.rsplit(\".\", 1)[0] + \".en\"\n",
    "#     else:\n",
    "#         en_path = ja_path.replace(\"ja\", \"en\")\n",
    "#     with open(ja_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         lines = [line.strip(\"\\n\") for line in f.readlines()]\n",
    "#         ja_lines.extend(lines)\n",
    "#     with open(en_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         lines = [line.strip(\"\\n\") for line in f.readlines()]\n",
    "#         en_lines.extend(lines)\n",
    "\n",
    "# with open(jesc_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     texts = [text.split(\"\\t\") for text in f.readlines()]\n",
    "#     for en, ja in tqdm(texts):\n",
    "#         ja_lines.append(ja)\n",
    "#         en_lines.append(en)\n",
    "\n",
    "# tqdm.pandas()\n",
    "# data = pd.DataFrame({'ja_raw': ja_lines, 'en': en_lines})\n",
    "# data['ja_raw'] = data['ja_raw'].progress_apply(clean_kanji)\n",
    "# data['ja_ro'] = data['ja_raw'].progress_apply(kanji2romaji)\n",
    "# data = data.dropna().reset_index(drop=True)\n",
    "# data['ja_hira'] = data['ja_ro'].progress_apply(lambda x: Romaji2Kana(x).strip())\n",
    "# data['en'] = data['en'].progress_apply(clean_en)\n",
    "# data = data.dropna().reset_index(drop=True)\n",
    "# data.to_csv(\n",
    "#     r\"E:\\Datasets\\Language_model\\text_data\\tokenizer_text.csv\",\n",
    "#     index=False, encoding=\"utf-8\")\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train SPM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_dir = \"E://Datasets/Language_model/text_data\"\n",
    "# data = pd.read_csv(f\"{main_dir}/tokenizer_text.csv\")\n",
    "# data = data.dropna().reset_index(drop=True)[['en', 'ja_hira']]\n",
    "\n",
    "# with open(f\"{main_dir}/ja_text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     ja_lines = data['ja_hira'].apply(lambda x: x + \"\\n\").tolist()\n",
    "#     f.writelines(ja_lines)\n",
    "\n",
    "# with open(f\"{main_dir}\\en_text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     en_lines = data['en'].apply(lambda x: x + \"\\n\").tolist()\n",
    "#     f.writelines(en_lines)\n",
    "\n",
    "# ja_file = f\"{main_dir}/ja_text.txt\"\n",
    "# en_file = f\"{main_dir}/en_text.txt\"\n",
    "# ja_prefix = f\"{main_dir}/ja_spm\"\n",
    "# en_prefix = f\"{main_dir}/en_spm\"\n",
    "# vocab_prefix = f\"{main_dir}/vocab_spm\"\n",
    "\n",
    "# spm.SentencePieceTrainer.train(\n",
    "#     f\"--input={ja_file} --model_prefix={ja_prefix} --vocab_size={10003} --model_type=unigram --pad_id=0 --unk_id=2 --bos_id=-1 --eos_id=1 --pad_piece=<pad> --unk_piece=<unk> --eos_piece=</s>\"\n",
    "# )\n",
    "\n",
    "# spm.SentencePieceTrainer.train(\n",
    "#     f\"--input={en_file} --model_prefix={en_prefix} --vocab_size={50716} --model_type=unigram --pad_id=0 --unk_id=2 --bos_id=-1 --eos_id=1 --pad_piece=<pad> --unk_piece=<unk> --eos_piece=</s>\"\n",
    "# )\n",
    "\n",
    "# vocab = []\n",
    "# with open(f\"{main_dir}/ja_spm.vocab\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     for text in f.readlines():\n",
    "#         token, _ = text.split(\"\\t\")\n",
    "#         vocab.append(token)\n",
    "\n",
    "# with open(f\"{main_dir}/en_spm.vocab\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     for text in f.readlines()[3:]:\n",
    "#         token, _ = text.split(\"\\t\")\n",
    "#         vocab.append(token)\n",
    "\n",
    "# with open(f\"{main_dir}/vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     vocab = {k: v for v, k in enumerate(vocab)}\n",
    "#     json.dump(vocab, f, ensure_ascii=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32b0bdd799648c98d0fe932d99e6bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3987235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6a2d5c06a0451a95c4f8b15c765210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3987235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>ja_hira</th>\n",
       "      <th>ja_token</th>\n",
       "      <th>en_token</th>\n",
       "      <th>ja_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kill alex danvers or let her kill you.</td>\n",
       "      <td>あれっくすだん ばあす を ころす か おまえ が ころされる か</td>\n",
       "      <td>[3016, 638, 678, 86, 6, 574, 11, 60, 7, 2075, ...</td>\n",
       "      <td>[10220, 11581, 29914, 10093, 10084, 10068, 102...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shut up. he can hear you, he can hear you.</td>\n",
       "      <td>あいつ に きこえる わ</td>\n",
       "      <td>[475, 5, 1493, 19, 1]</td>\n",
       "      <td>[10511, 10062, 10003, 10024, 10038, 10256, 100...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the v's used the human d.n.a. to replicate it,...</td>\n",
       "      <td>は にんげん の な を かいせき して ひふ を せいせい にんげん に うたがわれない ...</td>\n",
       "      <td>[4, 231, 3, 13, 6, 5164, 21, 4142, 6, 4763, 23...</td>\n",
       "      <td>[10006, 11704, 10005, 10009, 10260, 10006, 103...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hello?</td>\n",
       "      <td>こんにちは</td>\n",
       "      <td>[1416, 1]</td>\n",
       "      <td>[10433, 10011, 1]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>have a good night, dear.</td>\n",
       "      <td>よい よる を</td>\n",
       "      <td>[184, 353, 6, 1]</td>\n",
       "      <td>[10028, 10012, 10086, 10196, 10004, 11040, 100...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3987230</th>\n",
       "      <td>what good would that have done?</td>\n",
       "      <td>しって なん の いみ が ある</td>\n",
       "      <td>[282, 17, 3, 324, 7, 32, 1]</td>\n",
       "      <td>[10023, 10086, 10092, 10014, 10028, 10228, 100...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3987231</th>\n",
       "      <td>when ichinosukesan rushed over to ninosukesan..</td>\n",
       "      <td>に の すけさん に かけよった いちのすけさん わ</td>\n",
       "      <td>[5, 3, 3612, 77, 5, 767, 5128, 107, 269, 3171,...</td>\n",
       "      <td>[10077, 55162, 10843, 16500, 10130, 10010, 557...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3987232</th>\n",
       "      <td>your words are so encouraging.</td>\n",
       "      <td>そう いった おことば を ちから に</td>\n",
       "      <td>[28, 112, 6263, 6, 397, 5, 1]</td>\n",
       "      <td>[10033, 10588, 10031, 10041, 19016, 10003, 1]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3987233</th>\n",
       "      <td>goodbye, george.</td>\n",
       "      <td>きる ぞ</td>\n",
       "      <td>[2103, 72, 1]</td>\n",
       "      <td>[11143, 10004, 11641, 10003, 1]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3987234</th>\n",
       "      <td>do you want to hear it?</td>\n",
       "      <td>ききたく なければ いわないけれど</td>\n",
       "      <td>[5763, 1175, 1759, 2488, 1]</td>\n",
       "      <td>[10032, 10007, 10072, 10010, 10256, 10013, 100...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3821989 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        en  \\\n",
       "0                   kill alex danvers or let her kill you.   \n",
       "1               shut up. he can hear you, he can hear you.   \n",
       "2        the v's used the human d.n.a. to replicate it,...   \n",
       "4                                                   hello?   \n",
       "5                                 have a good night, dear.   \n",
       "...                                                    ...   \n",
       "3987230                    what good would that have done?   \n",
       "3987231    when ichinosukesan rushed over to ninosukesan..   \n",
       "3987232                     your words are so encouraging.   \n",
       "3987233                                   goodbye, george.   \n",
       "3987234                            do you want to hear it?   \n",
       "\n",
       "                                                   ja_hira  \\\n",
       "0                        あれっくすだん ばあす を ころす か おまえ が ころされる か   \n",
       "1                                             あいつ に きこえる わ   \n",
       "2        は にんげん の な を かいせき して ひふ を せいせい にんげん に うたがわれない ...   \n",
       "4                                                    こんにちは   \n",
       "5                                                  よい よる を   \n",
       "...                                                    ...   \n",
       "3987230                                   しって なん の いみ が ある   \n",
       "3987231                         に の すけさん に かけよった いちのすけさん わ   \n",
       "3987232                                そう いった おことば を ちから に   \n",
       "3987233                                               きる ぞ   \n",
       "3987234                                  ききたく なければ いわないけれど   \n",
       "\n",
       "                                                  ja_token  \\\n",
       "0        [3016, 638, 678, 86, 6, 574, 11, 60, 7, 2075, ...   \n",
       "1                                    [475, 5, 1493, 19, 1]   \n",
       "2        [4, 231, 3, 13, 6, 5164, 21, 4142, 6, 4763, 23...   \n",
       "4                                                [1416, 1]   \n",
       "5                                         [184, 353, 6, 1]   \n",
       "...                                                    ...   \n",
       "3987230                        [282, 17, 3, 324, 7, 32, 1]   \n",
       "3987231  [5, 3, 3612, 77, 5, 767, 5128, 107, 269, 3171,...   \n",
       "3987232                      [28, 112, 6263, 6, 397, 5, 1]   \n",
       "3987233                                      [2103, 72, 1]   \n",
       "3987234                        [5763, 1175, 1759, 2488, 1]   \n",
       "\n",
       "                                                  en_token  ja_len  \n",
       "0        [10220, 11581, 29914, 10093, 10084, 10068, 102...      12  \n",
       "1        [10511, 10062, 10003, 10024, 10038, 10256, 100...       5  \n",
       "2        [10006, 11704, 10005, 10009, 10260, 10006, 103...      17  \n",
       "4                                        [10433, 10011, 1]       2  \n",
       "5        [10028, 10012, 10086, 10196, 10004, 11040, 100...       4  \n",
       "...                                                    ...     ...  \n",
       "3987230  [10023, 10086, 10092, 10014, 10028, 10228, 100...       7  \n",
       "3987231  [10077, 55162, 10843, 16500, 10130, 10010, 557...      13  \n",
       "3987232      [10033, 10588, 10031, 10041, 19016, 10003, 1]       7  \n",
       "3987233                    [11143, 10004, 11641, 10003, 1]       3  \n",
       "3987234  [10032, 10007, 10072, 10010, 10256, 10013, 100...       5  \n",
       "\n",
       "[3821989 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_dir = \"E://Datasets/Language_model/text_data\"\n",
    "\n",
    "tokenizer = MarianTokenizer(\n",
    "    source_spm=f\"{main_dir}/ja_spm.model\",\n",
    "    target_spm=f\"{main_dir}/en_spm.model\",\n",
    "    source_lang=\"ja\",\n",
    "    target_lang=\"en\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    vocab=f\"{main_dir}/vocab.json\")\n",
    "\n",
    "tqdm.pandas()\n",
    "data = pd.read_csv(f\"{main_dir}/tokenizer_text.csv\", encoding=\"utf-8\")\n",
    "data = data.dropna().reset_index(drop=True)[['en', 'ja_hira']]\n",
    "data['ja_token'] = data['ja_hira'].progress_apply(lambda x: tokenizer(x).input_ids)\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    data['en_token'] = data['en'].progress_apply(lambda x: tokenizer(x).input_ids)\n",
    "data['ja_len'] = data['ja_token'].apply(len)\n",
    "data = data.query(\"ja_len <= 17\")\n",
    "data.to_csv(f\"{main_dir}/marian_text.csv\", index=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ArgParser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--random_state\", default=42)\n",
    "    parser.add_argument(\"--main_dir\", default=\"E://Datasets/Language_model\")\n",
    "    parser.add_argument(\"--n_shards\", default=10)\n",
    "    parser.add_argument(\"--n_samples\", default=1200000)\n",
    "    parser.add_argument(\"--test_size\", default=0.1)\n",
    "    parser.add_argument(\"--batch_size\", default=32)\n",
    "    parser.add_argument(\"--buffer_size\", default=1024)\n",
    "\n",
    "    # Scheduler\n",
    "    parser.add_argument(\"--epochs\", default=15)\n",
    "    parser.add_argument(\"--learning_rate\", default=3e-4)\n",
    "    parser.add_argument(\"--lr_start\", default=1e-8)\n",
    "    parser.add_argument(\"--lr_min\", default=1e-8)\n",
    "    parser.add_argument(\"--lr_max\", default=3e-4)\n",
    "    parser.add_argument(\"--n_cycles\", default=0.5)\n",
    "    parser.add_argument(\"--warmup_epochs\", default=2)\n",
    "    parser.add_argument(\"--sustain_epochs\", default=1)\n",
    "\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    n_train = int(args.n_samples * (1 - args.test_size))\n",
    "    n_val = int(args.n_samples * args.test_size)\n",
    "    train_steps = int(np.ceil(n_train / args.batch_size))\n",
    "    val_steps = int(np.ceil(n_val / args.batch_size))\n",
    "        \n",
    "    parser.add_argument(\"--n_train\", default=n_train)\n",
    "    parser.add_argument(\"--n_val\", default=n_val)\n",
    "    parser.add_argument(\"--train_steps\", default=train_steps)  \n",
    "    parser.add_argument(\"--val_steps\", default=val_steps)\n",
    "\n",
    "    return parser.parse_known_args()[0]\n",
    "\n",
    "args = ArgParser()\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFRWriter():\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.data = self.get_data()\n",
    "\n",
    "    def get_data(self):\n",
    "        tqdm.pandas()\n",
    "        data = pd.read_csv(\n",
    "            f\"{self.args.main_dir}/text_data/marian_text.csv\", encoding=\"utf-8\")\n",
    "        data['ja_token'] = data['ja_token'].progress_apply(ast.literal_eval)\n",
    "        data['en_token'] = data['en_token'].progress_apply(ast.literal_eval)\n",
    "        data = data.sample(\n",
    "            n=self.args.n_samples,\n",
    "            random_state=self.args.random_state,\n",
    "            ignore_index=True)\n",
    "        data = data.sort_values(by=\"ja_len\", ignore_index=True, ascending=True)\n",
    "        return data\n",
    "\n",
    "    def _bytes_feature(self, value):\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    def serialize_example(self, *args):\n",
    "        feature = {\n",
    "            'input_ids': self._bytes_feature(args[0]),\n",
    "            'attention_mask': self._bytes_feature(args[1]),\n",
    "            'label_ids': self._bytes_feature(args[2])}\n",
    "\n",
    "        example_proto = tf.train.Example(\n",
    "            features=tf.train.Features(feature=feature))\n",
    "        return example_proto.SerializeToString()\n",
    "\n",
    "    def get_shards(self):\n",
    "        skf = KFold(n_splits=self.args.n_shards, shuffle=False)\n",
    "        return [j for i,j in skf.split(self.data)]\n",
    "\n",
    "    def get_shard_data(self, samples):\n",
    "        for sample in samples:\n",
    "            input_ids = tf.convert_to_tensor(\n",
    "                self.data['ja_token'][sample], dtype=tf.int32)\n",
    "            attention_mask = tf.where(input_ids != 0, x=1, y=0)\n",
    "            label_ids = tf.convert_to_tensor(\n",
    "                self.data['en_token'][sample], dtype=tf.int32)\n",
    "            yield {\n",
    "                \"input_ids\": tf.io.serialize_tensor(input_ids),\n",
    "                \"attention_mask\": tf.io.serialize_tensor(attention_mask),\n",
    "                \"label_ids\": tf.io.serialize_tensor(label_ids)}\n",
    "\n",
    "    def write(self):\n",
    "        for shard, samples in tqdm(enumerate(self.get_shards()), total=self.args.n_shards):\n",
    "            with tf.io.TFRecordWriter(f\"{self.args.main_dir}/marian_tfrec/shard_{shard+1}.tfrec\") as f:\n",
    "                for sample in self.get_shard_data(samples):\n",
    "                    example = self.serialize_example(\n",
    "                        sample['input_ids'],\n",
    "                        sample['attention_mask'],\n",
    "                        sample['label_ids'])\n",
    "                    f.write(example)\n",
    "\n",
    "# TFRWriter(args).write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, args):\n",
    "        self.files = glob.glob(args.main_dir + \"/marian_tfrec/*.tfrec\")\n",
    "        self.args = args\n",
    "        self.AUTOTUNE = tf.data.AUTOTUNE\n",
    "        self.train_files, self.val_files = train_test_split(\n",
    "            self.files, test_size=args.test_size, shuffle=True, \n",
    "            random_state=args.random_state)\n",
    "        self.train = self.get_train()\n",
    "        self.val = self.get_val()\n",
    "\n",
    "    def read_tfrecord(self, example):\n",
    "        feature_description = {\n",
    "            'input_ids': tf.io.FixedLenFeature([], tf.string),\n",
    "            'attention_mask': tf.io.FixedLenFeature([], tf.string),\n",
    "            'label_ids': tf.io.FixedLenFeature([], tf.string)\n",
    "            }\n",
    "        \n",
    "        example = tf.io.parse_single_example(example, feature_description)\n",
    "        example['input_ids'] = tf.io.parse_tensor(\n",
    "            example['input_ids'], out_type=tf.int32)\n",
    "        example['attention_mask'] = tf.io.parse_tensor(\n",
    "            example['attention_mask'], out_type=tf.int32) \n",
    "        example['label_ids'] = tf.io.parse_tensor(\n",
    "            example['label_ids'], out_type=tf.int32)\n",
    "        return example\n",
    "\n",
    "    def load_dataset(self, files):\n",
    "        ignore_order = tf.data.Options()\n",
    "        ignore_order.experimental_deterministic = False\n",
    "        dataset = tf.data.TFRecordDataset(files)\n",
    "        dataset = dataset.with_options(ignore_order)\n",
    "        dataset = dataset.map(self.read_tfrecord, num_parallel_calls=self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_train(self):\n",
    "        dataset = self.load_dataset(self.train_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_ids': [None],\n",
    "                'attention_mask': [None],\n",
    "                'label_ids': [None]\n",
    "            },\n",
    "            padding_values={\n",
    "                'input_ids': tf.constant(0, dtype=tf.int32),\n",
    "                'attention_mask': tf.constant(0, dtype=tf.int32),\n",
    "                'label_ids': tf.constant(-100, dtype=tf.int32)\n",
    "            })        \n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_val(self):\n",
    "        dataset = self.load_dataset(self.val_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_ids': [None],\n",
    "                'attention_mask': [None],\n",
    "                'label_ids': [None]\n",
    "            },\n",
    "            padding_values={\n",
    "                'input_ids': tf.constant(0, dtype=tf.int32),\n",
    "                'attention_mask': tf.constant(0, dtype=tf.int32),\n",
    "                'label_ids': tf.constant(-100, dtype=tf.int32)\n",
    "            })\n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "# train = DataLoader(args).train\n",
    "\n",
    "# inputs = next(iter(train))\n",
    "# input_values = inputs['input_ids']\n",
    "# attention_mask = inputs['attention_mask']\n",
    "# labels = inputs['label_ids']\n",
    "# print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLEUMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"BLEU\", **kwargs):\n",
    "        super(BLEUMetric, self).__init__(name=name, **kwargs)\n",
    "        self.bleu = BLEU()\n",
    "        self.accumulator = self.add_weight(name=\"total_bleu\", initializer=\"zeros\")\n",
    "        self.counter = self.add_weight(name=\"counter\", initializer=\"zeros\")\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        try:\n",
    "            bleu_score = self.bleu.corpus_score(hypotheses=y_pred, references=y_true).score\n",
    "        except:\n",
    "            y_pred = [\".\" if x == \"\" else x for x in y_pred]\n",
    "            bleu_score = self.bleu.corpus_score(hypotheses=y_pred, references=y_true).score\n",
    "        self.accumulator.assign_add(bleu_score)\n",
    "        self.counter.assign_add(1)\n",
    "\n",
    "    def result(self):\n",
    "        return tf.math.divide_no_nan(self.accumulator, self.counter)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.accumulator.assign(0.0)\n",
    "        self.counter.assign(0.0)\n",
    "\n",
    "class WERMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"WER\", **kwargs):\n",
    "        super(WERMetric, self).__init__(name=name,  **kwargs)\n",
    "        self.accumulator = self.add_weight(name=\"total_wer\", initializer=\"zeros\")\n",
    "        self.counter = self.add_weight(name=\"wer_count\", initializer=\"zeros\")    \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        wer = jiwer.wer(y_true, y_pred)\n",
    "\n",
    "        # Add distance and number of batches to variables\n",
    "        self.accumulator.assign_add(wer)\n",
    "        self.counter.assign_add(1)\n",
    "\n",
    "    def result(self):\n",
    "        # Divides accumulated distance scores against number of batches passed,\n",
    "        # mimics mean reduction over batch\n",
    "        return tf.math.divide_no_nan(self.accumulator, self.counter)   \n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.accumulator.assign(0.0)\n",
    "        self.counter.assign(0.0)\n",
    "\n",
    "class CosineDecayWithWarmup(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "    def __call__(self, epoch):  \n",
    "        if epoch < self.args.warmup_epochs:\n",
    "            lr = ((self.args.lr_max - self.args.lr_start) / self.args.warmup_epochs) * epoch + self.args.lr_start\n",
    "        elif epoch < (self.args.warmup_epochs + self.args.sustain_epochs):\n",
    "            lr = self.args.lr_max\n",
    "        else:\n",
    "            progress = ((epoch - self.args.warmup_epochs - self.args.sustain_epochs) / \n",
    "            (self.args.epochs - self.args.warmup_epochs - self.args.sustain_epochs))\n",
    "            lr = (self.args.lr_max-self.args.lr_min) * (0.5 * (1.0 + tf.math.cos((22/7) * \n",
    "                self.args.n_cycles * 2.0 * progress)))\n",
    "            if self.args.lr_min is not None:\n",
    "                lr = tf.math.maximum(self.args.lr_min, lr)\n",
    "        return lr\n",
    "\n",
    "    def plot(self):\n",
    "        epochs = range(self.args.epochs+1)\n",
    "        lr = [self(epoch) for epoch in epochs]\n",
    "        plt.plot(epochs, lr)\n",
    "        plt.xlabel(\"learning_rate\")\n",
    "        plt.ylabel(\"epochs\")\n",
    "        plt.show()\n",
    "\n",
    "CosineDecayWithWarmup(args).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.tokenizer = MarianTokenizer(\n",
    "            source_spm=f\"{args.main_dir}/text_data/ja_spm.model\",\n",
    "            target_spm=f\"{args.main_dir}/text_data/en_spm.model\",\n",
    "            source_lang=\"ja\",\n",
    "            target_lang=\"en\",\n",
    "            eos_token=\"</s>\",\n",
    "            unk_token=\"<unk>\",\n",
    "            pad_token=\"<pad>\",\n",
    "            vocab=f\"{args.main_dir}/text_data/vocab.json\")\n",
    "        self.train_dataset = DataLoader(args).train\n",
    "        self.val_dataset = DataLoader(args).val\n",
    "        schedule = CosineDecayWithWarmup(args)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(schedule)\n",
    "        self.bleu_metric = BLEUMetric()\n",
    "\n",
    "        self.model = TFMarianMTModel.from_pretrained(\n",
    "            \"Helsinki-NLP/opus-mt-ja-en\",\n",
    "            bad_words_ids=[[0]],\n",
    "            bos_token_id=1,\n",
    "            eos_token_id=1,\n",
    "            pad_token_id=0,\n",
    "            decoder_start_token_id=0,\n",
    "            from_pt=True,\n",
    "            use_cache=False\n",
    "        )\n",
    "\n",
    "        self.model_name = f\"model_{int(self.args.n_samples/1000)}k\"\n",
    "        self.log_path = f\"{self.args.main_dir}/model_weights/{self.model_name}.csv\"\n",
    "        if not os.path.exists(self.log_path):\n",
    "            print(\"Log file created.\")\n",
    "            columns = \"epoch,loss,bleu,val_loss,val_bleu\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(columns)\n",
    "\n",
    "    def decoder(self, labels, logits):\n",
    "        labels = tf.where(labels == -100, x=0, y=labels)\n",
    "        labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        logits = tf.argmax(logits, axis=-1)\n",
    "        logits = self.tokenizer.batch_decode(logits, skip_special_tokens=True)\n",
    "        return labels, logits\n",
    "\n",
    "    def display(self, epoch, t_labels, t_logits, v_labels, v_logits):\n",
    "        print(\"-\" * 129)\n",
    "        print(\"Training\")\n",
    "        for y_true, y_pred in zip(t_labels, t_logits):\n",
    "            print(f\"Target:    {y_true}\")\n",
    "            print(f\"Predicted: {y_pred}\") \n",
    "\n",
    "        print(\"\\nValidation\")\n",
    "        for y_true, y_pred in zip(v_labels, v_logits):\n",
    "            print(f\"Target:    {y_true}\")\n",
    "            print(f\"Predicted: {y_pred}\")\n",
    "        print(\"-\" * 129)\n",
    "        \n",
    "    def fit(self):\n",
    "        # Checkpointing\n",
    "        self.ckpt_dir = f\"{self.args.main_dir}/checkpoints\"\n",
    "        self.ckpt = tf.train.Checkpoint(self.model)\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(\n",
    "            checkpoint=self.ckpt, directory=self.ckpt_dir, max_to_keep=5)\n",
    "\n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            self.start_epoch = int(self.ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "            self.ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "            print(f\"Resuming from epoch {self.start_epoch + 1}...\")\n",
    "        else:\n",
    "            self.start_epoch = 0\n",
    "            print(\"Starting from epoch 1...\")\n",
    "\n",
    "        for epoch in range(self.args.epochs):\n",
    "            print(f\"Epoch {epoch+1}/{self.args.epochs}: Learning rate @ {self.optimizer.lr(epoch):.2e}\")\n",
    "            stateful_metrics = [\"loss\", \"bleu\", \"val_loss\", \"val_bleu\"]\n",
    "            progbar = tf.keras.utils.Progbar(\n",
    "                self.args.train_steps, interval=0.05,\n",
    "                stateful_metrics=stateful_metrics)\n",
    "\n",
    "            # Training loop\n",
    "            for step, t_batch in enumerate(self.train_dataset):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    t_loss, t_logits = self.model(\n",
    "                        input_ids=t_batch['input_ids'],\n",
    "                        attention_mask=t_batch['attention_mask'],\n",
    "                        labels=t_batch['label_ids'],\n",
    "                        training=True)[:2]              \n",
    "                \n",
    "                gradients = tape.gradient(t_loss, self.model.trainable_weights)\n",
    "                self.optimizer.apply_gradients(zip(gradients, self.model.trainable_weights))\n",
    "                t_labels, t_logits = self.decoder(t_batch['label_ids'], t_logits)\n",
    "                self.bleu_metric.update_state(t_labels, t_logits)\n",
    "                t_bleu = self.bleu_metric.result()\n",
    "                t_values = [\n",
    "                    (\"loss\", tf.reduce_mean(t_loss)),\n",
    "                    (\"bleu\", t_bleu)]\n",
    "                progbar.update(step, values=t_values, finalize=False)\n",
    "            self.bleu_metric.reset_state()\n",
    "\n",
    "            # Validation loop\n",
    "            for v_batch in self.val_dataset:\n",
    "                v_loss, v_logits = self.model(\n",
    "                    input_ids=v_batch['input_ids'],\n",
    "                    attention_mask=v_batch['attention_mask'],\n",
    "                    labels=v_batch['label_ids'],\n",
    "                    training=False)[:2]\n",
    "                v_labels, v_logits = self.decoder(v_batch['label_ids'], v_logits)\n",
    "                self.bleu_metric.update_state(v_labels, v_logits)\n",
    "            \n",
    "            v_bleu = self.bleu_metric.result()\n",
    "            v_values = [\n",
    "                (\"loss\", tf.reduce_mean(t_loss)),\n",
    "                (\"bleu\", t_bleu),\n",
    "                (\"val_loss\", tf.reduce_mean(v_loss)),\n",
    "                (\"val_bleu\", v_bleu)]\n",
    "            progbar.update(self.args.train_steps, values=v_values, finalize=True)\n",
    "            self.bleu_metric.reset_state()\n",
    "\n",
    "            # Print sample transcriptions for both loops\n",
    "            self.display(epoch, t_labels, t_logits, v_labels, v_logits)\n",
    "\n",
    "            # Checkpointing\n",
    "            self.ckpt.save(file_prefix=f\"{self.ckpt_dir}/{self.model_name}\")\n",
    "\n",
    "            # Logging\n",
    "            log = f\"{epoch+1},{tf.reduce_mean(t_loss)},{t_bleu},{tf.reduce_mean(v_loss)},{v_bleu}\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(log)\n",
    "\n",
    "            save_path = f\"{self.args.main_dir}/model_weights\"\n",
    "            self.model.save_weights(f\"{save_path}/{self.model_name}_{epoch+1}.h5\")\n",
    "\n",
    "Trainer(args).fit()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b851d2923cfa3a2562599062e05fd9893d86a7c009c64d8ad3756552e4dd5f41"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('tf-gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
