{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from transformers import (\r\n",
    "    Wav2Vec2Processor, TFWav2Vec2ForCTC, \r\n",
    ")\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.keras.layers import *\r\n",
    "from tensorflow.keras import Model\r\n",
    "\r\n",
    "import glob\r\n",
    "import json\r\n",
    "import jiwer\r\n",
    "import re\r\n",
    "import random\r\n",
    "import pykakasi\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import argparse\r\n",
    "import heapq\r\n",
    "import operator\r\n",
    "from tqdm.notebook import tqdm\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "from collections import Counter, defaultdict\r\n",
    "\r\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\r\n",
    "\r\n",
    "def seed_everything(SEED):\r\n",
    "   random.seed(SEED)\r\n",
    "   np.random.seed(SEED)\r\n",
    "   tf.random.set_seed(SEED)\r\n",
    "   print(\"Random seed set.\")\r\n",
    "\r\n",
    "seed_everything(SEED=42)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Random seed set.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def Parser():\r\n",
    "    parser = argparse.ArgumentParser()\r\n",
    "    parser.add_argument('--main_dir', default=\"Datasets\\WIKI-corpus-ja-dataset\")\r\n",
    "    parser.add_argument(\"--n_samples\", default=79848)\r\n",
    "    parser.add_argument('--input_length', default=32)\r\n",
    "    parser.add_argument('--vocab_size', default=50000)\r\n",
    "    parser.add_argument('--embedding_dim', default=400)\r\n",
    "    parser.add_argument('--rnn_units', default=400)\r\n",
    "    parser.add_argument('--batch_size', default=64)\r\n",
    "    parser.add_argument('--n_splits', default=5)\r\n",
    "    parser.add_argument('--random_state', default=42)\r\n",
    "    parser.add_argument('--buffer_size', default=1024)\r\n",
    "    parser.add_argument('--dropout', default=0.2)\r\n",
    "    parser.add_argument('--learning_rate', default=1e-2)\r\n",
    "    parser.add_argument('--epochs', default=10)\r\n",
    "\r\n",
    "    args = parser.parse_known_args()[0]    \r\n",
    "\r\n",
    "    test_size = (1 / args.n_splits)\r\n",
    "    n_train = int(args.n_samples * (1 - test_size))\r\n",
    "    train_steps = int(np.ceil(n_train / args.batch_size)) - 1\r\n",
    "    parser.add_argument(\"--test_size\", type=float, default=test_size)\r\n",
    "    parser.add_argument(\"--train_steps\", type=int, default=train_steps)\r\n",
    "    args = parser.parse_known_args()[0]\r\n",
    "    return args\r\n",
    "\r\n",
    "args = Parser()\r\n",
    "args"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Namespace(batch_size=64, buffer_size=1024, dropout=0.2, embedding_dim=400, epochs=10, input_length=32, learning_rate=0.01, main_dir='Datasets\\\\WIKI-corpus-ja-dataset', n_samples=79848, n_splits=5, random_state=42, rnn_units=400, test_size=0.2, train_steps=998, vocab_size=50000)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## WikiCorpus"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def XMLExtract(xml_path):\r\n",
    "    symbols = r'[（.*?）！-～.,;..._。、-〿・■（）：ㇰ-ㇿ㈠-㉃㊀-㋾㌀-㍿「」『』→ー -~]'\r\n",
    "\r\n",
    "    def get_hira(text):\r\n",
    "        kks = pykakasi.kakasi()\r\n",
    "        result = kks.convert(text)\r\n",
    "        return [item['hira'] for item in result]\r\n",
    "\r\n",
    "    lines = open(xml_path, \"r\", encoding=\"utf-8\").readlines()\r\n",
    "    data = []\r\n",
    "    for line in lines:\r\n",
    "        if line.startswith(\"<j>\"):\r\n",
    "            ja_pattern = \"<j>(.*?)<\\/j>\"\r\n",
    "            line = re.findall(ja_pattern, line)[0]\r\n",
    "            line = re.sub(symbols, \"\", line)\r\n",
    "            line = line.strip(\"\\n|\\t| |　\")\r\n",
    "            data.append(line)\r\n",
    "        elif line.startswith('<e type=\"trans\" ver=\"2\">'):\r\n",
    "            en_pattern = \"<e.*?>(.*?)<\\/e>\"\r\n",
    "            line = re.findall(en_pattern, line)[0]\r\n",
    "            line = re.sub(\"\\([^()]*\\)|(&.*?;)+|\\n|\\t|\", \"\", line)\r\n",
    "            data.append(line)\r\n",
    "        else:\r\n",
    "            pass\r\n",
    "    return {\r\n",
    "        \"hira\": [get_hira(text) for text in data[0::2]],\r\n",
    "        \"en\": data[1::2]}\r\n",
    "\r\n",
    "all_hira, all_en = [], []\r\n",
    "for xml_path in tqdm(glob.glob(\"Datasets\\WIKI-corpus-ja-dataset\\*\\*.xml\")):\r\n",
    "    hira, en = list(XMLExtract(xml_path).values())\r\n",
    "    hira = [\"\".join(words) + \"\\n\" for words in hira]\r\n",
    "    en = [words + \"\\n\" for words in en]\r\n",
    "    all_hira += hira\r\n",
    "    all_en += en\r\n",
    "\r\n",
    "with open(r\"Datasets\\WIKI-corpus-ja-dataset\\all_hira.txt\", \"w\", encoding=\"utf-8\") as f:\r\n",
    "    f.writelines(all_hira)\r\n",
    "\r\n",
    "with open(r\"Datasets\\WIKI-corpus-ja-dataset\\all_en.txt\", \"w\", encoding=\"utf-8\") as f:\r\n",
    "    f.writelines(all_en)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d62e4421b7a8404e9787ba50e72f6fa8"
      },
      "text/plain": [
       "  0%|          | 0/14111 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# lines = open(r\"Datasets\\WIKI-corpus-ja-dataset\\all_data.txt\", 'r', encoding='utf-8').readlines()\r\n",
    "# seq_lens = [len(line.split()) for line in lines]\r\n",
    "# sns.countplot(x=seq_lens)\r\n",
    "# plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "corpus_path = args.main_dir + \"\\wiki_dataset_mecab_80000.txt\"\r\n",
    "lines = open(corpus_path, 'r', encoding='utf-8').readlines()\r\n",
    "seq_lens = [len(line.split()) for line in lines]\r\n",
    "print(\"Number of samples:\", len(lines))\r\n",
    "sns.countplot(x=seq_lens)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class WikiMeCabDataset:\r\n",
    "    def __init__(self, args):\r\n",
    "        self.args = args\r\n",
    "        self.buffer_size = 1024\r\n",
    "        self.AUTOTUNE = tf.data.AUTOTUNE\r\n",
    "        self.dict_path = f\"{args.main_dir}/vocab_dict.json\"\r\n",
    "        self.corpus_path = f\"{args.main_dir}/wiki_dataset_mecab_80000.txt\"\r\n",
    "        self.tfrec_path = f\"{args.main_dir}/wiki_tfrec\"\r\n",
    "        self.lines = self.apply_encoding(self.random_sample())\r\n",
    "\r\n",
    "    def random_sample(self):\r\n",
    "        lines = open(self.corpus_path, 'r', encoding='utf-8').readlines()\r\n",
    "        return random.sample(lines, len(lines) - 3)\r\n",
    "\r\n",
    "    def get_length(self, lines):\r\n",
    "        return [len(line) for line in lines]\r\n",
    "\r\n",
    "    def preprocess(self, lines):\r\n",
    "        for i, line in enumerate(lines):\r\n",
    "            line = line.strip(\"\\n\").split()\r\n",
    "            lines[i] = self.apply_padding(line)\r\n",
    "        return lines\r\n",
    "\r\n",
    "    def apply_padding(self, line):\r\n",
    "        line = [\"<BOS>/<BOS>\"] + line + [\"<EOS>/<EOS>\"]\r\n",
    "        pad_len = self.args.input_length - len(line)\r\n",
    "        line = np.pad(line, pad_width=(0, pad_len+1), constant_values=\"<PAD>/<PAD>\")\r\n",
    "        return line\r\n",
    "\r\n",
    "    def get_vocab(self, lines):\r\n",
    "        markers = [\"<PAD>/<PAD>\", \"<BOS>/<BOS>\", \"<EOS>/<EOS>\", \"<UNK>/<UNK>\"]\r\n",
    "        words = Counter(word for line in lines for word in line)\r\n",
    "        words = sorted(words, key=words.get, reverse=True)\r\n",
    "        words = [word for word in words if word not in markers]\r\n",
    "        return {word: i for i, word in enumerate(markers + words)}\r\n",
    "\r\n",
    "    def apply_encoding(self, lines):\r\n",
    "        lines = self.preprocess(lines)\r\n",
    "        vocab = self.get_vocab(lines)\r\n",
    "        with open(self.dict_path, \"w\", encoding=\"utf-8\") as f:\r\n",
    "            json.dump(vocab, f, sort_keys=False, indent=4, ensure_ascii=False)        \r\n",
    "        for i, line in enumerate(lines):\r\n",
    "            lines[i] = list(map(vocab.get, line))\r\n",
    "        return lines\r\n",
    "\r\n",
    "    def get_shards(self, lines):\r\n",
    "        skf = StratifiedKFold(\r\n",
    "            n_splits=self.args.n_splits, shuffle=True, \r\n",
    "            random_state=self.args.random_state)\r\n",
    "        self.length = self.get_length(lines)\r\n",
    "        return [\r\n",
    "            list(map(lambda x: lines[x], j))\r\n",
    "            for i, j in skf.split(lines, self.length)]\r\n",
    "\r\n",
    "    def get_shard_data(self, samples):\r\n",
    "        for sample in tqdm(samples):\r\n",
    "            yield {\r\n",
    "                'input': tf.io.serialize_tensor(sample[:-1]),\r\n",
    "                'label': tf.io.serialize_tensor(sample[1:])}\r\n",
    "\r\n",
    "    def _bytes_feature(self, value):\r\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\r\n",
    "        if isinstance(value, type(tf.constant(0))):\r\n",
    "            value = value.numpy()\r\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\r\n",
    "\r\n",
    "    def serialize_example(self, *args):\r\n",
    "        feature = {\r\n",
    "            'input': self._bytes_feature(args[0]),\r\n",
    "            'label': self._bytes_feature(args[1])}\r\n",
    "\r\n",
    "        example_proto = tf.train.Example(\r\n",
    "            features=tf.train.Features(feature=feature))\r\n",
    "        return example_proto.SerializeToString()  \r\n",
    "    \r\n",
    "    def write(self):\r\n",
    "        for shard, samples in enumerate(self.get_shards(self.lines)):\r\n",
    "            with tf.io.TFRecordWriter(\r\n",
    "                    f\"{self.tfrec_path}/shard_{shard+1}.tfrec\") as f:\r\n",
    "                for sample in self.get_shard_data(samples):\r\n",
    "                    example = self.serialize_example(\r\n",
    "                        sample['input'], sample['label'])\r\n",
    "                    f.write(example)\r\n",
    "\r\n",
    "# WikiMeCabDataset(args).write()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class DataLoader:\r\n",
    "    def __init__(self, args):\r\n",
    "        self.args = args\r\n",
    "        self.tfrec_path = f\"{args.main_dir}/wiki_tfrec\"\r\n",
    "        self.files = [os.path.join(self.tfrec_path, f) for f in os.listdir(self.tfrec_path)]\r\n",
    "        self.AUTOTUNE = tf.data.experimental.AUTOTUNE\r\n",
    "        self.train_files, self.val_files = train_test_split(\r\n",
    "            self.files, test_size=1/args.n_splits, shuffle=True)\r\n",
    "        self.train = self.train()\r\n",
    "        self.val = self.val()\r\n",
    "\r\n",
    "    def read_tfrecord(self, example):\r\n",
    "        feature_description = {\r\n",
    "            'input': tf.io.FixedLenFeature([], tf.string),\r\n",
    "            'label': tf.io.FixedLenFeature([], tf.string)}\r\n",
    "        \r\n",
    "        example = tf.io.parse_single_example(example, feature_description)\r\n",
    "        example['input'] = tf.io.parse_tensor(\r\n",
    "            example['input'], out_type=tf.int32)\r\n",
    "        example['label'] = tf.io.parse_tensor(\r\n",
    "            example['label'], out_type=tf.int32)\r\n",
    "        example['label'] = tf.one_hot(example['label'], self.args.vocab_size)\r\n",
    "        return example['input'], example['label']\r\n",
    "\r\n",
    "    def load_dataset(self, files):\r\n",
    "        ignore_order = tf.data.Options()\r\n",
    "        ignore_order.experimental_deterministic = False\r\n",
    "        dataset = tf.data.TFRecordDataset(files)\r\n",
    "        dataset = dataset.with_options(ignore_order)\r\n",
    "        dataset = dataset.map(self.read_tfrecord, num_parallel_calls=self.AUTOTUNE)\r\n",
    "        return dataset\r\n",
    "\r\n",
    "    def train(self):\r\n",
    "        dataset = self.load_dataset(self.train_files)\r\n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\r\n",
    "        dataset = dataset.batch(self.args.batch_size)\r\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\r\n",
    "        return dataset\r\n",
    "\r\n",
    "    def val(self):\r\n",
    "        dataset = self.load_dataset(self.val_files)\r\n",
    "        dataset = dataset.batch(self.args.batch_size)\r\n",
    "        return dataset\r\n",
    "\r\n",
    "val = DataLoader(args).val\r\n",
    "next(iter(val))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class HiraKanji(tf.keras.Model):\r\n",
    "    def __init__(self, args, name='HiraKanji'):\r\n",
    "        super(HiraKanji, self).__init__(name=name)\r\n",
    "        self.args = args\r\n",
    "        self.embedding = Embedding(\r\n",
    "            input_dim=args.vocab_size, \r\n",
    "            output_dim=args.embedding_dim, \r\n",
    "            input_length=args.input_length, \r\n",
    "            mask_zero=True)\r\n",
    "        self.lstm = Bidirectional(LSTM(\r\n",
    "            args.rnn_units, \r\n",
    "            dropout=args.dropout,\r\n",
    "            return_sequences=True, \r\n",
    "            return_state=True))\r\n",
    "        self.dropout = Dropout(args.dropout)\r\n",
    "        self.dense = Dense(args.vocab_size, activation='softmax')\r\n",
    "\r\n",
    "    def call(self, inputs, hidden_states, training):\r\n",
    "        x = self.embedding(inputs)\r\n",
    "        mask = tf.not_equal(inputs, 0)\r\n",
    "        x, forward_h, forward_c, backward_h, backward_c = self.lstm(\r\n",
    "            x, mask=mask, initial_state=hidden_states)\r\n",
    "        hidden_states = [forward_h, forward_c, backward_h, backward_c]\r\n",
    "        x = self.dropout(x, training=training)\r\n",
    "        x = self.dense(x)\r\n",
    "        return x, hidden_states\r\n",
    "\r\n",
    "    def initialize_hidden_states(self):\r\n",
    "        return [tf.zeros((self.args.batch_size, self.args.rnn_units)) for _ in range(4)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Trainer:\r\n",
    "    def __init__(self, args):\r\n",
    "        self.args = args\r\n",
    "        self.model = HiraKanji(args)\r\n",
    "        self.optimizer = tf.keras.optimizers.Adam(\r\n",
    "            learning_rate=args.learning_rate)\r\n",
    "        self.loss_fn = tf.keras.losses.CategoricalCrossentropy(\r\n",
    "            from_logits=False, label_smoothing=0.2)\r\n",
    "        self.metric = tf.keras.metrics.CategoricalAccuracy()\r\n",
    "        self.dataloader = DataLoader(args)   \r\n",
    "\r\n",
    "    def train(self):\r\n",
    "        for epoch in range(self.args.epochs):\r\n",
    "            stateful_metrics = ['loss', 'acc', 'val_loss', 'val_acc']\r\n",
    "            print(f\"Epoch {epoch+1}/{self.args.epochs}\")\r\n",
    "            progbar = tf.keras.utils.Progbar(\r\n",
    "                self.args.train_steps, interval=0.05,\r\n",
    "                stateful_metrics=stateful_metrics)\r\n",
    "            hidden_states = self.model.initialize_hidden_states()\r\n",
    "\r\n",
    "            for step, (t_inputs, t_labels) in enumerate(self.dataloader.train):\r\n",
    "                t_mask = tf.not_equal(tf.math.argmax(t_labels, axis=-1), 0)\r\n",
    "\r\n",
    "                with tf.GradientTape() as tape:\r\n",
    "                    t_logits, hidden_states = self.model(t_inputs, hidden_states, training=True)\r\n",
    "                    t_loss = self.loss_fn(t_labels, t_logits)\r\n",
    "\r\n",
    "                grads = tape.gradient(t_loss, self.model.trainable_weights)\r\n",
    "                self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\r\n",
    "                self.metric.update_state(t_labels, t_logits, sample_weight=t_mask)\r\n",
    "                t_acc = self.metric.result()\r\n",
    "                values=[('loss', t_loss), ('acc', t_acc)]\r\n",
    "                progbar.update(step, values=values, finalize=False)\r\n",
    "                self.metric.reset_states()\r\n",
    "\r\n",
    "            for v_inputs, v_labels in self.dataloader.val:\r\n",
    "                v_mask = tf.not_equal(tf.math.argmax(v_labels, axis=-1), 0)\r\n",
    "                v_logits, hidden_states = model(v_inputs, hidden_states, training=False)\r\n",
    "                v_loss = self.loss_fn(v_labels, v_logits)\r\n",
    "                self.metric.update_state(v_labels, v_logits, sample_weight=v_mask)\r\n",
    "            values = [\r\n",
    "                ('loss', t_loss), ('acc', t_acc), ('val_loss', v_loss),\r\n",
    "                ('val_acc', self.metric.result())]\r\n",
    "            progbar.update(self.args.train_steps, values=values, finalize=True)\r\n",
    "            self.metric.reset_states()\r\n",
    "        return model\r\n",
    "\r\n",
    "model = Trainer(args).train()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def load_dict(vocab_path):\r\n",
    "    dictionary = defaultdict(list)\r\n",
    "    with open(vocab_path, 'r', encoding='utf-8') as j:\r\n",
    "        data = list(json.load(j).keys())\r\n",
    "    for i, inputs in enumerate(data):\r\n",
    "        target, source = inputs.split(\"/\")\r\n",
    "        dictionary[source].append((target, i))\r\n",
    "    return dictionary\r\n",
    "\r\n",
    "def create_lattice(inputs, dictionary):\r\n",
    "    lattice = [[[] for _ in range(len(inputs) + 1)] for _ in range(len(inputs) + 2)]\r\n",
    "    unk_id = dictionary['<UNK>'][0][1]\r\n",
    "    for i in range(1, len(inputs) + 1):\r\n",
    "        for j in range(i):\r\n",
    "            key = inputs[j:i]\r\n",
    "            if key in dictionary:\r\n",
    "                for target, word_id in dictionary[key]:\r\n",
    "                    lattice[i][j].append((target, word_id))\r\n",
    "            elif len(key) == 1:\r\n",
    "                lattice[i][j].append((key, unk_id))\r\n",
    "\r\n",
    "    eos_id = dictionary['<EOS>'][0][1]\r\n",
    "    lattice[-1][-1].append(('', eos_id))\r\n",
    "    return lattice\r\n",
    "\r\n",
    "def initalize_queues(lattice, model, dictionary):\r\n",
    "    bos_id = dictionary['<BOS>'][0][1]\r\n",
    "    inputs = tf.expand_dims([bos_id], axis=0)\r\n",
    "    hidden_states = model.initialize_hidden_state()  \r\n",
    "    bos_pred = model(inputs, hidden_states, training=False)\r\n",
    "\r\n",
    "    bos_pred = tf.squeeze(bos_pred, axis=0)\r\n",
    "    bos_pred = tf.squeeze(bos_pred, axis=0)\r\n",
    "    bos_pred = -1 * tf.nn.log_softmax(bos_pred, axis=0)\r\n",
    "\r\n",
    "    hidden_states = tf.expand_dims(hidden_states, axis=0)\r\n",
    "    bos_hypothesis = (0.0, '', hidden_states[0], bos_pred)\r\n",
    "    queues = [[] for _ in range(len(lattice))]\r\n",
    "    queues[0].append(bos_hypothesis)\r\n",
    "    return queues\r\n",
    "\r\n",
    "def search(lattice, queues, model, beam_size, viterbi_size):\r\n",
    "    for i in range(len(lattice)):\r\n",
    "        queue = []\r\n",
    "\r\n",
    "        for j in range(len(lattice[i])):\r\n",
    "            for target, word_id in lattice[i][j]:\r\n",
    "                word_queue = []\r\n",
    "                for prev_cost, prev_str, prev_states, prev_pred in queues[j]:\r\n",
    "                    cost = prev_cost + prev_pred[word_id]\r\n",
    "                    string = prev_str + target\r\n",
    "                    hypothesis = (cost, string, word_id, prev_states)\r\n",
    "                    word_queue.append(hypothesis)\r\n",
    "                if viterbi_size > 0:\r\n",
    "                    word_queue = heapq.nsmallest(\r\n",
    "                        viterbi_size, word_queue, key=operator.itemgetter(0))\r\n",
    "                queue += word_queue\r\n",
    "\r\n",
    "        if beam_size > 0:\r\n",
    "            queue = heapq.nsmallest(beam_size, queue, key=operator.itemgetter(0))\r\n",
    "        \r\n",
    "        for cost, string, word_id, prev_states in queue:\r\n",
    "            inputs = tf.expand_dims([word_id], axis=0)\r\n",
    "            pred, hidden_states = model(inputs, [prev_states], training=False)\r\n",
    "\r\n",
    "            pred = tf.squeeze(pred, axis=0)\r\n",
    "            pred = tf.squeeze(pred, axis=0)\r\n",
    "            pred = -1 * tf.nn.log_softmax(pred, axis=0)\r\n",
    "\r\n",
    "            hidden_states = tf.expand_dims(hidden_states, axis=0)\r\n",
    "            hypothesis = (cost, string, hidden_states[0], pred)\r\n",
    "            queues[i].append(hypothesis)\r\n",
    "    return queues\r\n",
    "\r\n",
    "vocab_path = f\"{args.main_dir}/vocab_dict.json\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# idx = 0\r\n",
    "# data = next(iter(val))\r\n",
    "# y_pred = model.predict(data[0])[idx]\r\n",
    "# y_pred = tf.argmax(y_pred, axis=-1)\r\n",
    "# y_true = tf.argmax(data[1][idx], axis=-1)\r\n",
    "# print(y_pred, y_true)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hira = list(\"このまえさがったときはとちゆうにはんこんのりゆうきがあったのでついそこがいきどまりだとばかりおもってああいったんですが\")\r\n",
    "kanji = \"この前探った時は途中に瘢痕の隆起があったのでついそこが行きどまりだとばかり思って、ああ云ったんですが\""
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('tf-gpu': conda)"
  },
  "interpreter": {
   "hash": "b851d2923cfa3a2562599062e05fd9893d86a7c009c64d8ad3756552e4dd5f41"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}