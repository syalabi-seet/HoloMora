{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import cutlet\n",
    "import jiwer\n",
    "import argparse\n",
    "import MeCab\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from convert_romaji import Romaji2Kana\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sacrebleu.metrics import BLEU, CHRF\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "from transformers import (\n",
    "    MarianConfig,\n",
    "    MarianTokenizer,\n",
    "    AutoTokenizer,\n",
    "    TFMarianMTModel,\n",
    "    logging)\n",
    "\n",
    "# policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "# tf.keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "def seed_everything(SEED):\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    print(\"Random seed set.\")\n",
    "\n",
    "seed_everything(42)\n",
    "tf.get_logger().setLevel('FATAL')\n",
    "logging.set_verbosity_error()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_kanji(sentence):\n",
    "    symbols = r\"\\（.*\\）|\\(.*\\)|\\「.*\\」|\\『.*\\』\"\n",
    "    sentence = re.sub(symbols, \"\", sentence.strip())\n",
    "    return sentence\n",
    "\n",
    "def get_kanji_unicode():\n",
    "        vocab = set()\n",
    "        with open(\"E:\\Datasets\\Language_model\\kanji_unicode.txt\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                for char in line.split()[1:]:\n",
    "                    vocab.add(char)\n",
    "        return \"|\".join(sorted(vocab))\n",
    "\n",
    "def clean_kanji(sentence):\n",
    "        sentence = \"\".join(sentence.split())\n",
    "        pattern = f\"[^{kanji_unicode}]\"\n",
    "        sentence = re.sub(pattern, \"\", sentence)\n",
    "        return sentence\n",
    "\n",
    "def clean_en(sentence):\n",
    "    sentence = re.sub(r\"\\(.*\\)|\\[.*\\]|\\{.*\\}\", \"\", sentence.strip().lower()) # Parenthesis\n",
    "    sentence = re.sub(r\"\\.{3,}\", \",\", sentence) # Ellipsis\n",
    "    sentence = re.sub(r\"[^A-Za-z0-9\\ \\?\\.\\!\\,\\'\\\"]\", \"\", sentence) # Non alphanumeric\n",
    "    return sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_dir = \"D:\\School-stuff\\Sem-2\\PR-Project\\HoloASR\\Datasets\"\n",
    "# opus_ja_paths = glob.glob(f\"{main_dir}\\OPUS100-dataset\\*.ja\")\n",
    "# tatoeba_ja_paths = glob.glob(f\"{main_dir}\\Tatoeba-dataset\\*.ja\")\n",
    "# jesc_path = f\"{main_dir}/JESC-dataset/raw\"\n",
    "# ja_paths = opus_ja_paths + tatoeba_ja_paths\n",
    "# kanji_unicode = get_kanji_unicode()\n",
    "\n",
    "# ja_lines, en_lines = [], []\n",
    "# for ja_path in ja_paths:\n",
    "#     if ja_path.endswith(\".ja\"):\n",
    "#         en_path = ja_path.rsplit(\".\", 1)[0] + \".en\"\n",
    "#     else:\n",
    "#         en_path = ja_path.replace(\"ja\", \"en\")\n",
    "#     with open(ja_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         lines = [line.strip(\"\\n\") for line in f.readlines()]\n",
    "#         ja_lines.extend(lines)\n",
    "#     with open(en_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         lines = [line.strip(\"\\n\") for line in f.readlines()]\n",
    "#         en_lines.extend(lines)\n",
    "\n",
    "# with open(jesc_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     texts = [text.split(\"\\t\") for text in f.readlines()]\n",
    "#     for en, ja in tqdm(texts):\n",
    "#         ja_lines.append(ja)\n",
    "#         en_lines.append(en)\n",
    "\n",
    "# tqdm.pandas()\n",
    "# data = pd.DataFrame({'ja_raw': ja_lines, 'en': en_lines})\n",
    "# data['ja_raw'] = data['ja_raw'].progress_apply(clean_kanji)\n",
    "# data['en'] = data['en'].progress_apply(clean_en)\n",
    "# data = data.dropna().reset_index(drop=True)\n",
    "# data.to_csv(\n",
    "#     r\"E:\\Datasets\\Language_model\\tokenizer_text.csv\",\n",
    "#     index=False, encoding=\"utf-8\")\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train SPM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_dir = \"E://Datasets/Language_model\"\n",
    "# data = pd.read_csv(f\"{main_dir}/tokenizer_text.csv\")\n",
    "# data = data.dropna().reset_index(drop=True)\n",
    "\n",
    "# with open(f\"{main_dir}/ja_text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     ja_lines = data['ja_raw'].apply(lambda x: x + \"\\n\").tolist()\n",
    "#     f.writelines(ja_lines)\n",
    "\n",
    "# with open(f\"{main_dir}\\en_text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     en_lines = data['en'].apply(lambda x: x + \"\\n\").tolist()\n",
    "#     f.writelines(en_lines)\n",
    "\n",
    "# ja_file = f\"{main_dir}/ja_text.txt\"\n",
    "# en_file = f\"{main_dir}/en_text.txt\"\n",
    "# ja_prefix = f\"{main_dir}/ja_spm\"\n",
    "# en_prefix = f\"{main_dir}/en_spm\"\n",
    "# vocab_prefix = f\"{main_dir}/vocab_spm\"\n",
    "\n",
    "# spm.SentencePieceTrainer.train(\n",
    "#     f\"--input={ja_file} --model_prefix={ja_prefix} --vocab_size={10358} \" \\\n",
    "#     \"--model_type=unigram --pad_id=0 --unk_id=2 --bos_id=-1 --eos_id=1 \" \\\n",
    "#     \"--pad_piece=<pad> --unk_piece=<unk> --eos_piece=</s>\")\n",
    "\n",
    "# spm.SentencePieceTrainer.train(\n",
    "#     f\"--input={en_file} --model_prefix={en_prefix} --vocab_size={50358} \" \\\n",
    "#     \"--model_type=unigram --pad_id=0 --unk_id=2 --bos_id=-1 --eos_id=1 \" \\\n",
    "#     \"--pad_piece=<pad> --unk_piece=<unk> --eos_piece=</s>\")\n",
    "\n",
    "# vocab = []\n",
    "# with open(f\"{main_dir}/ja_spm.vocab\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     for text in f.readlines():\n",
    "#         token, _ = text.split(\"\\t\")\n",
    "#         vocab.append(token)\n",
    "\n",
    "# with open(f\"{main_dir}/en_spm.vocab\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     for text in f.readlines()[3:]:\n",
    "#         token, _ = text.split(\"\\t\")\n",
    "#         vocab.append(token)\n",
    "\n",
    "# with open(f\"{main_dir}/vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     vocab = {k: v for v, k in enumerate(vocab)}\n",
    "#     json.dump(vocab, f, ensure_ascii=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_dir = \"E://Datasets/Language_model\"\n",
    "\n",
    "# tokenizer = MarianTokenizer(\n",
    "#     source_spm=f\"{main_dir}/ja_spm.model\",\n",
    "#     target_spm=f\"{main_dir}/en_spm.model\",\n",
    "#     source_lang=\"ja\",\n",
    "#     target_lang=\"en\",\n",
    "#     eos_token=\"</s>\",\n",
    "#     unk_token=\"<unk>\",\n",
    "#     pad_token=\"<pad>\",\n",
    "#     vocab=f\"{main_dir}/vocab.json\")\n",
    "\n",
    "# # tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-ja-en\")\n",
    "\n",
    "# tqdm.pandas()\n",
    "# data = pd.read_csv(f\"{main_dir}/tokenizer_text.csv\", encoding=\"utf-8\")\n",
    "# data = data.dropna().reset_index(drop=True)\n",
    "# data['ja_token'] = data['ja_raw'].progress_apply(lambda x: tokenizer(x).input_ids)\n",
    "# with tokenizer.as_target_tokenizer():\n",
    "#     data['en_token'] = data['en'].progress_apply(lambda x: tokenizer(x).input_ids)\n",
    "# data['ja_len'] = data['ja_token'].apply(len)\n",
    "# q1 = data['ja_len'].quantile(0.1)\n",
    "# q2 = data['ja_len'].quantile(0.9)\n",
    "# data = data[data['ja_len'].between(q1, q2)].reset_index(drop=True)\n",
    "# data.to_csv(f\"{main_dir}/marian_text.csv\", index=False)\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=32, buffer_size=512, epochs=15, learning_rate=0.0003, lr_max=0.0003, lr_min=1e-08, lr_start=1e-08, main_dir='E://Datasets/Language_model', n_cycles=0.5, n_samples=1000000, n_shards=10, n_train=800000, n_val=200000, random_state=42, sustain_epochs=0, test_size=0.2, train_steps=25000, val_steps=6250, warmup_epochs=3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ArgParser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--random_state\", default=42)\n",
    "    parser.add_argument(\"--main_dir\", default=\"E://Datasets/Language_model\")\n",
    "    parser.add_argument(\"--n_shards\", default=10)\n",
    "    parser.add_argument(\"--n_samples\", default=1000000)\n",
    "    parser.add_argument(\"--test_size\", default=0.2)\n",
    "    parser.add_argument(\"--batch_size\", default=32)\n",
    "    parser.add_argument(\"--buffer_size\", default=512)\n",
    "\n",
    "    # Scheduler\n",
    "    parser.add_argument(\"--epochs\", default=15)\n",
    "    parser.add_argument(\"--learning_rate\", default=3e-4)\n",
    "    parser.add_argument(\"--lr_start\", default=1e-8)\n",
    "    parser.add_argument(\"--lr_min\", default=1e-8)\n",
    "    parser.add_argument(\"--lr_max\", default=3e-4)\n",
    "    parser.add_argument(\"--n_cycles\", default=0.5)\n",
    "    parser.add_argument(\"--warmup_epochs\", default=3)\n",
    "    parser.add_argument(\"--sustain_epochs\", default=0)\n",
    "\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    n_train = int(args.n_samples * (1 - args.test_size))\n",
    "    n_val = int(args.n_samples * args.test_size)\n",
    "    train_steps = int(np.ceil(n_train / args.batch_size))\n",
    "    val_steps = int(np.ceil(n_val / args.batch_size))\n",
    "        \n",
    "    parser.add_argument(\"--n_train\", default=n_train)\n",
    "    parser.add_argument(\"--n_val\", default=n_val)\n",
    "    parser.add_argument(\"--train_steps\", default=train_steps)  \n",
    "    parser.add_argument(\"--val_steps\", default=val_steps)\n",
    "\n",
    "    return parser.parse_known_args()[0]\n",
    "\n",
    "args = ArgParser()\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFRWriter():\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.data = self.get_data()\n",
    "\n",
    "    def get_data(self):\n",
    "        tqdm.pandas()\n",
    "        data = pd.read_csv(\n",
    "            f\"{self.args.main_dir}/marian_text.csv\", encoding=\"utf-8\")\n",
    "        data['ja_token'] = data['ja_token'].progress_apply(ast.literal_eval)\n",
    "        data['en_token'] = data['en_token'].progress_apply(ast.literal_eval)\n",
    "        data = data.sample(\n",
    "            n=self.args.n_samples,\n",
    "            random_state=self.args.random_state,\n",
    "            ignore_index=True)\n",
    "        data = data.sort_values(by=\"ja_len\", ignore_index=True, ascending=True)\n",
    "        return data\n",
    "\n",
    "    def _bytes_feature(self, value):\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    def serialize_example(self, *args):\n",
    "        feature = {\n",
    "            'input_ids': self._bytes_feature(args[0]),\n",
    "            'attention_mask': self._bytes_feature(args[1]),\n",
    "            'label_ids': self._bytes_feature(args[2])}\n",
    "\n",
    "        example_proto = tf.train.Example(\n",
    "            features=tf.train.Features(feature=feature))\n",
    "        return example_proto.SerializeToString()\n",
    "\n",
    "    def get_shards(self):\n",
    "        skf = KFold(n_splits=self.args.n_shards, shuffle=False)\n",
    "        return [j for i,j in skf.split(self.data)]\n",
    "\n",
    "    def get_shard_data(self, samples):\n",
    "        for sample in samples:\n",
    "            input_ids = tf.convert_to_tensor(\n",
    "                self.data['ja_token'][sample], dtype=tf.int32)\n",
    "            attention_mask = tf.where(input_ids != 0, x=1, y=0)\n",
    "            label_ids = tf.convert_to_tensor(\n",
    "                self.data['en_token'][sample], dtype=tf.int32)\n",
    "            yield {\n",
    "                \"input_ids\": tf.io.serialize_tensor(input_ids),\n",
    "                \"attention_mask\": tf.io.serialize_tensor(attention_mask),\n",
    "                \"label_ids\": tf.io.serialize_tensor(label_ids)}\n",
    "\n",
    "    def write(self):\n",
    "        for shard, samples in tqdm(enumerate(self.get_shards()), total=self.args.n_shards):\n",
    "            with tf.io.TFRecordWriter(f\"{self.args.main_dir}/marian_tfrec/shard_{shard+1}.tfrec\") as f:\n",
    "                for sample in self.get_shard_data(samples):\n",
    "                    example = self.serialize_example(\n",
    "                        sample['input_ids'],\n",
    "                        sample['attention_mask'],\n",
    "                        sample['label_ids'])\n",
    "                    f.write(example)\n",
    "\n",
    "# TFRWriter(args).write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, args):\n",
    "        self.files = glob.glob(args.main_dir + \"/marian_tfrec/*.tfrec\")\n",
    "        self.args = args\n",
    "        self.AUTOTUNE = tf.data.AUTOTUNE\n",
    "        self.train_files, self.val_files = train_test_split(\n",
    "            self.files, test_size=args.test_size, shuffle=True, \n",
    "            random_state=args.random_state)\n",
    "        self.train = self.get_train()\n",
    "        self.val = self.get_val()\n",
    "\n",
    "    def read_tfrecord(self, example):\n",
    "        feature_description = {\n",
    "            'input_ids': tf.io.FixedLenFeature([], tf.string),\n",
    "            'attention_mask': tf.io.FixedLenFeature([], tf.string),\n",
    "            'label_ids': tf.io.FixedLenFeature([], tf.string)\n",
    "            }\n",
    "        \n",
    "        example = tf.io.parse_single_example(example, feature_description)\n",
    "        example['input_ids'] = tf.io.parse_tensor(\n",
    "            example['input_ids'], out_type=tf.int32)\n",
    "        example['attention_mask'] = tf.io.parse_tensor(\n",
    "            example['attention_mask'], out_type=tf.int32) \n",
    "        example['label_ids'] = tf.io.parse_tensor(\n",
    "            example['label_ids'], out_type=tf.int32)\n",
    "        return example\n",
    "\n",
    "    def load_dataset(self, files):\n",
    "        ignore_order = tf.data.Options()\n",
    "        ignore_order.experimental_deterministic = False\n",
    "        dataset = tf.data.TFRecordDataset(files)\n",
    "        dataset = dataset.with_options(ignore_order)\n",
    "        dataset = dataset.map(self.read_tfrecord, num_parallel_calls=self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_train(self):\n",
    "        dataset = self.load_dataset(self.train_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_ids': [None],\n",
    "                'attention_mask': [None],\n",
    "                'label_ids': [None]\n",
    "            },\n",
    "            padding_values={\n",
    "                'input_ids': tf.constant(0, dtype=tf.int32),\n",
    "                'attention_mask': tf.constant(0, dtype=tf.int32),\n",
    "                'label_ids': tf.constant(-100, dtype=tf.int32)\n",
    "            })        \n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_val(self):\n",
    "        dataset = self.load_dataset(self.val_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_ids': [None],\n",
    "                'attention_mask': [None],\n",
    "                'label_ids': [None]\n",
    "            },\n",
    "            padding_values={\n",
    "                'input_ids': tf.constant(0, dtype=tf.int32),\n",
    "                'attention_mask': tf.constant(0, dtype=tf.int32),\n",
    "                'label_ids': tf.constant(-100, dtype=tf.int32)\n",
    "            })\n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "# train = DataLoader(args).train\n",
    "# inputs = next(iter(train))\n",
    "# input_values = inputs['input_ids']\n",
    "# attention_mask = inputs['attention_mask']\n",
    "# labels = inputs['label_ids']\n",
    "# print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class BLEUMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"BLEU\", **kwargs):\n",
    "        super(BLEUMetric, self).__init__(name=name, **kwargs)\n",
    "        self.bleu = BLEU(lowercase=True, tokenize=\"ja-mecab\", trg_lang=\"ja\")\n",
    "        self.accumulator = self.add_weight(name=\"total_bleu\", initializer=\"zeros\")\n",
    "        self.counter = self.add_weight(name=\"counter\", initializer=\"zeros\")\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        try:\n",
    "            bleu_score = self.bleu.corpus_score(hypotheses=y_pred, references=y_true).score\n",
    "        except:\n",
    "            y_pred = [\".\" if x == \"\" else x for x in y_pred]\n",
    "            bleu_score = self.bleu.corpus_score(hypotheses=y_pred, references=y_true).score\n",
    "        self.accumulator.assign_add(bleu_score)\n",
    "        self.counter.assign_add(1)\n",
    "\n",
    "    def result(self):\n",
    "        return tf.math.divide_no_nan(self.accumulator, self.counter)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.accumulator.assign(0.0)\n",
    "        self.counter.assign(0.0)\n",
    "\n",
    "class WERMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"WER\", **kwargs):\n",
    "        super(WERMetric, self).__init__(name=name,  **kwargs)\n",
    "        self.accumulator = self.add_weight(name=\"total_wer\", initializer=\"zeros\")\n",
    "        self.counter = self.add_weight(name=\"wer_count\", initializer=\"zeros\")    \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        wer = jiwer.wer(y_true, y_pred)\n",
    "\n",
    "        # Add distance and number of batches to variables\n",
    "        self.accumulator.assign_add(wer)\n",
    "        self.counter.assign_add(1)\n",
    "\n",
    "    def result(self):\n",
    "        # Divides accumulated distance scores against number of batches passed,\n",
    "        # mimics mean reduction over batch\n",
    "        return tf.math.divide_no_nan(self.accumulator, self.counter)   \n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.accumulator.assign(0.0)\n",
    "        self.counter.assign(0.0)\n",
    "\n",
    "class CosineDecayWithWarmup(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "    def __call__(self, epoch):  \n",
    "        if epoch < self.args.warmup_epochs:\n",
    "            lr = ((self.args.lr_max - self.args.lr_start) / self.args.warmup_epochs) * epoch + self.args.lr_start\n",
    "        elif epoch < (self.args.warmup_epochs + self.args.sustain_epochs):\n",
    "            lr = self.args.lr_max\n",
    "        else:\n",
    "            progress = ((epoch - self.args.warmup_epochs - self.args.sustain_epochs) / \n",
    "            (self.args.epochs - self.args.warmup_epochs - self.args.sustain_epochs))\n",
    "            lr = (self.args.lr_max-self.args.lr_min) * (0.5 * (1.0 + tf.math.cos((22/7) * \n",
    "                self.args.n_cycles * 2.0 * progress)))\n",
    "            if self.args.lr_min is not None:\n",
    "                lr = tf.math.maximum(self.args.lr_min, lr)\n",
    "        return lr\n",
    "\n",
    "    def plot(self):\n",
    "        epochs = range(self.args.epochs+1)\n",
    "        lr = [self(epoch) for epoch in epochs]\n",
    "        plt.plot(epochs, lr)\n",
    "        plt.xlabel(\"learning_rate\")\n",
    "        plt.ylabel(\"epochs\")\n",
    "        plt.show()\n",
    "\n",
    "CosineDecayWithWarmup(args).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from epoch 0...\n",
      "Epoch 0/15: Learning rate @ 1.00e-08\n",
      " 5828/25000 [=====>........................] - ETA: 3:39:28 - loss: 4.3731 - bleu: 1.0443"
     ]
    },
    {
     "ename": "StagingError",
     "evalue": "in user code:\n\n    C:\\Users\\syala\\AppData\\Local\\Temp/ipykernel_9424/1185021734.py:13 update_state  *\n        bleu_score = self.bleu.corpus_score(hypotheses=y_pred, references=y_true).score\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sacrebleu\\metrics\\base.py:424 corpus_score  *\n        actual_score = self._aggregate_and_compute(stats)\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sacrebleu\\metrics\\bleu.py:311 _aggregate_and_compute  *\n        return self._compute_score_from_stats(sum_of_lists(stats))\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sacrebleu\\utils.py:256 sum_of_lists  *\n        size = len(lists[0])\n\n    IndexError: list index out of range\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStagingError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9424/651885573.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{save_path}/{self.model_name}_{epoch}.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m \u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9424/651885573.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     92\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m                 \u001b[0mt_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label_ids'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_logits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbleu_metric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_logits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m                 \u001b[0mt_bleu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbleu_metric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                 t_values = [\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\metrics_utils.py\u001b[0m in \u001b[0;36mdecorated\u001b[1;34m(metric_obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_context_for_symbolic_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m       \u001b[0mupdate_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_state_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mupdate_op\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# update_op will be None in eager execution.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m       \u001b[0mmetric_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36mupdate_state_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[0mcontrol_status\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[0mag_update_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj_update_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontrol_status\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mag_update_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdef_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFunction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    693\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 695\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    696\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    697\u001b[0m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStagingError\u001b[0m: in user code:\n\n    C:\\Users\\syala\\AppData\\Local\\Temp/ipykernel_9424/1185021734.py:13 update_state  *\n        bleu_score = self.bleu.corpus_score(hypotheses=y_pred, references=y_true).score\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sacrebleu\\metrics\\base.py:424 corpus_score  *\n        actual_score = self._aggregate_and_compute(stats)\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sacrebleu\\metrics\\bleu.py:311 _aggregate_and_compute  *\n        return self._compute_score_from_stats(sum_of_lists(stats))\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sacrebleu\\utils.py:256 sum_of_lists  *\n        size = len(lists[0])\n\n    IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.tokenizer = MarianTokenizer(\n",
    "            source_spm=f\"{args.main_dir}/ja_spm.model\",\n",
    "            target_spm=f\"{args.main_dir}/en_spm.model\",\n",
    "            source_lang=\"ja\",\n",
    "            target_lang=\"en\",\n",
    "            eos_token=\"</s>\",\n",
    "            unk_token=\"<unk>\",\n",
    "            pad_token=\"<pad>\",\n",
    "            vocab=f\"{args.main_dir}/vocab.json\")\n",
    "        \n",
    "        # self.tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-ja-en\")\n",
    "        self.train_dataset = DataLoader(args).train\n",
    "        self.val_dataset = DataLoader(args).val\n",
    "        schedule = CosineDecayWithWarmup(args)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(schedule)\n",
    "        self.bleu_metric = BLEUMetric()\n",
    "\n",
    "        self.model = TFMarianMTModel.from_pretrained(\n",
    "            \"Helsinki-NLP/opus-mt-ja-en\",\n",
    "            bad_words_ids=[[0]],\n",
    "            bos_token_id=1,\n",
    "            eos_token_id=1,\n",
    "            pad_token_id=0,\n",
    "            decoder_start_token_id=0,\n",
    "            from_pt=True,\n",
    "            use_cache=False\n",
    "        )\n",
    "\n",
    "        self.model_name = f\"model_{int(self.args.n_samples/1000)}k\"\n",
    "        self.log_path = f\"{self.args.main_dir}/model_weights/{self.model_name}.csv\"\n",
    "        if not os.path.exists(self.log_path):\n",
    "            print(\"Log file created.\")\n",
    "            columns = \"epoch,loss,bleu,val_loss,val_bleu\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(columns)\n",
    "\n",
    "    def decoder(self, labels, logits):\n",
    "        labels = tf.where(labels == -100, x=0, y=labels)\n",
    "        labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        logits = tf.argmax(logits, axis=-1)\n",
    "        logits = self.tokenizer.batch_decode(logits, skip_special_tokens=True)\n",
    "        return labels, logits\n",
    "\n",
    "    def display(self, t_labels, t_logits, v_labels, v_logits):\n",
    "        print(\"-\" * 129)\n",
    "        print(\"Training\")\n",
    "        for y_true, y_pred in zip(t_labels, t_logits):\n",
    "            print(f\"Target:    {y_true}\")\n",
    "            print(f\"Predicted: {y_pred}\") \n",
    "\n",
    "        print(\"\\nValidation\")\n",
    "        for y_true, y_pred in zip(v_labels, v_logits):\n",
    "            print(f\"Target:    {y_true}\")\n",
    "            print(f\"Predicted: {y_pred}\")\n",
    "        print(\"-\" * 129)\n",
    "        \n",
    "    def fit(self):\n",
    "        # Checkpointing\n",
    "        self.ckpt_dir = f\"{self.args.main_dir}/checkpoints\"\n",
    "        self.ckpt = tf.train.Checkpoint(self.model)\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(\n",
    "            checkpoint=self.ckpt, directory=self.ckpt_dir, max_to_keep=5)\n",
    "\n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            self.start_epoch = int(self.ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "            self.ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "            print(f\"Resuming from epoch {self.start_epoch}...\")\n",
    "        else:\n",
    "            self.start_epoch = 0\n",
    "            print(\"Starting from epoch 0...\")\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.args.epochs):\n",
    "            print(f\"Epoch {epoch}/{self.args.epochs}: Learning rate @ {self.optimizer.lr(epoch):.2e}\")\n",
    "            stateful_metrics = [\"loss\", \"bleu\", \"val_loss\", \"val_bleu\"]\n",
    "            progbar = tf.keras.utils.Progbar(\n",
    "                self.args.train_steps, interval=0.05,\n",
    "                stateful_metrics=stateful_metrics)\n",
    "\n",
    "            # Training loop\n",
    "            for step, t_batch in enumerate(self.train_dataset):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    t_loss, t_logits = self.model(\n",
    "                        input_ids=t_batch['input_ids'],\n",
    "                        attention_mask=t_batch['attention_mask'],\n",
    "                        labels=t_batch['label_ids'],\n",
    "                        training=True)[:2]              \n",
    "                \n",
    "                gradients = tape.gradient(t_loss, self.model.trainable_weights)\n",
    "                self.optimizer.apply_gradients(zip(gradients, self.model.trainable_weights))\n",
    "                t_labels, t_logits = self.decoder(t_batch['label_ids'], t_logits)\n",
    "                self.bleu_metric.update_state(t_labels, t_logits)\n",
    "                t_bleu = self.bleu_metric.result()\n",
    "                t_values = [\n",
    "                    (\"loss\", tf.reduce_mean(t_loss)),\n",
    "                    (\"bleu\", t_bleu)]\n",
    "                progbar.update(step, values=t_values, finalize=False)\n",
    "            self.bleu_metric.reset_state()\n",
    "\n",
    "            # Validation loop\n",
    "            for v_batch in self.val_dataset:\n",
    "                v_loss, v_logits = self.model(\n",
    "                    input_ids=v_batch['input_ids'],\n",
    "                    attention_mask=v_batch['attention_mask'],\n",
    "                    labels=v_batch['label_ids'],\n",
    "                    training=False)[:2]\n",
    "                v_labels, v_logits = self.decoder(v_batch['label_ids'], v_logits)\n",
    "                self.bleu_metric.update_state(v_labels, v_logits)\n",
    "            \n",
    "            v_bleu = self.bleu_metric.result()\n",
    "            v_values = [\n",
    "                (\"loss\", tf.reduce_mean(t_loss)),\n",
    "                (\"bleu\", t_bleu),\n",
    "                (\"val_loss\", tf.reduce_mean(v_loss)),\n",
    "                (\"val_bleu\", v_bleu)]\n",
    "            progbar.update(self.args.train_steps, values=v_values, finalize=True)\n",
    "            self.bleu_metric.reset_state()\n",
    "\n",
    "            # Print sample transcriptions for both loops\n",
    "            self.display(t_labels, t_logits, v_labels, v_logits)\n",
    "\n",
    "            # Checkpointing\n",
    "            self.ckpt.save(file_prefix=f\"{self.ckpt_dir}/{self.model_name}\")\n",
    "\n",
    "            # Logging\n",
    "            log = f\"{epoch+1},{tf.reduce_mean(t_loss)},{t_bleu},{tf.reduce_mean(v_loss)},{v_bleu}\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(log)\n",
    "\n",
    "            save_path = f\"{self.args.main_dir}/model_weights\"\n",
    "            self.model.save_weights(f\"{save_path}/{self.model_name}_{epoch}.h5\")\n",
    "\n",
    "Trainer(args).fit()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b851d2923cfa3a2562599062e05fd9893d86a7c009c64d8ad3756552e4dd5f41"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('tf-gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
