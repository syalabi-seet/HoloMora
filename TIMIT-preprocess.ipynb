{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "import re\r\n",
    "import glob\r\n",
    "import json\r\n",
    "import subprocess\r\n",
    "import soundfile as sf\r\n",
    "import argparse\r\n",
    "import warnings\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import librosa\r\n",
    "from tqdm.notebook import tqdm\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "from collections import Counter\r\n",
    "from functools import partial\r\n",
    "\r\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\r\n",
    "\r\n",
    "import tensorflow as tf\r\n",
    "import tensorflow_io as tfio\r\n",
    "import tensorflow_addons as tfa\r\n",
    "\r\n",
    "from tensorflow.keras import Model, Sequential\r\n",
    "from tensorflow.keras.layers import *\r\n",
    "from tensorflow.keras.utils import plot_model\r\n",
    "\r\n",
    "warnings.filterwarnings(\"ignore\")\r\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\r\n",
    "main_dir = r\"Datasets\\TIMIT-dataset\\data\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "wav_paths = [path for path in glob.glob(main_dir + \"/*/*/*.wav\") \r\n",
    "    if not (path.endswith(\"_1.wav\") or path.endswith(\"_2.wav\"))]\r\n",
    "frames = [len(librosa.load(wav_path, sr=16000)[0]) for wav_path in wav_paths]\r\n",
    "print(\"Max frames:\", np.max(frames))\r\n",
    "\r\n",
    "plt.figure(figsize=(10,4))\r\n",
    "sns.histplot(data=frames)\r\n",
    "plt.axvline(x=np.mean(frames), color='red')\r\n",
    "plt.xlabel(\"Number of frames per sample\")\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clip non-verbal units"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def Clip(wav_paths, clip_buffer=320):\r\n",
    "    for wav_path in tqdm(wav_paths, desc='Clip'):\r\n",
    "        phn_path = wav_path.replace(\".wav\", \".phn\")\r\n",
    "        lines = open(phn_path, \"r\").readlines()\r\n",
    "        y = librosa.load(wav_path, sr=16000)[0]\r\n",
    "        y_1 = len(y)\r\n",
    "\r\n",
    "        # First line\r\n",
    "        start, end, phoneme = lines[0].split()\r\n",
    "        if phoneme == \"h#\":\r\n",
    "            if int(end) > clip_buffer:\r\n",
    "                clip_factor = int(end) - clip_buffer\r\n",
    "                y = y[clip_factor::]\r\n",
    "                for i, line in enumerate(lines):\r\n",
    "                    start, end, phoneme = line.split()\r\n",
    "                    start = int(start) - clip_factor\r\n",
    "                    end = int(end) - clip_factor\r\n",
    "                    lines[i] = f\"{max(0, start)} {end} {phoneme}\\n\"\r\n",
    "\r\n",
    "        # Middle lines\r\n",
    "        phonemes = np.array([line.split()[-1] for line in lines])\r\n",
    "        cond_1 = (phonemes == \"pau\") | (phonemes == \"sil\") | (phonemes == \"epi\")\r\n",
    "        for i in np.where(cond_1)[0]:\r\n",
    "            start, end, phoneme = lines[i].split()\r\n",
    "            start, end = int(start), int(end)\r\n",
    "            delta = end - start\r\n",
    "            clip_factor = delta - clip_buffer\r\n",
    "            if delta >= clip_buffer:\r\n",
    "                new_end = int(end) - clip_factor\r\n",
    "                lines[i] = f\"{start} {new_end} {phoneme}\\n\"\r\n",
    "                y = np.concatenate((y[:new_end], y[end:]), axis=0)\r\n",
    "                for j in range(i+1, len(lines)):\r\n",
    "                    start, end, phoneme = lines[j].split()\r\n",
    "                    start = int(start) - clip_factor\r\n",
    "                    end = int(end) - clip_factor\r\n",
    "                    lines[j] = f\"{start} {end} {phoneme}\\n\"\r\n",
    "\r\n",
    "        # Last line\r\n",
    "        start, end, phoneme = lines[-1].split()\r\n",
    "        if phoneme == \"h#\":\r\n",
    "            delta = int(end) - int(start)\r\n",
    "            if delta > clip_buffer:\r\n",
    "                end = int(start) + clip_buffer\r\n",
    "                lines[-1] = f\"{start} {end} {phoneme}\\n\"\r\n",
    "                y = y[:end]\r\n",
    "    \r\n",
    "        with open(phn_path, \"w\") as f:\r\n",
    "            f.writelines(lines)\r\n",
    "\r\n",
    "        sf.write(wav_path, y, 16000)\r\n",
    "\r\n",
    "Clip(wav_paths)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "wav_paths = [path for path in glob.glob(main_dir + \"/*/*/*.wav\") \r\n",
    "    if not (path.endswith(\"_1.wav\") or path.endswith(\"_2.wav\"))]\r\n",
    "frames = [len(librosa.load(wav_path, sr=16000)[0]) for wav_path in wav_paths]\r\n",
    "print(\"Max frames:\", np.max(frames))\r\n",
    "\r\n",
    "plt.figure(figsize=(10,4))\r\n",
    "sns.histplot(data=frames)\r\n",
    "plt.axvline(x=np.mean(frames), color='red')\r\n",
    "plt.xlabel(\"Number of frames per sample\")\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Discretize phoneme classes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def Discretize(wav_paths):\r\n",
    "    vowels = [\r\n",
    "        \"iy\", \"ih\", \"eh\", \"ay\", \"ae\", \"aa\", \"aw\", \"ay\", \"ah\", \"ao\", \"oy\", \"ow\",\r\n",
    "        \"uh\", \"uw\", \"er\", \"ax\", \"axr\"]\r\n",
    "    merge_dict = {\r\n",
    "        \"em\": \"m\", \"eng\": \"ng\", \"en\": \"n\", \"dx\": \"d\", \"nx\": \"n\", \"el\": \"l\", \r\n",
    "        \"hv\": \"hh\", \"ux\": \"uw\", \"ax-h\": \"ax\", \"ix\": \"ih\"}\r\n",
    "    silence = [\"sil\", \"pau\", \"epi\"]\r\n",
    "    stops = {\r\n",
    "        \"bcl\": \"b\", \"dcl\": \"d\", \"gcl\": \"g\", \"pcl\": \"p\", \"tck\": \"t\", \"kcl\": \"k\", \r\n",
    "        \"tcl\": \"ch\"}\r\n",
    "    phonemes = set()\r\n",
    "    for wav_path in tqdm(wav_paths, desc='Discretize'):\r\n",
    "        phn_path = wav_path.replace(\".wav\", \".phn\")\r\n",
    "        lines = open(phn_path, \"r\").readlines()\r\n",
    "        y = librosa.load(wav_path, sr=16000)[0]\r\n",
    "        for i, line in enumerate(lines):\r\n",
    "            phoneme = line.split()[-1]\r\n",
    "            if phoneme in silence:\r\n",
    "               lines[i] = lines[i].replace(phoneme, \"h#\") \r\n",
    "            if phoneme in merge_dict:\r\n",
    "                lines[i] = lines[i].replace(phoneme, merge_dict[phoneme])\r\n",
    "            if i != len(lines):\r\n",
    "                if phoneme in stops:\r\n",
    "                    phoneme_after = lines[i+1].split()[-1]\r\n",
    "                    if phoneme_after in (vowels + [\"h#\"]):\r\n",
    "                        lines[i] = lines[i].replace(phoneme, \"q\")\r\n",
    "                    else:\r\n",
    "                        lines[i] = lines[i].replace(phoneme, stops[phoneme])\r\n",
    "            phonemes.add(phoneme)\r\n",
    "\r\n",
    "        with open(phn_path, \"w\") as f:\r\n",
    "            f.writelines(lines)        \r\n",
    "    \r\n",
    "    print(\"Number of phonemes:\", len(phonemes))\r\n",
    "            \r\n",
    "Discretize(wav_paths)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Merge duplicated phoneme frames"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def Merge(wav_paths):\r\n",
    "    for wav_path in tqdm(wav_paths, desc='Merge'):\r\n",
    "        phn_path = wav_path.replace(\".wav\", \".phn\")\r\n",
    "        lines = open(phn_path, \"r\").readlines()\r\n",
    "        y = librosa.load(wav_path, sr=16000)[0]\r\n",
    "        for i, line in enumerate(lines):\r\n",
    "            if i != len(lines) - 1:\r\n",
    "                phoneme = line.split()[-1]\r\n",
    "                phoneme_after = lines[i+1].split()[-1]\r\n",
    "                if phoneme == phoneme_after:\r\n",
    "                    new_end = lines.pop(i+1).split()[1]\r\n",
    "                    start, end, phoneme = line.split()\r\n",
    "                    lines[i] = f\"{start} {new_end} {phoneme}\\n\"\r\n",
    "        \r\n",
    "        with open(phn_path, \"w\") as f:\r\n",
    "            f.writelines(lines)\r\n",
    "\r\n",
    "Merge(wav_paths)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def Split(wav_paths):\r\n",
    "    cutoff_limit = 60000\r\n",
    "    split_files, frames = [], []\r\n",
    "    for wav_path in tqdm(wav_paths, desc='Split'):\r\n",
    "        y = librosa.load(wav_path, sr=16000)[0]\r\n",
    "        n_frames = len(y)\r\n",
    "        frames.append(n_frames)\r\n",
    "        if n_frames >= cutoff_limit:\r\n",
    "            phn_path = wav_path.replace(\"wav\", \"phn\")\r\n",
    "            lines = open(phn_path, \"r\").readlines()\r\n",
    "            end = np.array([int(line.split()[1]) for line in lines])\r\n",
    "            mid_point = n_frames // 2\r\n",
    "            idx = np.where(end <= mid_point)[0][-1]\r\n",
    "            lines_1 = lines[:idx]\r\n",
    "            lines_2 = lines[idx:]\r\n",
    "            factor = int(lines_2[0].split()[0])\r\n",
    "            for i, line in enumerate(lines_2):\r\n",
    "                start, end, phoneme = line.split()\r\n",
    "                start = int(start) - factor\r\n",
    "                end = int(end) - factor\r\n",
    "                line = \" \".join([str(start), str(end), phoneme + \"\\n\"])\r\n",
    "                lines_2[i] = line\r\n",
    "            f1_path = os.path.splitext(phn_path)[0] + \"_1.phn\"\r\n",
    "            f2_path = os.path.splitext(phn_path)[0] + \"_2.phn\"\r\n",
    "            if (len(y[:factor]) > cutoff_limit) | (len(y[factor:]) > cutoff_limit):\r\n",
    "                print(lines_1, lines_2)\r\n",
    "            for path, lines in zip([f1_path, f2_path], [lines_1, lines_2]):\r\n",
    "                with open(path, \"w\") as f:\r\n",
    "                    f.writelines(lines)    \r\n",
    "            sf.write(f1_path.replace(\"phn\", \"wav\"), y[:factor], 16000)\r\n",
    "            sf.write(f2_path.replace(\"phn\", \"wav\"), y[factor:], 16000)\r\n",
    "            split_files.append(wav_path)\r\n",
    "\r\n",
    "    print(\"Number of splitted files:\", len(split_files))\r\n",
    "    return split_files\r\n",
    "\r\n",
    "split_files = Split(wav_paths)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "wav_paths = glob.glob(main_dir + \"/*/*/*.wav\")\r\n",
    "wav_paths = [path for path in wav_paths if not path in split_files]\r\n",
    "frames = []\r\n",
    "for wav_path in wav_paths:\r\n",
    "    y = librosa.load(wav_path, sr=16000)[0]\r\n",
    "    n_frames = len(y)\r\n",
    "    frames.append(n_frames)\r\n",
    "\r\n",
    "pd.DataFrame({'wav_paths': wav_paths}).to_csv(\"Datasets\\TIMIT-dataset\\data.csv\", index=False)\r\n",
    "\r\n",
    "print(\"Number of files:\", len(wav_paths))\r\n",
    "print(\"Max frames:\", np.max(frames))\r\n",
    "plt.figure(figsize=(10, 4))\r\n",
    "sns.histplot(data=frames)\r\n",
    "plt.axvline(x=np.mean(frames), color='red')\r\n",
    "plt.xlabel(\"Number of frames per sample\")\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Padding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# wav_paths = glob.glob(main_dir + \"/*/*/*.wav\")\r\n",
    "# wav_paths = [path for path in wav_paths if not path in split_files]\r\n",
    "    \r\n",
    "# for wav_path in tqdm(wav_paths, desc='Padding'):\r\n",
    "#     y = librosa.load(wav_path, sr=16000)[0]\r\n",
    "#     n_frames = len(y)\r\n",
    "#     y_sil = np.array([])\r\n",
    "#     if n_frames != cutoff_limit:\r\n",
    "#         pad_length = cutoff_limit - n_frames\r\n",
    "\r\n",
    "#         with open(wav_path.replace(\"wav\", \"phn\"), \"r\") as f:\r\n",
    "#             lines = f.readlines()\r\n",
    "#         phonemes = np.array([line.split()[-1] for line in lines])\r\n",
    "#         for i in np.where(phonemes == \"h#\")[0]:\r\n",
    "#             start_sil, end_sil = list(map(int, lines[i].split()[:-1]))\r\n",
    "#             y_sil = np.concatenate([y_sil, y[start_sil:end_sil]])\r\n",
    "        \r\n",
    "#         if len(y_sil) >= pad_length:\r\n",
    "#             y_sil = y_sil[:pad_length]\r\n",
    "#         else:\r\n",
    "#             extension = pad_length - len(y_sil)\r\n",
    "#             y_sil = np.pad(y_sil, (0, extension), mode='wrap')\r\n",
    "        \r\n",
    "#         np.random.shuffle(y_sil)\r\n",
    "#         y = np.concatenate([y, y_sil])\r\n",
    "        \r\n",
    "#         start, end, phoneme = lines.pop(-1).split()\r\n",
    "#         if lines[-1].split()[-1] == \"h#\":\r\n",
    "#             line = \" \".join([end, str(cutoff_limit), phoneme])\r\n",
    "#         else:\r\n",
    "#             line = \" \".join([start, str(cutoff_limit), \"h#\\n\"])\r\n",
    "#         lines.append(line)\r\n",
    "\r\n",
    "#         with open(wav_path.replace(\"wav\", \"phn\"), \"w\") as f:\r\n",
    "#             f.writelines(lines)\r\n",
    "\r\n",
    "#         sf.write(wav_path, y, 16000)\r\n",
    "\r\n",
    "#     else:\r\n",
    "#         pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "d = pd.read_csv(\"Datasets\\TIMIT-dataset\\data.csv\")\r\n",
    "d = d.sample(len(d)-2, random_state=42)\r\n",
    "d"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TFrecords\r\n",
    "\r\n",
    "## Write"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def ArgParser():\r\n",
    "    parser = argparse.ArgumentParser()\r\n",
    "    \r\n",
    "    parser.add_argument(\"--n_splits\", dest=\"n_splits\", type=int, default=5)\r\n",
    "    parser.add_argument(\"--n_samples\", dest=\"n_samples\", type=int, default=6950)\r\n",
    "    parser.add_argument(\"--n_classes\", dest=\"n_classes\", type=int, default=43)\r\n",
    "    parser.add_argument(\"--sample_rate\", dest=\"sample_rate\", type=int, default=16000)\r\n",
    "    parser.add_argument(\"--n_fft\", dest=\"n_fft\", type=int, default=2048)\r\n",
    "    parser.add_argument(\"--window_size\", dest=\"window_size\", type=int, default=320)\r\n",
    "    parser.add_argument(\"--hop_length\", dest=\"hop_length\", type=int, default=160) # 160 samples = 10ms\r\n",
    "    parser.add_argument(\"--n_mels\", dest=\"n_mels\", type=int, default=64)\r\n",
    "    parser.add_argument(\"--n_mfcc\", dest=\"n_mfcc\", type=int, default=13)\r\n",
    "    parser.add_argument(\"--batch_size\", dest=\"batch_size\", type=int, default=64)\r\n",
    "    parser.add_argument(\"--buffer_size\", dest=\"buffer_size\", type=int, default=1024)\r\n",
    "    parser.add_argument(\"--max_samples\", dest=\"max_samples\", type=int, default=60000)\r\n",
    "    parser.add_argument(\"--delta_width\", dest=\"delta_width\", type=int, default=3)\r\n",
    "    parser.add_argument(\"--learning_rate\", dest=\"learning_rate\", type=float, default=1e-3)\r\n",
    "    parser.add_argument(\"--epochs\", dest=\"epochs\", type=int, default=50)\r\n",
    "    parser.add_argument(\"--dropout\", dest=\"dropout\", type=float, default=0.4)\r\n",
    "    parser.add_argument(\"--type\", dest=\"type\", type=str, default=\"mfcc\")\r\n",
    "    parser.add_argument(\"--main_dir\", dest='main_dir', type=str, default=\"Datasets/TIMIT-dataset/tfrec_data\")\r\n",
    "    \r\n",
    "    args = parser.parse_known_args()[0]\r\n",
    "    test_size = (1 / args.n_splits)\r\n",
    "    seq_len = int(np.ceil(args.max_samples / args.hop_length))\r\n",
    "    if args.type == \"mel\":\r\n",
    "        input_shape = (seq_len, args.n_mels, 1)\r\n",
    "    else:\r\n",
    "        input_shape = (seq_len, args.n_mfcc*3, 1)\r\n",
    "    n_train = int(args.n_samples * (1 - test_size))\r\n",
    "    train_steps = int(np.ceil(n_train / args.batch_size))\r\n",
    "    parser.add_argument(\"--test_size\", type=float, default=test_size)\r\n",
    "    parser.add_argument(\"--input_shape\", type=tuple, default=input_shape)\r\n",
    "    parser.add_argument(\"--seq_len\", type=int, default=seq_len)\r\n",
    "    parser.add_argument(\"--train_steps\", type=int, default=train_steps)\r\n",
    "    return parser.parse_known_args()[0]\r\n",
    "\r\n",
    "args = ArgParser()\r\n",
    "args"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class TFRWriter():\r\n",
    "    def __init__(self, args):\r\n",
    "        self.samples = d['wav_paths'].tolist()\r\n",
    "        self.args = args\r\n",
    "        self.dict_path = \"Datasets\\TIMIT-dataset\\phoneme_dict.json\"\r\n",
    "        self.phoneme_dict = self.get_dict()\r\n",
    "\r\n",
    "    def _bytes_feature(self, value):\r\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\r\n",
    "        if isinstance(value, type(tf.constant(0))):\r\n",
    "            value = value.numpy()\r\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\r\n",
    "\r\n",
    "    def _int64_feature(self, value):\r\n",
    "        \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\r\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\r\n",
    "\r\n",
    "    def serialize_example(self, *args):\r\n",
    "        feature = {\r\n",
    "            'audio': self._bytes_feature(args[0]),\r\n",
    "            'binary_labels': self._bytes_feature(args[1]),\r\n",
    "            'framewise_labels': self._bytes_feature(args[2]),\r\n",
    "            'attention_mask': self._bytes_feature(args[3]),\r\n",
    "            'true_length': self._int64_feature(args[4]),\r\n",
    "            'filename': self._bytes_feature(args[5])}\r\n",
    "\r\n",
    "        example_proto = tf.train.Example(\r\n",
    "            features=tf.train.Features(feature=feature))\r\n",
    "        return example_proto.SerializeToString()\r\n",
    "\r\n",
    "    def get_binary_labels(self, p_frames):\r\n",
    "        p_frames = tf.convert_to_tensor(p_frames)\r\n",
    "        labels = tf.tensor_scatter_nd_update(\r\n",
    "            tensor=tf.zeros([p_frames[-1]+1], dtype=tf.int32), \r\n",
    "            indices=tf.expand_dims(p_frames[1:], axis=1), \r\n",
    "            updates=tf.ones([p_frames.shape[0]-1], dtype=tf.int32))\r\n",
    "        padding = tf.zeros([self.args.seq_len-len(labels)], dtype=tf.int32)\r\n",
    "        labels = tf.concat([labels, padding], axis=0)\r\n",
    "        labels = tf.cast(labels, dtype=tf.float32)\r\n",
    "        return labels        \r\n",
    "\r\n",
    "    def get_framewise_labels(self, p_frames, phonemes):\r\n",
    "        labels = []\r\n",
    "        for i in range(1, len(p_frames)):\r\n",
    "            for j in range(p_frames[i-1], p_frames[i]):\r\n",
    "                labels.append(phonemes[i-1])\r\n",
    "        labels = tf.convert_to_tensor(labels)\r\n",
    "        padding = tf.zeros([self.args.seq_len-len(labels)], dtype=tf.int32)\r\n",
    "        labels = tf.concat([labels, padding], axis=0)\r\n",
    "        labels = tf.one_hot(labels, depth=self.args.n_classes, dtype=tf.int32)\r\n",
    "        return labels\r\n",
    "\r\n",
    "    def get_attention_mask(self, p_frames):\r\n",
    "        mask = tf.convert_to_tensor(\r\n",
    "            [True if i < p_frames[-1] else False for i in range(self.args.seq_len)])\r\n",
    "        return mask\r\n",
    "\r\n",
    "    def get_shards(self):\r\n",
    "        speaker_id = [sample.split('\\\\')[4] for sample in self.samples]\r\n",
    "        skf = StratifiedKFold(\r\n",
    "            n_splits=self.args.n_splits, shuffle=True, random_state=42)\r\n",
    "        return [\r\n",
    "            list(map(lambda x: self.samples[x], j)) \r\n",
    "            for i, j in skf.split(self.samples, speaker_id)]\r\n",
    "\r\n",
    "    def get_dict(self):\r\n",
    "        phonemes = set()\r\n",
    "        markers = ['h#']\r\n",
    "        for sample in self.samples:\r\n",
    "            base_path = os.path.splitext(sample)[0]\r\n",
    "            with open(base_path + '.phn', \"r\") as f:\r\n",
    "                for line in f.readlines():\r\n",
    "                    phoneme = line.split()[-1]\r\n",
    "                    if not phoneme in markers:\r\n",
    "                        phonemes.add(phoneme)\r\n",
    "        phonemes = markers + sorted(Counter(phonemes), key=Counter(phonemes).get, reverse=True)\r\n",
    "        phonemes_dict = {v: i+1 for i, v in enumerate(phonemes)}\r\n",
    "        with open(self.dict_path, \"w\") as f:\r\n",
    "            json.dump(phonemes_dict, f, sort_keys=False, indent=4)\r\n",
    "        return phonemes_dict \r\n",
    "\r\n",
    "    def get_audio(self, wav_path, feature_type):\r\n",
    "        y, sr = librosa.load(wav_path, sr=self.args.sample_rate)\r\n",
    "        y = librosa.util.fix_length(y, self.args.max_samples)\r\n",
    "        if feature_type == \"mfcc\":\r\n",
    "            mfcc = librosa.feature.mfcc(\r\n",
    "                y=y, sr=sr, n_mfcc=self.args.n_mfcc, hop_length=self.args.hop_length,\r\n",
    "                win_length=self.args.window_size, n_mels=self.args.n_mels,\r\n",
    "                n_fft=self.args.n_fft, fmin=0, fmax=8000)[:, :self.args.seq_len]\r\n",
    "            mfcc = np.transpose(mfcc)\r\n",
    "            delta = librosa.feature.delta(\r\n",
    "                mfcc, width=self.args.delta_width, order=1, axis=0)\r\n",
    "            delta2 = librosa.feature.delta(\r\n",
    "                mfcc, width=self.args.delta_width, order=2, axis=0)\r\n",
    "            mfcc = np.concatenate((mfcc, delta, delta2), axis=-1)\r\n",
    "            return mfcc\r\n",
    "        else:\r\n",
    "            mel_spectrogram = librosa.feature.melspectrogram(\r\n",
    "                y=y, sr=sr, n_fft=self.args.n_fft, \r\n",
    "                hop_length=self.args.hop_length, win_length=self.args.window_size, \r\n",
    "                n_mels=self.args.n_mels, fmin=0, fmax=8000)\r\n",
    "            mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\r\n",
    "            mel_spectrogram = np.transpose(mel_spectrogram)[:self.args.seq_len, :]\r\n",
    "            return mel_spectrogram\r\n",
    "        \r\n",
    "    def get_shard_data(self, samples, shard):\r\n",
    "        for sample in tqdm(\r\n",
    "                samples, total=len(samples), desc=f\"Writing shard {shard}\"):\r\n",
    "            base_path = os.path.splitext(sample)[0]\r\n",
    "            p_frames, phonemes = [0], []\r\n",
    "            with open(base_path + \".phn\") as f:\r\n",
    "                for line in f.readlines():\r\n",
    "                    p_frame, phoneme = line.split()[1::]\r\n",
    "                    p_frames.append(int(p_frame) // self.args.hop_length)\r\n",
    "                    phonemes.append(str(phoneme))\r\n",
    "            phonemes = list(map(self.phoneme_dict.get, phonemes))\r\n",
    "            binary_labels = self.get_binary_labels(p_frames)\r\n",
    "            framewise_labels = self.get_framewise_labels(p_frames, phonemes)\r\n",
    "            audio = self.get_audio(base_path + \".wav\", self.args.type)\r\n",
    "            spec_mask = self.get_attention_mask(p_frames)\r\n",
    "            filename = str.encode(\"/\".join(sample.split('\\\\')[-3::]))\r\n",
    "            yield {\r\n",
    "                \"audio\": tf.io.serialize_tensor(audio),\r\n",
    "                \"binary_labels\": tf.io.serialize_tensor(binary_labels),\r\n",
    "                \"framewise_labels\": tf.io.serialize_tensor(framewise_labels),\r\n",
    "                \"attention_mask\": tf.io.serialize_tensor(spec_mask),\r\n",
    "                \"true_length\": p_frames[-1],\r\n",
    "                \"filename\": filename}\r\n",
    "\r\n",
    "    def write(self):\r\n",
    "        for shard, samples in enumerate(self.get_shards()):\r\n",
    "            with tf.io.TFRecordWriter(\r\n",
    "                    f\"Datasets/TIMIT-dataset/tfrec_data/train_{shard+1}.tfrec\") as f:\r\n",
    "                for sample in self.get_shard_data(samples, shard+1):\r\n",
    "                    example = self.serialize_example(\r\n",
    "                        sample['audio'], sample['binary_labels'], \r\n",
    "                        sample['framewise_labels'], sample['attention_mask'],\r\n",
    "                        sample['true_length'], sample['filename'])\r\n",
    "                    f.write(example)\r\n",
    "\r\n",
    "TFRWriter(args).write()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class DataLoader():\r\n",
    "    def __init__(self, args):\r\n",
    "        self.files = [os.path.join(args.main_dir, f) for f in os.listdir(args.main_dir)]\r\n",
    "        self.args = args\r\n",
    "        self.AUTOTUNE = tf.data.experimental.AUTOTUNE\r\n",
    "        self.train_files, self.val_files = train_test_split(\r\n",
    "            self.files, test_size=args.test_size, shuffle=True)\r\n",
    "        self.train = self.train()\r\n",
    "        self.val = self.val()\r\n",
    "\r\n",
    "    def decode_audio(self, string):\r\n",
    "        audio = tf.audio.decode_wav(string, desired_samples=self.args.max_samples)[0]\r\n",
    "        return tf.squeeze(audio, axis=-1)\r\n",
    "\r\n",
    "    def read_tfrecord(self, example):\r\n",
    "        feature_description = {\r\n",
    "            'audio': tf.io.FixedLenFeature([], tf.string),\r\n",
    "            'framewise_labels': tf.io.FixedLenFeature([], tf.string),\r\n",
    "            'binary_labels': tf.io.FixedLenFeature([], tf.string),\r\n",
    "            'attention_mask': tf.io.FixedLenFeature([], tf.string),\r\n",
    "            'filename': tf.io.FixedLenFeature([], tf.string)}\r\n",
    "        \r\n",
    "        example = tf.io.parse_single_example(example, feature_description)\r\n",
    "        example['audio'] = tf.io.parse_tensor(\r\n",
    "            example['audio'], out_type=tf.float32)\r\n",
    "        example['binary_labels'] = tf.io.parse_tensor(\r\n",
    "            example['binary_labels'], out_type=tf.float32)\r\n",
    "        example['framewise_labels'] = tf.io.parse_tensor(\r\n",
    "            example['framewise_labels'], out_type=tf.int32)\r\n",
    "        example['attention_mask'] = tf.io.parse_tensor(\r\n",
    "            example['attention_mask'], out_type=tf.bool)\r\n",
    "        return example\r\n",
    "\r\n",
    "    def load_dataset(self, files):\r\n",
    "        ignore_order = tf.data.Options()\r\n",
    "        ignore_order.experimental_deterministic = False\r\n",
    "        dataset = tf.data.TFRecordDataset(files)\r\n",
    "        dataset = dataset.with_options(ignore_order)\r\n",
    "        dataset = dataset.map(self.read_tfrecord, num_parallel_calls=self.AUTOTUNE)\r\n",
    "        return dataset\r\n",
    " \r\n",
    "    def SpecAugment(self, sample, training):\r\n",
    "        audio = sample['audio']\r\n",
    "        if training == True:\r\n",
    "            audio = tfio.audio.fade(audio, fade_in=50, fade_out=100, mode=\"logarithmic\")\r\n",
    "            audio = tfio.audio.freq_mask(audio, param=10)\r\n",
    "            audio = tfio.audio.time_mask(audio, param=10)\r\n",
    "        inputs = {\"audio\": audio, \"mask\": sample['attention_mask']}\r\n",
    "        outputs = {'labels': sample['framewise_labels']}\r\n",
    "        return inputs, outputs\r\n",
    "\r\n",
    "    def train(self):\r\n",
    "        dataset = self.load_dataset(self.train_files)\r\n",
    "        dataset = dataset.map(\r\n",
    "            partial(self.SpecAugment, training=True), \r\n",
    "            num_parallel_calls=self.AUTOTUNE)\r\n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\r\n",
    "        dataset = dataset.batch(self.args.batch_size)\r\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\r\n",
    "        return dataset\r\n",
    "\r\n",
    "    def val(self):\r\n",
    "        dataset = self.load_dataset(self.val_files)\r\n",
    "        dataset = dataset.map(\r\n",
    "            partial(self.SpecAugment, training=False), \r\n",
    "            num_parallel_calls=self.AUTOTUNE)\r\n",
    "        dataset = dataset.batch(self.args.batch_size)\r\n",
    "        dataset = dataset.cache()\r\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\r\n",
    "        return dataset\r\n",
    "\r\n",
    "train = DataLoader(args).train\r\n",
    "val = DataLoader(args).val\r\n",
    "inputs, outputs = next(iter(train))\r\n",
    "print(\"audio shape:\", inputs['audio'].shape)\r\n",
    "print(\"mask shape:\", inputs['mask'].shape)\r\n",
    "print(\"labels shape:\", outputs['labels'].shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SpecAugment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import librosa.display\r\n",
    "\r\n",
    "train = train.unbatch().batch(16)\r\n",
    "spectrogram = next(iter(train))[0]['audio']\r\n",
    "\r\n",
    "plt.figure(figsize=(10, 7))\r\n",
    "n_samples = spectrogram.shape[0]\r\n",
    "row = 4\r\n",
    "col = int(n_samples / 4)\r\n",
    "\r\n",
    "for i in range(n_samples):\r\n",
    "    plt.subplot(row, col, i+1)\r\n",
    "    plt.axis(\"off\")\r\n",
    "    librosa.display.specshow(\r\n",
    "        np.transpose(spectrogram[i].numpy()), \r\n",
    "        sr=args.sample_rate,\r\n",
    "        hop_length=args.hop_length)\r\n",
    "\r\n",
    "plt.tight_layout()\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def Segmentor(args):\r\n",
    "    spectrogram = Input(shape=args.input_shape, dtype=tf.float32, name='audio')\r\n",
    "    mask = Input(shape=args.input_shape[0], dtype=tf.bool, name='mask')\r\n",
    "\r\n",
    "    x = TimeDistributed(Conv1D(128, 3))(spectrogram)\r\n",
    "    x = TimeDistributed(ReLU())(x)\r\n",
    "    x = TimeDistributed(BatchNormalization())(x)\r\n",
    "    x = TimeDistributed(MaxPool1D(3))(x)\r\n",
    "\r\n",
    "    x = TimeDistributed(Conv1D(64, 3))(x)\r\n",
    "    x = TimeDistributed(ReLU())(x)\r\n",
    "    x = TimeDistributed(BatchNormalization())(x)\r\n",
    "    x = TimeDistributed(MaxPool1D(3))(x)\r\n",
    "    \r\n",
    "    x = TimeDistributed(Flatten())(x)\r\n",
    "    x = Bidirectional(LSTM(200, dropout=args.dropout, return_sequences=True))(x, mask=mask)\r\n",
    "    x = Bidirectional(LSTM(100, dropout=args.dropout, return_sequences=True))(x, mask=mask)\r\n",
    "    x = Bidirectional(LSTM(50, dropout=args.dropout, return_sequences=True))(x, mask=mask)\r\n",
    "    x = TimeDistributed(Dense(args.n_classes, activation='softmax'))(x, mask=mask)\r\n",
    "    return Model(inputs=[spectrogram, mask], outputs=x, name='Segmentor')\r\n",
    "\r\n",
    "model = Segmentor(args)\r\n",
    "model.summary()\r\n",
    "plot_model(model, show_shapes=True, show_layer_names=False, show_dtype=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\r\n",
    "    initial_learning_rate=args.learning_rate, \r\n",
    "    decay_steps=5,\r\n",
    "    decay_rate=0.25,\r\n",
    "    staircase=True)\r\n",
    "\r\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\r\n",
    "x_range = range(0, args.epochs)\r\n",
    "\r\n",
    "plt.figure(figsize=(10,5))\r\n",
    "plt.plot(x_range, [optimizer.lr(epoch) for epoch in x_range])\r\n",
    "plt.xticks(range(0, args.epochs, 10))\r\n",
    "plt.xlabel(\"Epochs\")\r\n",
    "plt.ylabel(\"learning_rate\")\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class SegmentorTrainer():\r\n",
    "    def __init__(self, args):\r\n",
    "        self.args = args\r\n",
    "        self.model = Segmentor(args)\r\n",
    "        self.dataloader = DataLoader(args)\r\n",
    "        schedule = tf.keras.optimizers.schedules.ExponentialDecay(\r\n",
    "            initial_learning_rate=args.learning_rate, \r\n",
    "            decay_steps=5,\r\n",
    "            decay_rate=0.25,\r\n",
    "            staircase=True)\r\n",
    "        self.optimizer = tf.keras.optimizers.Adam(\r\n",
    "            learning_rate=args.learning_rate)\r\n",
    "\r\n",
    "        self.loss_fn = tf.keras.losses.CategoricalCrossentropy(\r\n",
    "            from_logits=False, label_smoothing=0.1)\r\n",
    "        self.metric = tfa.metrics.F1Score(\r\n",
    "            num_classes=args.n_classes, average='micro')\r\n",
    "    \r\n",
    "    def train(self):\r\n",
    "        for epoch in range(self.args.epochs):\r\n",
    "            stateful_metrics = ['loss', 'f1', 'val_loss', 'val_f1']\r\n",
    "            print(f\"Epoch {epoch+1}/{self.args.epochs}: Learning rate @ {optimizer.lr(epoch):.2e}\")\r\n",
    "            progbar = tf.keras.utils.Progbar(\r\n",
    "                self.args.train_steps, interval=0.05,\r\n",
    "                stateful_metrics=stateful_metrics)\r\n",
    "            for step, (X_train, y_train) in enumerate(self.dataloader.train):\r\n",
    "                labels = y_train['labels']\r\n",
    "                t_mask = tf.cast(X_train['mask'], dtype=tf.float32)\r\n",
    "                t_mask = tf.tile(\r\n",
    "                    tf.expand_dims(t_mask, axis=-1), [1,1,self.args.n_classes])\r\n",
    "                with tf.GradientTape() as tape:\r\n",
    "                    t_logits = model(X_train, training=True)\r\n",
    "                    t_loss = self.loss_fn(labels, t_logits)\r\n",
    "                grads = tape.gradient(t_loss, model.trainable_weights)\r\n",
    "                self.optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n",
    "                self.metric.update_state(labels, t_logits, sample_weight=t_mask)\r\n",
    "                t_f1 = self.metric.result()\r\n",
    "                progbar.update(\r\n",
    "                    step, values=[('loss', t_loss), ('f1', t_f1)],\r\n",
    "                    finalize=False)\r\n",
    "                self.metric.reset_states()\r\n",
    "\r\n",
    "            for X_val, y_val in self.dataloader.val:\r\n",
    "                labels = y_val['labels']\r\n",
    "                v_mask = tf.cast(X_val['mask'], dtype=tf.float32)\r\n",
    "                v_mask = tf.tile(\r\n",
    "                    tf.expand_dims(v_mask, axis=-1), [1,1,self.args.n_classes])\r\n",
    "                v_logits = model(X_val, training=False)\r\n",
    "                v_loss = self.loss_fn(labels, v_logits)\r\n",
    "                self.metric.update_state(labels, v_logits, sample_weight=v_mask)\r\n",
    "            \r\n",
    "            values = [\r\n",
    "                ('loss', t_loss), ('f1', t_f1),\r\n",
    "                ('val_loss', v_loss), ('val_f1', self.metric.result())]\r\n",
    "            progbar.update(self.args.train_steps, values=values, finalize=True)\r\n",
    "            self.metric.reset_states()\r\n",
    "        return model\r\n",
    "\r\n",
    "model = SegmentorTrainer(args).train()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_pred = model.predict(val)[0]\r\n",
    "y_true = next(iter(val))[1]['labels'][0]\r\n",
    "tfa.metrics.F1Score(num_classes=args.n_classes, average='micro')(y_true, y_pred)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tf.math.argmax(y_pred, axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tf.math.argmax(y_true, axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('tf-gpu': conda)"
  },
  "interpreter": {
   "hash": "b851d2923cfa3a2562599062e05fd9893d86a7c009c64d8ad3756552e4dd5f41"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}