{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "import json\r\n",
    "import argparse\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.keras import Model, Sequential\r\n",
    "from tensorflow.keras.layers import *\r\n",
    "from tensorflow.keras.utils import plot_model\r\n",
    "from functools import partial\r\n",
    "from collections import Counter\r\n",
    "from tqdm.notebook import tqdm\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\r\n",
    "\r\n",
    "import tensorflow_io as tfio\r\n",
    "\r\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\r\n",
    "\r\n",
    "d = pd.read_csv(\"Datasets\\TIMIT-dataset\\data.csv\")\r\n",
    "d = d.sample(len(d)-2, random_state=42)\r\n",
    "d"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wav_paths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6025</th>\n",
       "      <td>Datasets\\TIMIT-dataset\\data\\DR7\\MDVC0\\SX216.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4367</th>\n",
       "      <td>Datasets\\TIMIT-dataset\\data\\DR5\\MCMB0\\SX98.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>Datasets\\TIMIT-dataset\\data\\DR7\\MTKD0\\SI630_1.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3947</th>\n",
       "      <td>Datasets\\TIMIT-dataset\\data\\DR5\\FDMY0\\SA1_1.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5982</th>\n",
       "      <td>Datasets\\TIMIT-dataset\\data\\DR7\\MDLM0\\SX424.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5734</th>\n",
       "      <td>Datasets\\TIMIT-dataset\\data\\DR7\\FPAC0\\SI661_2.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3092</th>\n",
       "      <td>Datasets\\TIMIT-dataset\\data\\DR4\\FSEM0\\SX298.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3772</th>\n",
       "      <td>Datasets\\TIMIT-dataset\\data\\DR4\\MSMC0\\SI647.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>Datasets\\TIMIT-dataset\\data\\DR6\\MAJP0\\SX174.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5226</th>\n",
       "      <td>Datasets\\TIMIT-dataset\\data\\DR6\\MCMJ0\\SX194.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6950 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              wav_paths\n",
       "6025    Datasets\\TIMIT-dataset\\data\\DR7\\MDVC0\\SX216.wav\n",
       "4367     Datasets\\TIMIT-dataset\\data\\DR5\\MCMB0\\SX98.wav\n",
       "6494  Datasets\\TIMIT-dataset\\data\\DR7\\MTKD0\\SI630_1.wav\n",
       "3947    Datasets\\TIMIT-dataset\\data\\DR5\\FDMY0\\SA1_1.wav\n",
       "5982    Datasets\\TIMIT-dataset\\data\\DR7\\MDLM0\\SX424.wav\n",
       "...                                                 ...\n",
       "5734  Datasets\\TIMIT-dataset\\data\\DR7\\FPAC0\\SI661_2.wav\n",
       "3092    Datasets\\TIMIT-dataset\\data\\DR4\\FSEM0\\SX298.wav\n",
       "3772    Datasets\\TIMIT-dataset\\data\\DR4\\MSMC0\\SI647.wav\n",
       "5191    Datasets\\TIMIT-dataset\\data\\DR6\\MAJP0\\SX174.wav\n",
       "5226    Datasets\\TIMIT-dataset\\data\\DR6\\MCMJ0\\SX194.wav\n",
       "\n",
       "[6950 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def ArgParser():\r\n",
    "    parser = argparse.ArgumentParser()\r\n",
    "\r\n",
    "    # RNN layer\r\n",
    "    parser.add_argument(\"--units\", dest=\"units\", type=int, default=50)\r\n",
    "    parser.add_argument(\"--n_layers\", dest=\"n_layers\", type=int, default=2)\r\n",
    "    parser.add_argument(\"--dropout\", dest=\"dropout\", type=int, default=0.1)\r\n",
    "    parser.add_argument(\"--bidirectional\", dest=\"bidirectional\", type=bool, default=True, choices=[True, False])\r\n",
    "\r\n",
    "    # Segmentor\r\n",
    "    parser.add_argument(\"--n_classes\", dest=\"n_classes\", type=int, default=61)\r\n",
    "    parser.add_argument(\"--batch_size\", dest=\"batch_size\", type=int, default=4)\r\n",
    "    parser.add_argument(\"--n_mels\", dest=\"n_mels\", type=int, default=32)\r\n",
    "    parser.add_argument(\"--max_seg_size\", dest=\"max_seg_size\", type=int, default=100)\r\n",
    "    parser.add_argument(\"--min_seg_size\", dest=\"min_seg_size\", type=int, default=0)\r\n",
    "    parser.add_argument(\"--n_fft\", dest=\"n_fft\", type=int, default=2048)\r\n",
    "    parser.add_argument(\"--window_size\", dest=\"window_size\", type=int, default=480)\r\n",
    "    parser.add_argument(\"--hop_length\", dest=\"hop_length\", type=int, default=160) # 160 samples = 10ms\r\n",
    "    parser.add_argument(\"--sample_rate\", dest=\"sample_rate\", type=int, default=16000)\r\n",
    "\r\n",
    "    # Dataset\r\n",
    "    parser.add_argument(\"--main_dir\", dest=\"main_dir\", type=str, default=\"Datasets/TIMIT-dataset/tfrec_data\")\r\n",
    "    parser.add_argument(\"--buffer_size\", dest=\"buffer_size\", type=int, default=512)\r\n",
    "    parser.add_argument(\"--test_size\", dest=\"test_size\", type=float, default=0.2)\r\n",
    "    parser.add_argument(\"--n_splits\", dest=\"n_splits\", type=int, default=5)\r\n",
    "    parser.add_argument(\"--max_samples\", dest=\"max_samples\", type=int, default=60000)\r\n",
    "    parser.add_argument(\"--n_samples\", dest=\"n_samples\", type=int, default=6950)\r\n",
    "\r\n",
    "    args = parser.parse_known_args()[0]\r\n",
    "    seq_len = int(np.ceil(args.max_samples / args.hop_length))\r\n",
    "    input_shape = [args.max_samples // args.hop_length, args.n_mels]\r\n",
    "    train_steps = int((args.n_samples * (1-args.test_size)) // args.batch_size)\r\n",
    "    val_steps = int((args.n_samples * args.test_size) // args.batch_size)\r\n",
    "    parser.add_argument(\"--seq_len\", type=int, default=seq_len)\r\n",
    "    parser.add_argument(\"--input_shape\", type=list, default=input_shape)\r\n",
    "    parser.add_argument(\"--train_steps\", type=int, default=train_steps)\r\n",
    "    parser.add_argument(\"--val_steps\", type=int, default=val_steps)\r\n",
    "\r\n",
    "    return parser.parse_known_args()[0]\r\n",
    "\r\n",
    "args = ArgParser()\r\n",
    "args"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Namespace(batch_size=4, bidirectional=True, buffer_size=512, dropout=0.1, hop_length=160, input_shape=[375, 32], main_dir='Datasets/TIMIT-dataset/tfrec_data', max_samples=60000, max_seg_size=100, min_seg_size=0, n_classes=61, n_fft=2048, n_layers=2, n_mels=32, n_samples=6950, n_splits=5, sample_rate=16000, seq_len=375, test_size=0.2, train_steps=1390, units=50, val_steps=347, window_size=480)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class TFRWriter():\r\n",
    "    def __init__(self, args):\r\n",
    "        self.samples = d['wav_paths'].tolist()\r\n",
    "        self.args = args\r\n",
    "        self.dict_path = \"Datasets\\TIMIT-dataset\\phoneme_dict.json\"\r\n",
    "        self.phoneme_dict = self.get_dict()\r\n",
    "\r\n",
    "    def _bytes_feature(self, value):\r\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\r\n",
    "        if isinstance(value, type(tf.constant(0))):\r\n",
    "            value = value.numpy()\r\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\r\n",
    "\r\n",
    "    def _int64_feature(self, value):\r\n",
    "        \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\r\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\r\n",
    "\r\n",
    "    def serialize_example(self, *args):\r\n",
    "        feature = {\r\n",
    "            'audio': self._bytes_feature(args[0]),\r\n",
    "            'binary_labels': self._bytes_feature(args[1]),\r\n",
    "            'framewise_labels': self._bytes_feature(args[2]),\r\n",
    "            'attention_mask': self._bytes_feature(args[3]),\r\n",
    "            'true_length': self._int64_feature(args[4]),\r\n",
    "            'filename': self._bytes_feature(args[5])}\r\n",
    "\r\n",
    "        example_proto = tf.train.Example(\r\n",
    "            features=tf.train.Features(feature=feature))\r\n",
    "        return example_proto.SerializeToString()\r\n",
    "\r\n",
    "    def get_binary_labels(self, p_frames):\r\n",
    "        p_frames = tf.convert_to_tensor(p_frames)\r\n",
    "        labels = tf.tensor_scatter_nd_update(\r\n",
    "            tensor=tf.zeros([p_frames[-1]+1], dtype=tf.int32), \r\n",
    "            indices=tf.expand_dims(p_frames, axis=1), \r\n",
    "            updates=tf.ones([p_frames.shape[0]], dtype=tf.int32))\r\n",
    "        padding = tf.zeros([self.args.seq_len-len(labels)], dtype=tf.int32)\r\n",
    "        return tf.concat([labels, padding], axis=0)\r\n",
    "\r\n",
    "    def get_framewise_labels(self, p_frames, phonemes):\r\n",
    "        labels = []\r\n",
    "        for i in range(1, len(p_frames)):\r\n",
    "            for j in range(p_frames[i-1], p_frames[i]):\r\n",
    "                labels.append(phonemes[i-1])\r\n",
    "        labels = tf.convert_to_tensor(labels)\r\n",
    "        padding = tf.zeros([self.args.seq_len-len(labels)], dtype=tf.int32)\r\n",
    "        return tf.concat([labels, padding], axis=0)\r\n",
    "\r\n",
    "    def get_attention_mask(self, p_frames):\r\n",
    "        mask = tf.convert_to_tensor(\r\n",
    "            [True if i < p_frames[-1] else False for i in range(self.args.seq_len)])\r\n",
    "        return mask\r\n",
    "\r\n",
    "    def get_shards(self):\r\n",
    "        speaker_id = [sample.split('\\\\')[4] for sample in self.samples]\r\n",
    "        skf = StratifiedKFold(\r\n",
    "            n_splits=self.args.n_splits, shuffle=True, random_state=42)\r\n",
    "        return [\r\n",
    "            list(map(lambda x: self.samples[x], j)) \r\n",
    "            for i, j in skf.split(self.samples, speaker_id)]\r\n",
    "\r\n",
    "    def get_dict(self):\r\n",
    "        phonemes = set()\r\n",
    "        markers = ['h#', 'pau', 'epi']\r\n",
    "        for sample in self.samples:\r\n",
    "            base_path = os.path.splitext(sample)[0]\r\n",
    "            with open(base_path + '.phn', \"r\") as f:\r\n",
    "                for line in f.readlines():\r\n",
    "                    phoneme = line.split()[-1]\r\n",
    "                    if not phoneme in markers:\r\n",
    "                        phonemes.add(phoneme)\r\n",
    "        phonemes = markers + sorted(Counter(phonemes), key=Counter(phonemes).get, reverse=True)\r\n",
    "        phonemes_dict = {v: i+1 for i, v in enumerate(phonemes)}\r\n",
    "        with open(self.dict_path, \"w\") as f:\r\n",
    "            json.dump(phonemes_dict, f, sort_keys=False, indent=4)\r\n",
    "        return phonemes_dict \r\n",
    "    \r\n",
    "    def get_shard_data(self, samples, shard):\r\n",
    "        for sample in tqdm(\r\n",
    "                samples, total=len(samples), desc=f\"Writing shard {shard}\"):\r\n",
    "            base_path = os.path.splitext(sample)[0]\r\n",
    "            p_frames, phonemes = [0], []\r\n",
    "            with open(base_path + \".phn\") as f:\r\n",
    "                for line in f.readlines():\r\n",
    "                    p_frame, phoneme = line.split()[1::]\r\n",
    "                    p_frames.append(int(p_frame) // self.args.hop_length)\r\n",
    "                    phonemes.append(str(phoneme))\r\n",
    "            phonemes = list(map(self.phoneme_dict.get, phonemes))\r\n",
    "            binary_labels = self.get_binary_labels(p_frames)\r\n",
    "            framewise_labels = self.get_framewise_labels(p_frames, phonemes)\r\n",
    "            waveform = tf.io.read_file(base_path + \".wav\")\r\n",
    "            spec_mask = self.get_attention_mask(p_frames)\r\n",
    "            filename = str.encode(\"/\".join(sample.split('\\\\')[-3::]))\r\n",
    "            yield {\r\n",
    "                \"audio\": waveform,\r\n",
    "                \"binary_labels\": tf.io.serialize_tensor(binary_labels),\r\n",
    "                \"framewise_labels\": tf.io.serialize_tensor(framewise_labels),\r\n",
    "                \"attention_mask\": tf.io.serialize_tensor(spec_mask),\r\n",
    "                \"true_length\": (p_frames[-1] - 1),\r\n",
    "                \"filename\": filename}\r\n",
    "\r\n",
    "    def write(self):\r\n",
    "        for shard, samples in enumerate(self.get_shards()):\r\n",
    "            with tf.io.TFRecordWriter(\r\n",
    "                    f\"Datasets/TIMIT-dataset/tfrec_data/train_{shard+1}.tfrec\") as f:\r\n",
    "                for sample in self.get_shard_data(samples, shard+1):\r\n",
    "                    example = self.serialize_example(\r\n",
    "                        sample['audio'], sample['binary_labels'],\r\n",
    "                        sample['framewise_labels'], sample['attention_mask'],\r\n",
    "                        sample['true_length'], sample['filename'])\r\n",
    "                    f.write(example)\r\n",
    "\r\n",
    "TFRWriter(args).write()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e7828ae9a59741c4b2490a93a937121a"
      },
      "text/plain": [
       "Writing shard 1:   0%|          | 0/1390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c841c79c634d4d74839b978181c82967"
      },
      "text/plain": [
       "Writing shard 2:   0%|          | 0/1390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5238cc8e69cb4e09908164da4236fc61"
      },
      "text/plain": [
       "Writing shard 3:   0%|          | 0/1390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8aa0034340134079a447064f137a4080"
      },
      "text/plain": [
       "Writing shard 4:   0%|          | 0/1390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "367d266c315b4266a874c1864eb9243b"
      },
      "text/plain": [
       "Writing shard 5:   0%|          | 0/1390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class TIMITDataset():\r\n",
    "    def __init__(self, args):\r\n",
    "        self.files = [os.path.join(args.main_dir, f) for f in os.listdir(args.main_dir)]\r\n",
    "        self.args = args\r\n",
    "        self.AUTOTUNE = tf.data.experimental.AUTOTUNE\r\n",
    "        self.train_files, self.val_files = train_test_split(\r\n",
    "            self.files, test_size=args.test_size, shuffle=True)\r\n",
    "\r\n",
    "    def decode_audio(self, string):\r\n",
    "        audio = tf.audio.decode_wav(string, desired_samples=self.args.max_samples)[0]\r\n",
    "        return tf.squeeze(audio, axis=-1)\r\n",
    "\r\n",
    "    def read_tfrecord(self, example):\r\n",
    "        feature_description = {\r\n",
    "            'audio': tf.io.FixedLenFeature([], tf.string),\r\n",
    "            'binary_labels': tf.io.FixedLenFeature([], tf.string),\r\n",
    "            'framewise_labels': tf.io.FixedLenFeature([], tf.string),\r\n",
    "            'attention_mask': tf.io.FixedLenFeature([], tf.string),\r\n",
    "            'true_length': tf.io.FixedLenFeature([], tf.int64)}\r\n",
    "        \r\n",
    "        example = tf.io.parse_single_example(example, feature_description)\r\n",
    "        example['audio'] = self.decode_audio(example['audio'])\r\n",
    "        example['binary_labels'] = tf.io.parse_tensor(\r\n",
    "            example['binary_labels'], out_type=tf.int32)\r\n",
    "        example['framewise_labels'] = tf.io.parse_tensor(\r\n",
    "            example['framewise_labels'], out_type=tf.int32)\r\n",
    "        example['attention_mask'] = tf.io.parse_tensor(\r\n",
    "            example['attention_mask'], out_type=tf.bool)\r\n",
    "        example['true_length'] = tf.cast(example['true_length'], dtype=tf.int32) - 1\r\n",
    "        return example\r\n",
    "\r\n",
    "    def load_dataset(self, files):\r\n",
    "        ignore_order = tf.data.Options()\r\n",
    "        ignore_order.experimental_deterministic = False\r\n",
    "        dataset = tf.data.TFRecordDataset(files, num_parallel_reads=self.AUTOTUNE)\r\n",
    "        dataset = dataset.with_options(ignore_order)\r\n",
    "        dataset = dataset.map(self.read_tfrecord, num_parallel_calls=self.AUTOTUNE)\r\n",
    "        return dataset\r\n",
    " \r\n",
    "    def SpecAugment(self, sample, training):\r\n",
    "        waveform = sample['audio'] / 32678\r\n",
    "        if training == True:\r\n",
    "            waveform = tfio.audio.fade(\r\n",
    "                waveform, fade_in=1000, fade_out=2000, mode=\"logarithmic\")\r\n",
    "        spectrogram = tf.abs(tfio.audio.spectrogram(\r\n",
    "            waveform, nfft=self.args.n_fft, window=self.args.window_size, \r\n",
    "            stride=self.args.hop_length))\r\n",
    "        mel_spectrogram = tfio.audio.melscale(\r\n",
    "            spectrogram, rate=self.args.sample_rate, mels=self.args.n_mels, \r\n",
    "            fmin=0, fmax=8000)\r\n",
    "        mel_spectrogram = tfio.audio.dbscale(mel_spectrogram, top_db=80)\r\n",
    "        if training == True:\r\n",
    "            mel_spectrogram = tfio.audio.freq_mask(mel_spectrogram, param=7)\r\n",
    "            mel_spectrogram = tfio.audio.time_mask(mel_spectrogram, param=10)\r\n",
    "        inputs = (mel_spectrogram, sample['attention_mask'], sample['true_length'])\r\n",
    "        outputs = (sample['binary_labels'], sample['framewise_labels'])\r\n",
    "        return inputs, outputs\r\n",
    "\r\n",
    "    def train(self):\r\n",
    "        dataset = self.load_dataset(self.train_files)\r\n",
    "        dataset = dataset.map(\r\n",
    "            partial(self.SpecAugment, training=True), num_parallel_calls=self.AUTOTUNE)\r\n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\r\n",
    "        dataset = dataset.batch(self.args.batch_size)\r\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\r\n",
    "        return dataset\r\n",
    "\r\n",
    "    def val(self):\r\n",
    "        dataset = self.load_dataset(self.val_files)\r\n",
    "        dataset = dataset.map(\r\n",
    "            partial(self.SpecAugment, training=False), num_parallel_calls=self.AUTOTUNE)\r\n",
    "        dataset = dataset.batch(self.args.batch_size)\r\n",
    "        dataset = dataset.cache()\r\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\r\n",
    "        return dataset\r\n",
    "\r\n",
    "train = TIMITDataset(args).train()\r\n",
    "val = TIMITDataset(args).val()\r\n",
    "\r\n",
    "print(train)\r\n",
    "inputs, outputs = next(iter(train))\r\n",
    "print(\"\\nspectrogram shape:\", inputs[0].shape)\r\n",
    "print(\"mask shape:\", inputs[1].shape)\r\n",
    "print(\"length shape:\", inputs[2].shape)\r\n",
    "print(\"binary_labels shape:\", outputs[0].shape)\r\n",
    "print(\"framewise_labels shape:\", outputs[1].shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<PrefetchDataset shapes: (((None, 375, 32), <unknown>, (None,)), (<unknown>, <unknown>)), types: ((tf.float32, tf.bool, tf.int32), (tf.int32, tf.int32))>\n",
      "\n",
      "spectrogram shape: (4, 375, 32)\n",
      "mask shape: (4, 375)\n",
      "length shape: (4,)\n",
      "binary_labels shape: (4, 375)\n",
      "framewise_labels shape: (4, 375)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class Segmentor(Model):\r\n",
    "    def __init__(self, args):\r\n",
    "        super(Segmentor, self).__init__(name=\"Segmentor\")\r\n",
    "        self.args = args\r\n",
    "        self.rnn = self.RNNBlock()\r\n",
    "        self.scorer = self.ScorerBlock()\r\n",
    "        self.classifier = self.ClassifierBlock()\r\n",
    "        self.bi_classifier = self.BiClassifierBlock()\r\n",
    "\r\n",
    "    def RNNBlock(self):\r\n",
    "        spectrogram = Input(\r\n",
    "            shape=self.args.input_shape, dtype=tf.float32)\r\n",
    "        mask = Input(\r\n",
    "            shape=self.args.input_shape[0], dtype=tf.bool) \r\n",
    "\r\n",
    "        x = Bidirectional(\r\n",
    "                LSTM(self.args.units, return_sequences=True), name=\"layer_1\")(\r\n",
    "                    inputs=spectrogram, mask=mask)\r\n",
    "        x = Bidirectional(\r\n",
    "                LSTM(self.args.units, return_sequences=True), name=\"layer_2\")(x)\r\n",
    "        outputs = Bidirectional(\r\n",
    "                LSTM(self.args.units, return_sequences=True), name=\"layer_3\")(x)\r\n",
    "        return Model(inputs=[spectrogram, mask], outputs=outputs, name=\"rnn_block\")\r\n",
    "\r\n",
    "    def ScorerBlock(self):\r\n",
    "        return Sequential([\r\n",
    "            PReLU(),\r\n",
    "            Dense(100),\r\n",
    "            PReLU(),\r\n",
    "            Dense(1)], name=\"scorer\")\r\n",
    "\r\n",
    "    def ClassifierBlock(self):\r\n",
    "        return Sequential([\r\n",
    "            PReLU(),\r\n",
    "            Dense(self.args.n_classes * 2),\r\n",
    "            PReLU(),\r\n",
    "            Dense(self.args.n_classes)], name=\"classifier\")\r\n",
    "\r\n",
    "    def BiClassifierBlock(self):\r\n",
    "        return Sequential([\r\n",
    "            PReLU(),\r\n",
    "            Dense(self.args.n_classes * 2),\r\n",
    "            PReLU(),\r\n",
    "            Dense(2)], name=\"bi_classifier\")\r\n",
    "\r\n",
    "    def compute_phi(self, rnn_out):\r\n",
    "        batch_size, (seq_len, feat_dim) = self.args.batch_size, rnn_out.shape[1:]\r\n",
    "\r\n",
    "        rnn_cum = tf.math.cumsum(rnn_out, axis=1)\r\n",
    "        output_shape = [batch_size, seq_len, seq_len, feat_dim]\r\n",
    "        \r\n",
    "        a = tf.repeat(rnn_cum, [1, seq_len, 1])\r\n",
    "        b = tf.reshape(tf.repeat(rnn_cum, [1, 1, seq_len]), [batch_size, -1, feat_dim])\r\n",
    "        c = tf.reshape(tf.math.subtract(a, b), output_shape)\r\n",
    "        d = tf.reshape(tf.repeat(rnn_out, [1, 1, seq_len]), output_shape)\r\n",
    "        e = tf.reshape(tf.repeat(rnn_out, [1, seq_len, 1]), output_shape)\r\n",
    "        return tf.concat([c, d, e], axis=-1)\r\n",
    "\r\n",
    "    def segment_search(self, scores, lengths):\r\n",
    "        \"\"\"\r\n",
    "        Dynamic search algorithm\r\n",
    "        \"\"\"\r\n",
    "        batch_size, seq_len = self.args.batch_size, scores.shape[1]\r\n",
    "\r\n",
    "        best_scores = tf.zeros([batch_size, seq_len])\r\n",
    "        segmentations =  [[tf.zeros([2,], dtype=tf.int32)] for _ in range(batch_size)]\r\n",
    "\r\n",
    "        for i in range(1, seq_len):\r\n",
    "            start_idx = max(0, i - self.args.max_seg_size)\r\n",
    "            end_idx = i\r\n",
    "            current_scores = tf.zeros([batch_size, end_idx - start_idx])\r\n",
    "\r\n",
    "            for j in range(start_idx, end_idx):\r\n",
    "                index = tf.constant([[k, (j - start_idx)] for k in range(batch_size)])\r\n",
    "                update = (best_scores[:, j] + scores[:, j, i])\r\n",
    "                tf.tensor_scatter_nd_update(current_scores, index, update)\r\n",
    "\r\n",
    "            best_score, best_index = tf.math.top_k(current_scores, k=1)\r\n",
    "            best_score = tf.squeeze(best_score, axis=1)\r\n",
    "            best_index += start_idx\r\n",
    "            best_indices = tf.constant([[m, i] for m in range(batch_size)])\r\n",
    "            tf.tensor_scatter_nd_update(best_scores, best_indices, best_score)\r\n",
    "\r\n",
    "            for n in range(batch_size):\r\n",
    "                current_segment = tf.concat(\r\n",
    "                    [best_index[n], tf.constant([i])], axis=-1)\r\n",
    "                segmentations[n].append(current_segment)\r\n",
    "        \r\n",
    "        batch_segments = []\r\n",
    "        for i, segments in enumerate(segmentations):\r\n",
    "            segments = tf.stack(segments, axis=0)\r\n",
    "            segments = tf.gather(segments, lengths[i], axis=0)\r\n",
    "            batch_segments.append(segments)\r\n",
    "        return tf.stack(batch_segments, axis=1)\r\n",
    "\r\n",
    "    def compute_segmentation_score(self, scores, segments):\r\n",
    "        out_scores = tf.zeros([scores.shape[0]])\r\n",
    "        return out_scores\r\n",
    "\r\n",
    "    def call(self, inputs):\r\n",
    "        spectrogram, mask, lengths = inputs\r\n",
    "        rnn_out = self.rnn([spectrogram, mask])\r\n",
    "        phi = self.compute_phi(rnn_out)\r\n",
    "        scores = tf.squeeze(self.scorer(phi), axis=-1)\r\n",
    "        segments = self.segment_search(scores, lengths)\r\n",
    "\r\n",
    "        return {\r\n",
    "            \"classifier_out\": self.classifier(rnn_out),\r\n",
    "            \"bi_classifier_out\": self.bi_classifier(rnn_out),\r\n",
    "            \"segments\": segments,\r\n",
    "            \"segmentation_scores\": self.compute_segmentation_score(scores, segments)\r\n",
    "        }\r\n",
    "\r\n",
    "model = Segmentor(args)\r\n",
    "inputs = [\r\n",
    "    Input(args.input_shape), \r\n",
    "    Input(args.input_shape[0], dtype=tf.bool), \r\n",
    "    Input([1], dtype=tf.int32)]\r\n",
    "model(inputs)\r\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"Segmentor\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "rnn_block (Functional)       (None, 375, 100)          154000    \n",
      "_________________________________________________________________\n",
      "scorer (Sequential)          (4, 375, 375, 1)          56280201  \n",
      "_________________________________________________________________\n",
      "classifier (Sequential)      (None, 375, 61)           103075    \n",
      "_________________________________________________________________\n",
      "bi_classifier (Sequential)   (None, 375, 2)            95818     \n",
      "=================================================================\n",
      "Total params: 56,633,094\n",
      "Trainable params: 56,633,094\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "preds = model.predict(val, steps=args.val_steps, verbose=1)\r\n",
    "preds"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "347/347 [==============================] - 21s 60ms/step\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'classifier_out': array([[[ 0.05500124,  0.00255291,  0.04632472, ...,  0.03665183,\n",
       "           0.0378599 ,  0.05363213],\n",
       "         [ 0.0683851 , -0.01608344,  0.03931045, ...,  0.0382652 ,\n",
       "           0.05577371,  0.05699685],\n",
       "         [ 0.07487895, -0.02881021,  0.03773267, ...,  0.04176209,\n",
       "           0.06445428,  0.05766593],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]],\n",
       " \n",
       "        [[ 0.06836875,  0.00992684,  0.06274986, ...,  0.02899618,\n",
       "           0.04501845,  0.02658806],\n",
       "         [ 0.07628577, -0.00321908,  0.0625666 , ...,  0.03833392,\n",
       "           0.06116769,  0.02788127],\n",
       "         [ 0.07483246, -0.01857652,  0.06501008, ...,  0.04590325,\n",
       "           0.07448661,  0.03352801],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]],\n",
       " \n",
       "        [[ 0.06283506,  0.00604081,  0.06346454, ...,  0.02889668,\n",
       "           0.0576719 ,  0.0424805 ],\n",
       "         [ 0.07519243, -0.01288323,  0.05942393, ...,  0.03399086,\n",
       "           0.07561275,  0.04274381],\n",
       "         [ 0.08306713, -0.02794342,  0.05378846, ...,  0.03515382,\n",
       "           0.08573344,  0.0414985 ],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.06506575,  0.00141196,  0.06728697, ...,  0.03314425,\n",
       "           0.04768831,  0.03297707],\n",
       "         [ 0.07359014, -0.01898566,  0.06877223, ...,  0.03738369,\n",
       "           0.0630791 ,  0.0337057 ],\n",
       "         [ 0.07602389, -0.03013053,  0.06813987, ...,  0.03873753,\n",
       "           0.074007  ,  0.04035589],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]],\n",
       " \n",
       "        [[ 0.06586148,  0.00139737,  0.07050062, ...,  0.02913514,\n",
       "           0.04872782,  0.03787114],\n",
       "         [ 0.06273031, -0.02225845,  0.06869143, ...,  0.02826616,\n",
       "           0.05543135,  0.04886195],\n",
       "         [ 0.05651362, -0.03827335,  0.06372953, ...,  0.02902753,\n",
       "           0.0563144 ,  0.04712623],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]],\n",
       " \n",
       "        [[ 0.03973671,  0.00389094,  0.07470717, ...,  0.02630068,\n",
       "           0.05262114,  0.02223936],\n",
       "         [ 0.04377516, -0.01078318,  0.09222861, ...,  0.0330928 ,\n",
       "           0.07139482,  0.02104582],\n",
       "         [ 0.04575812, -0.03083336,  0.10306643, ...,  0.03573208,\n",
       "           0.08080078,  0.0180911 ],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]]], dtype=float32),\n",
       " 'bi_classifier_out': array([[[ 0.00169151, -0.05607415],\n",
       "         [-0.01125306, -0.0516964 ],\n",
       "         [-0.01825861, -0.05341294],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       " \n",
       "        [[ 0.02736109, -0.09369424],\n",
       "         [ 0.02262474, -0.10117739],\n",
       "         [ 0.01788054, -0.1054099 ],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       " \n",
       "        [[ 0.02203344, -0.07501659],\n",
       "         [ 0.0180415 , -0.08987074],\n",
       "         [ 0.01773661, -0.09948541],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.01558523, -0.07857873],\n",
       "         [ 0.00601074, -0.08435009],\n",
       "         [ 0.00029593, -0.09196597],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       " \n",
       "        [[-0.00730804, -0.08935133],\n",
       "         [-0.02452955, -0.08708063],\n",
       "         [-0.03387942, -0.08190134],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       " \n",
       "        [[ 0.01570553, -0.05910632],\n",
       "         [ 0.00625661, -0.05811904],\n",
       "         [-0.00303465, -0.06048466],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]]], dtype=float32),\n",
       " 'segments': array([[[111, 211],\n",
       "         [115, 215],\n",
       "         [119, 219],\n",
       "         [ 71, 171]],\n",
       " \n",
       "        [[125, 225],\n",
       "         [ 99, 199],\n",
       "         [180, 280],\n",
       "         [192, 292]],\n",
       " \n",
       "        [[ 88, 188],\n",
       "         [ 90, 190],\n",
       "         [155, 255],\n",
       "         [259, 359]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[151, 251],\n",
       "         [151, 251],\n",
       "         [ 98, 198],\n",
       "         [168, 268]],\n",
       " \n",
       "        [[110, 210],\n",
       "         [209, 309],\n",
       "         [258, 358],\n",
       "         [145, 245]],\n",
       " \n",
       "        [[116, 216],\n",
       "         [ 91, 191],\n",
       "         [125, 225],\n",
       "         [ 15, 115]]]),\n",
       " 'segmentation_scores': array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)}"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "preds = model.predict(train, steps=args.train_steps, verbose=1)\r\n",
    "preds"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1390/1390 [==============================] - 84s 59ms/step\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'classifier_out': array([[[ 0.08078156,  0.04427957,  0.06166065, ...,  0.02581537,\n",
       "           0.03768326,  0.05716383],\n",
       "         [ 0.0938343 ,  0.03112601,  0.07883322, ...,  0.0375433 ,\n",
       "           0.05411808,  0.07403104],\n",
       "         [ 0.1007898 ,  0.02810994,  0.08645593, ...,  0.04309379,\n",
       "           0.069264  ,  0.08206148],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]],\n",
       " \n",
       "        [[ 0.01311169, -0.00890859,  0.03970594, ...,  0.03202897,\n",
       "          -0.00689473, -0.0065309 ],\n",
       "         [ 0.01769395, -0.03222793,  0.04441531, ...,  0.02598259,\n",
       "           0.00691484, -0.0070019 ],\n",
       "         [ 0.01412298, -0.05172816,  0.05016503, ...,  0.02490132,\n",
       "           0.0244036 , -0.01011348],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]],\n",
       " \n",
       "        [[ 0.04359866,  0.02331292,  0.0851239 , ...,  0.02243254,\n",
       "           0.04188811,  0.03362639],\n",
       "         [ 0.0443566 , -0.00769371,  0.1079893 , ...,  0.02933977,\n",
       "           0.04516571,  0.04031997],\n",
       "         [ 0.04964033, -0.04299453,  0.12619469, ...,  0.03487346,\n",
       "           0.04695338,  0.03921381],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.08790695,  0.02044289,  0.05884096, ...,  0.01994333,\n",
       "           0.04756306,  0.01677481],\n",
       "         [ 0.08483764, -0.00539513,  0.05148153, ...,  0.02535244,\n",
       "           0.05778816,  0.02063625],\n",
       "         [ 0.07810616, -0.0256846 ,  0.04734639, ...,  0.02914287,\n",
       "           0.0624733 ,  0.01852136],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]],\n",
       " \n",
       "        [[ 0.07892913,  0.03500769, -0.00292081, ..., -0.01976994,\n",
       "           0.0526529 ,  0.04069294],\n",
       "         [ 0.08070628,  0.01549484,  0.00403495, ..., -0.01362425,\n",
       "           0.05996279,  0.02760711],\n",
       "         [ 0.08801657,  0.00445142,  0.01032146, ..., -0.00566345,\n",
       "           0.07087584,  0.01398982],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]],\n",
       " \n",
       "        [[ 0.10692091, -0.02363734,  0.08090135, ..., -0.00651255,\n",
       "           0.04896818,  0.07344694],\n",
       "         [ 0.11771608, -0.05405417,  0.07592088, ...,  0.00350329,\n",
       "           0.05805062,  0.08373914],\n",
       "         [ 0.12807627, -0.07259236,  0.07341459, ...,  0.01112236,\n",
       "           0.07146063,  0.08901764],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]]], dtype=float32),\n",
       " 'bi_classifier_out': array([[[ 0.06467854, -0.09247509],\n",
       "         [ 0.06174661, -0.10236621],\n",
       "         [ 0.06089178, -0.11332253],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       " \n",
       "        [[-0.00521127, -0.00871276],\n",
       "         [-0.0331324 , -0.00127342],\n",
       "         [-0.05363958,  0.00235189],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       " \n",
       "        [[ 0.0353309 , -0.15471488],\n",
       "         [ 0.03075385, -0.15842295],\n",
       "         [ 0.02882364, -0.16130432],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.01517637, -0.12407053],\n",
       "         [ 0.01339882, -0.12537326],\n",
       "         [ 0.00918373, -0.12176849],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       " \n",
       "        [[-0.01593287, -0.06858274],\n",
       "         [-0.00909826, -0.09182537],\n",
       "         [ 0.00028742, -0.11269724],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]],\n",
       " \n",
       "        [[ 0.03029271, -0.09684625],\n",
       "         [ 0.02928481, -0.10506906],\n",
       "         [ 0.03005154, -0.10950164],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ]]], dtype=float32),\n",
       " 'segments': array([[[171, 271],\n",
       "         [133, 233],\n",
       "         [252, 352],\n",
       "         [162, 262]],\n",
       " \n",
       "        [[ 98, 198],\n",
       "         [175, 275],\n",
       "         [205, 305],\n",
       "         [ 81, 181]],\n",
       " \n",
       "        [[158, 258],\n",
       "         [114, 214],\n",
       "         [100, 200],\n",
       "         [202, 302]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 98, 198],\n",
       "         [139, 239],\n",
       "         [120, 220],\n",
       "         [151, 251]],\n",
       " \n",
       "        [[131, 231],\n",
       "         [195, 295],\n",
       "         [150, 250],\n",
       "         [ 79, 179]],\n",
       " \n",
       "        [[166, 266],\n",
       "         [205, 305],\n",
       "         [110, 210],\n",
       "         [218, 318]]]),\n",
       " 'segmentation_scores': array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)}"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "preds['segments']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[171, 271],\n",
       "        [133, 233],\n",
       "        [252, 352],\n",
       "        [162, 262]],\n",
       "\n",
       "       [[ 98, 198],\n",
       "        [175, 275],\n",
       "        [205, 305],\n",
       "        [ 81, 181]],\n",
       "\n",
       "       [[158, 258],\n",
       "        [114, 214],\n",
       "        [100, 200],\n",
       "        [202, 302]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 98, 198],\n",
       "        [139, 239],\n",
       "        [120, 220],\n",
       "        [151, 251]],\n",
       "\n",
       "       [[131, 231],\n",
       "        [195, 295],\n",
       "        [150, 250],\n",
       "        [ 79, 179]],\n",
       "\n",
       "       [[166, 266],\n",
       "        [205, 305],\n",
       "        [110, 210],\n",
       "        [218, 318]]])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('tf-gpu': conda)"
  },
  "interpreter": {
   "hash": "b851d2923cfa3a2562599062e05fd9893d86a7c009c64d8ad3756552e4dd5f41"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}