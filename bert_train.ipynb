{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import cutlet\n",
    "import argparse\n",
    "import MeCab\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "from convert_romaji import Romaji2Kana\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Dense, \n",
    "    TimeDistributed)\n",
    "\n",
    "from transformers import (\n",
    "    BertJapaneseTokenizer,\n",
    "    TFBertModel,\n",
    "    GradientAccumulator,\n",
    "    logging)\n",
    "\n",
    "def seed_everything(SEED):\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    print(\"Random seed set.\")\n",
    "\n",
    "seed_everything(42)\n",
    "tf.get_logger().setLevel('FATAL')\n",
    "logging.set_verbosity_error()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=64, buffer_size=1024, epochs=20, learning_rate=0.0001, lr_max=0.0001, lr_min=1e-08, lr_start=1e-08, main_dir='E://Datasets/Decoder_model', n_cycles=0.5, n_samples=1500000, n_shards=20, n_train=1200000, n_val=300000, random_state=42, sustain_epochs=0, test_size=0.2, train_steps=18750, val_steps=4688, vocab_size=4000, warmup_epochs=2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ArgParser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Dataset\n",
    "    parser.add_argument(\"--main_dir\", default=\"E://Datasets/Decoder_model\")\n",
    "    parser.add_argument(\"--random_state\", default=42)\n",
    "    parser.add_argument(\"--n_shards\", default=20)\n",
    "    parser.add_argument(\"--n_samples\", default=1500000)\n",
    "    parser.add_argument(\"--test_size\", default=0.2)\n",
    "    parser.add_argument(\"--vocab_size\", default=4000)\n",
    "    parser.add_argument(\"--batch_size\", default=64)\n",
    "    parser.add_argument(\"--buffer_size\", default=1024)\n",
    "\n",
    "    # Scheduler\n",
    "    parser.add_argument(\"--epochs\", default=20)\n",
    "    parser.add_argument(\"--learning_rate\", default=1e-4)\n",
    "    parser.add_argument(\"--lr_start\", default=1e-8)\n",
    "    parser.add_argument(\"--lr_min\", default=1e-8)\n",
    "    parser.add_argument(\"--lr_max\", default=1e-4)\n",
    "    parser.add_argument(\"--n_cycles\", default=0.5)\n",
    "    parser.add_argument(\"--warmup_epochs\", default=2)\n",
    "    parser.add_argument(\"--sustain_epochs\", default=0)\n",
    "\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    n_train = int(args.n_samples * (1 - args.test_size))\n",
    "    n_val = int(args.n_samples * args.test_size)\n",
    "    train_steps = int(np.ceil(n_train / args.batch_size))\n",
    "    val_steps = int(np.ceil(n_val / args.batch_size))\n",
    "    \n",
    "    # Trainer\n",
    "    parser.add_argument(\"--n_train\", default=n_train)\n",
    "    parser.add_argument(\"--n_val\", default=n_val)\n",
    "    parser.add_argument(\"--train_steps\", default=train_steps)  \n",
    "    parser.add_argument(\"--val_steps\", default=val_steps)\n",
    "\n",
    "    return parser.parse_known_args()[0]\n",
    "\n",
    "args = ArgParser()\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset:\n",
    "    def __init__(self):\n",
    "        self.main_dir = \"D:\\School-stuff\\Sem-2\\PR-Project\\HoloASR\\Datasets\"\n",
    "        opus_ja_paths = glob.glob(f\"{self.main_dir}\\OPUS100-dataset\\*.ja\")\n",
    "        tatoeba_ja_paths = glob.glob(f\"{self.main_dir}\\Tatoeba-dataset\\*.ja\")\n",
    "        self.jesc_path = f\"{self.main_dir}/JESC-dataset/raw\"\n",
    "        self.ja_paths = opus_ja_paths + tatoeba_ja_paths\n",
    "        self.kanji_unicode = self.get_kanji_unicode()\n",
    "        self.katsu = cutlet.Cutlet()\n",
    "        self.katsu.use_foreign_spelling = False\n",
    "\n",
    "        tqdm.pandas()\n",
    "        self.data = pd.DataFrame({\"raw_text\": self.get_data()})\n",
    "\n",
    "        # Remove rows that contains non-kanji characters\n",
    "        self.data = self.data[self.data['raw_text'].progress_apply(self.check_kanji)]   \n",
    "\n",
    "        # Remove words within parenthesis\n",
    "        parenthesis =  r\"\\（.*\\）|\\(.*\\)|\\「.*\\」|\\『.*\\』\"\n",
    "        self.data = self.data[~self.data['raw_text'].str.contains(parenthesis)]\n",
    "\n",
    "        # Remove punctuations from sentences\n",
    "        self.data['raw_text'] = self.data['raw_text'].progress_apply(self.clean_kanji)\n",
    "\n",
    "        # Converts kanji to hiragana sentences\n",
    "        self.data['hira_text'] = self.data['raw_text'].progress_apply(self.kanji2hira)\n",
    "\n",
    "        # Remove null rows\n",
    "        self.data = self.data[~(self.data['raw_text']==\"\") | ~(self.data['hira_text']==\"\")]\n",
    "        self.data = self.data.dropna().reset_index(drop=True)\n",
    "\n",
    "        # Generate vocab file\n",
    "        self.vocab_file = r\"E:\\Datasets\\Language_model\\bert_vocab.txt\"\n",
    "        self.get_vocab(self.data)\n",
    "\n",
    "        # Construct tokenizer\n",
    "        self.tokenizer = BertJapaneseTokenizer(\n",
    "            vocab_file=self.vocab_file,\n",
    "            do_lower_case=False,\n",
    "            do_word_tokenize=False,\n",
    "            do_subword_tokenize=True,\n",
    "            word_tokenizer_type=\"mecab\",\n",
    "            subword_tokenizer_type=\"character\")\n",
    "\n",
    "        # Tokenize inputs and labels\n",
    "        self.data['input_ids'] = self.data['hira_text'].progress_apply(\n",
    "            lambda x: self.tokenizer(x).input_ids)\n",
    "        self.data['label_ids'] = self.data['raw_text'].progress_apply(\n",
    "            lambda x: self.tokenizer(x).input_ids)\n",
    "\n",
    "        # Apply padding to either input or labels to same length\n",
    "        new_input_ids, new_label_ids = [], []\n",
    "        for row_idx in tqdm(range(len(self.data)), total=len(self.data)):\n",
    "            input_ids, label_ids = self.pad_longest(row_idx)\n",
    "            new_input_ids.append(input_ids)\n",
    "            new_label_ids.append(label_ids)\n",
    "\n",
    "        self.data['input_ids'] = new_input_ids\n",
    "        self.data['label_ids'] = new_label_ids\n",
    "        self.data['input_len'] = self.data['input_ids'].apply(len)\n",
    "\n",
    "        # Remove rows that has unknown tokens\n",
    "        self.data = self.data[~self.data['input_ids'].apply(\n",
    "            lambda x: self.tokenizer.unk_token_id in x)]\n",
    "        self.data = self.data[~self.data['label_ids'].apply(\n",
    "            lambda x: self.tokenizer.unk_token_id in x)]\n",
    "        self.data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Save to csv\n",
    "        self.data.to_csv(\n",
    "            r\"E:\\Datasets\\Language_model\\bert_data.csv\", \n",
    "            encoding=\"utf-8\", index=False)\n",
    "\n",
    "    def get_kanji_unicode(self):\n",
    "        vocab = set()\n",
    "        with open(f\"{self.main_dir}\\kanji_unicode.txt\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                for char in line.split()[1:]:\n",
    "                    vocab.add(char)\n",
    "        return \"|\".join(sorted(vocab))\n",
    "\n",
    "    def get_data(self):\n",
    "        ja_lines = []\n",
    "        for ja_path in self.ja_paths:\n",
    "            with open(ja_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f.readlines():\n",
    "                    line = line.strip(\"\\n| \")\n",
    "                    ja_lines.append(line)\n",
    "        with open(self.jesc_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            texts = [text.split(\"\\t\") for text in f.readlines()]\n",
    "            for _, line in texts:\n",
    "                line = line.strip(\"\\n| \")\n",
    "                ja_lines.append(line)\n",
    "        return ja_lines\n",
    "\n",
    "    def check_kanji(self, sentence):\n",
    "        pattern = f\"[^{self.kanji_unicode}]\"\n",
    "        if re.match(pattern, sentence) == None:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def clean_kanji(self, sentence):\n",
    "        sentence = \"\".join(sentence.split())\n",
    "        pattern = r\"[・\\。\\！\\.\\？\\、]\"\n",
    "        sentence = re.sub(pattern, \"\", sentence)\n",
    "        return sentence\n",
    "\n",
    "    def kanji2hira(self, sentence):\n",
    "        try:\n",
    "            sentence = self.katsu.romaji(sentence)\n",
    "            sentence = sentence.replace(\" \", \"\").lower()\n",
    "            sentence = Romaji2Kana(sentence)\n",
    "        except:\n",
    "            sentence = None\n",
    "        return sentence\n",
    "\n",
    "    def get_vocab(self, data):\n",
    "        vocab = []\n",
    "        texts = data['raw_text'].tolist() + data['hira_text'].tolist()\n",
    "        for text in tqdm(texts):\n",
    "            for char in text:\n",
    "                vocab.append(char)\n",
    "\n",
    "        tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "        \n",
    "        for i in Counter(vocab).most_common():\n",
    "            if i[0] in self.kanji_unicode:\n",
    "                tokens.append(i[0])\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for token in tokens[:args.vocab_size]:\n",
    "                f.write(token + \"\\n\")    \n",
    "\n",
    "    def pad_longest(self, row):\n",
    "        input_len = len(self.data['input_ids'][row])\n",
    "        label_len = len(self.data['label_ids'][row])\n",
    "        input_ids = self.data['input_ids'][row]\n",
    "        label_ids = self.data['label_ids'][row]\n",
    "        if label_len > input_len:\n",
    "            pad_width = label_len - input_len\n",
    "            input_ids = np.pad(\n",
    "                self.data['input_ids'][row], pad_width=(0, pad_width)).tolist()\n",
    "        elif label_len < input_len:\n",
    "            pad_width = input_len - label_len\n",
    "            label_ids = np.pad(\n",
    "                self.data['label_ids'][row], pad_width=(0, pad_width)).tolist()\n",
    "        return input_ids, label_ids\n",
    "\n",
    "# data = BertDataset().data\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFRWriter():\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.data = self.get_data()\n",
    "\n",
    "    def get_data(self):\n",
    "        tqdm.pandas()\n",
    "        data = pd.read_csv(f\"{self.args.main_dir}/bert_data.csv\")\n",
    "        data = data.dropna().reset_index(drop=True)\n",
    "        data['input_ids'] = data['input_ids'].progress_apply(ast.literal_eval)\n",
    "        data['label_ids'] = data['label_ids'].progress_apply(ast.literal_eval)\n",
    "        data = data.query(f\"input_len <= {data['input_len'].quantile(0.90)}\")\n",
    "        data = data.sample(n=self.args.n_samples, random_state=self.args.random_state)\n",
    "        data = data.sort_values(by=\"input_len\", ascending=True, ignore_index=True)\n",
    "        return data[['input_ids', 'label_ids']]\n",
    "\n",
    "    def _bytes_feature(self, value):\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    def serialize_example(self, *args):\n",
    "        feature = {\n",
    "            'input_ids': self._bytes_feature(args[0]),\n",
    "            'attention_mask': self._bytes_feature(args[1]),\n",
    "            'label_ids': self._bytes_feature(args[2])}\n",
    "\n",
    "        example_proto = tf.train.Example(\n",
    "            features=tf.train.Features(feature=feature))\n",
    "        return example_proto.SerializeToString()\n",
    "\n",
    "    def get_shards(self):\n",
    "        skf = KFold(n_splits=self.args.n_shards, shuffle=False)\n",
    "        return [j for i,j in skf.split(self.data)]\n",
    "\n",
    "    def get_shard_data(self, samples):\n",
    "        for sample in samples:\n",
    "            input_ids = tf.convert_to_tensor(\n",
    "                self.data['input_ids'][sample], dtype=tf.int32)\n",
    "            attention_mask = tf.where(input_ids != 0, x=1, y=0)\n",
    "            label_ids = tf.convert_to_tensor(\n",
    "                self.data['label_ids'][sample], dtype=tf.int32)\n",
    "            yield {\n",
    "                \"input_ids\": tf.io.serialize_tensor(input_ids),\n",
    "                \"attention_mask\": tf.io.serialize_tensor(attention_mask),\n",
    "                \"label_ids\": tf.io.serialize_tensor(label_ids)}\n",
    "\n",
    "    def write(self):\n",
    "        for shard, samples in tqdm(enumerate(self.get_shards()), total=self.args.n_shards):\n",
    "            with tf.io.TFRecordWriter(f\"{self.args.main_dir}/bert_tfrec/shard_{shard+1}.tfrec\") as f:\n",
    "                for sample in self.get_shard_data(samples):\n",
    "                    example = self.serialize_example(\n",
    "                        sample['input_ids'],\n",
    "                        sample['attention_mask'],\n",
    "                        sample['label_ids'])\n",
    "                    f.write(example)\n",
    "\n",
    "# TFRWriter(args).write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, args):\n",
    "        self.files = glob.glob(args.main_dir + \"/bert_tfrec/*.tfrec\")\n",
    "        self.args = args\n",
    "        self.AUTOTUNE = tf.data.AUTOTUNE\n",
    "        self.train_files, self.val_files = train_test_split(\n",
    "            self.files, test_size=args.test_size, shuffle=True,\n",
    "            random_state=args.random_state)\n",
    "        self.train = self.get_train()\n",
    "        self.val = self.get_val()\n",
    "\n",
    "    def read_tfrecord(self, example):\n",
    "        feature_description = {\n",
    "            'input_ids': tf.io.FixedLenFeature([], tf.string),\n",
    "            'attention_mask': tf.io.FixedLenFeature([], tf.string),\n",
    "            'label_ids': tf.io.FixedLenFeature([], tf.string)\n",
    "            }\n",
    "        \n",
    "        example = tf.io.parse_single_example(example, feature_description)\n",
    "        example['input_ids'] = tf.io.parse_tensor(\n",
    "            example['input_ids'], out_type=tf.int32)\n",
    "        example['attention_mask'] = tf.io.parse_tensor(\n",
    "            example['attention_mask'], out_type=tf.int32) \n",
    "        example['label_ids'] = tf.io.parse_tensor(\n",
    "            example['label_ids'], out_type=tf.int32)\n",
    "        return example\n",
    "\n",
    "    def load_dataset(self, files):\n",
    "        ignore_order = tf.data.Options()\n",
    "        ignore_order.experimental_deterministic = False\n",
    "        dataset = tf.data.TFRecordDataset(files)\n",
    "        dataset = dataset.with_options(ignore_order)\n",
    "        dataset = dataset.map(self.read_tfrecord, num_parallel_calls=self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_train(self):\n",
    "        dataset = self.load_dataset(self.train_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_ids': [None],\n",
    "                'attention_mask': [None],\n",
    "                'label_ids': [None]\n",
    "            },\n",
    "            padding_values={\n",
    "                'input_ids': tf.constant(0, dtype=tf.int32),\n",
    "                'attention_mask': tf.constant(0, dtype=tf.int32),\n",
    "                'label_ids': tf.constant(0, dtype=tf.int32)\n",
    "            })        \n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_val(self):\n",
    "        dataset = self.load_dataset(self.val_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_ids': [None],\n",
    "                'attention_mask': [None],\n",
    "                'label_ids': [None]\n",
    "            },\n",
    "            padding_values={\n",
    "                'input_ids': tf.constant(0, dtype=tf.int32),\n",
    "                'attention_mask': tf.constant(0, dtype=tf.int32),\n",
    "                'label_ids': tf.constant(0, dtype=tf.int32)\n",
    "            })\n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "# train = DataLoader(args).train\n",
    "\n",
    "# inputs = next(iter(train))\n",
    "# print(\"input_ids shape:\", inputs['input_ids'].shape)\n",
    "# print(\"attention_mask shape:\", inputs['attention_mask'].shape)\n",
    "# print(\"label_ids shape:\", inputs['label_ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CERMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"CER\", **kwargs):\n",
    "        super(CERMetric, self).__init__(name=name,  **kwargs)\n",
    "        self.accumulator = self.add_weight(name=\"total_cer\", initializer=\"zeros\")\n",
    "        self.counter = self.add_weight(name=\"cer_count\", initializer=\"zeros\")    \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        hypothesis = tf.cast(tf.sparse.from_dense(y_pred), dtype=tf.int32)\n",
    "\n",
    "        # Convert dense to sparse tensor for edit_distance function\n",
    "        truth = tf.RaggedTensor.from_tensor(y_true, padding=0).to_sparse()\n",
    "\n",
    "        # Calculate Levenshtein distance\n",
    "        distance = tf.edit_distance(hypothesis, truth, normalize=True)\n",
    "\n",
    "        # Add distance and number of samples to variables\n",
    "        self.accumulator.assign_add(tf.reduce_sum(distance))\n",
    "        self.counter.assign_add(len(y_true))\n",
    "\n",
    "    def result(self):\n",
    "        # Divides accumulated distance scores against number of samples passed,\n",
    "        # mimics mean reduction over batch\n",
    "        return tf.math.divide_no_nan(self.accumulator, self.counter)   \n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.accumulator.assign(0.0)\n",
    "        self.counter.assign(0.0)\n",
    "\n",
    "class CosineDecayWithWarmup(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "    def __call__(self, epoch):  \n",
    "        if epoch < self.args.warmup_epochs:\n",
    "            lr = ((self.args.lr_max - self.args.lr_start) / self.args.warmup_epochs) * epoch + self.args.lr_start\n",
    "        elif epoch < (self.args.warmup_epochs + self.args.sustain_epochs):\n",
    "            lr = self.args.lr_max\n",
    "        else:\n",
    "            progress = ((epoch - self.args.warmup_epochs - self.args.sustain_epochs) / \n",
    "            (self.args.epochs - self.args.warmup_epochs - self.args.sustain_epochs))\n",
    "            lr = (self.args.lr_max-self.args.lr_min) * (0.5 * (1.0 + tf.math.cos((22/7) * \n",
    "                self.args.n_cycles * 2.0 * progress)))\n",
    "            if self.args.lr_min is not None:\n",
    "                lr = tf.math.maximum(self.args.lr_min, lr)\n",
    "        return lr\n",
    "\n",
    "    def plot(self):\n",
    "        epochs = range(self.args.epochs+1)\n",
    "        lr = [self(epoch) for epoch in epochs]\n",
    "        plt.plot(epochs, lr)\n",
    "        plt.xlabel(\"learning_rate\")\n",
    "        plt.ylabel(\"epochs\")\n",
    "        plt.show()\n",
    "\n",
    "CosineDecayWithWarmup(args).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file created.\n",
      "Starting from epoch 1...\n",
      "Epoch 1/20: Learning rate @ 1.00e-08\n",
      "18750/18750 [==============================] - 6722s 358ms/step - loss: 0.8829 - cer: 0.2228 - val_loss: 0.4146 - val_cer: 0.1045\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Training\n",
      "Target:    ここへ来るといつも心が休まります\n",
      "Predicted: ここへ来るといつも心が休まますす\n",
      "Target:    あまりにもクソなことが山ほどあってな\n",
      "Predicted: あまりに木素なことが山ほほっってな\n",
      "Target:    だがあなたたちの努力に感謝する\n",
      "Predicted: だがあなたたちの努に感感謝する\n",
      "Target:    かつて手術に使ったようなものね\n",
      "Predicted: かつて手術に使ったようなものね\n",
      "\n",
      "Validation\n",
      "Target:    地図がいい加減なんだろう\n",
      "Predicted: 地図がいい加減んんんろろ\n",
      "Target:    お酒は毎日飲みますか\n",
      "Predicted: お酒は毎日の見ますか\n",
      "Target:    坊やおかあさんは何処だい\n",
      "Predicted: 坊ややォかささんはどこだい\n",
      "Target:    いくらなんでも無理じゃろう\n",
      "Predicted: いくらなんでも無理じゃろう\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 2/20: Learning rate @ 5.00e-05\n",
      "18749/18750 [============================>.] - ETA: 0s - loss: 0.3677 - cer: 0.1211"
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.train_dataset = DataLoader(args).train\n",
    "        self.val_dataset = DataLoader(args).val\n",
    "\n",
    "        self.tokenizer = BertJapaneseTokenizer(\n",
    "            vocab_file=f\"{self.args.main_dir}/bert_vocab.txt\",\n",
    "            do_lower_case=False,\n",
    "            do_word_tokenize=False,\n",
    "            do_subword_tokenize=True,\n",
    "            word_tokenizer_type=\"mecab\",\n",
    "            subword_tokenizer_type=\"character\")\n",
    "        \n",
    "        self.model = self.Kana2Kanji(args)\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam(CosineDecayWithWarmup(args))\n",
    "        self.loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=False)\n",
    "        self.cer_metric = CERMetric()\n",
    "\n",
    "        self.model_name = f\"model_{int(self.args.n_samples/1000)}k\"\n",
    "        self.log_path = f\"{self.args.main_dir}/bert_model_weights/{self.model_name}.csv\"\n",
    "        if not os.path.exists(self.log_path):\n",
    "            print(\"Log file created.\")\n",
    "            columns = \"epoch,loss,cer,val_loss,val_cer\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(columns)\n",
    "\n",
    "    def Kana2Kanji(self, args):\n",
    "        input_ids = Input(type_spec=tf.TensorSpec(\n",
    "            shape=(args.batch_size, None), dtype=tf.int32), name=\"input_ids\")\n",
    "        mask = Input(type_spec=tf.TensorSpec(\n",
    "            shape=(args.batch_size, None), dtype=tf.int32), name=\"attention_mask\")\n",
    "\n",
    "        bert = TFBertModel.from_pretrained(\n",
    "            \"cl-tohoku/bert-base-japanese-char\",\n",
    "            output_hidden_states=False,\n",
    "            output_attentions=False,\n",
    "            name=\"bert_model\")\n",
    "\n",
    "        x = bert(input_ids=input_ids, attention_mask=mask).last_hidden_state\n",
    "        x = TimeDistributed(Dense(args.vocab_size, activation=\"softmax\"), name=\"output\")(x, mask=mask)\n",
    "\n",
    "        return tf.keras.Model(inputs=[input_ids, mask], outputs=x, name=\"Kana2Kanji\")\n",
    "\n",
    "    def decoder(self, labels, logits):\n",
    "        labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        logits = tf.argmax(logits, axis=-1)\n",
    "        logits = self.tokenizer.batch_decode(logits, skip_special_tokens=True)\n",
    "        return labels, logits\n",
    "\n",
    "    def display(self, epoch, t_labels, t_logits, v_labels, v_logits):\n",
    "        print(\"-\" * 129)\n",
    "        print(\"Training\")\n",
    "        for y_true, y_pred in zip(t_labels[:4], t_logits[:4]):\n",
    "            print(f\"Target:    {y_true.replace(' ', '')}\")\n",
    "            print(f\"Predicted: {y_pred.replace(' ', '')}\") \n",
    "\n",
    "        print(\"\\nValidation\")\n",
    "        for y_true, y_pred in zip(v_labels[:4], v_logits[:4]):\n",
    "            print(f\"Target:    {y_true.replace(' ', '')}\")\n",
    "            print(f\"Predicted: {y_pred.replace(' ', '')}\")\n",
    "        print(\"-\" * 129)\n",
    "        \n",
    "    def fit(self):\n",
    "        # Checkpointing\n",
    "        self.ckpt_dir = f\"{self.args.main_dir}/bert_checkpoints\"\n",
    "        self.ckpt = tf.train.Checkpoint(self.model)\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(\n",
    "            checkpoint=self.ckpt, directory=self.ckpt_dir, max_to_keep=5)\n",
    "\n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            self.start_epoch = int(self.ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "            self.ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "            print(f\"Resuming from epoch {self.start_epoch + 1}...\")\n",
    "        else:\n",
    "            self.start_epoch = 0\n",
    "            print(\"Starting from epoch 1...\")\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.args.epochs+1):\n",
    "            print(f\"Epoch {epoch+1}/{self.args.epochs}: Learning rate @ {self.optimizer.lr(epoch):.2e}\")\n",
    "            stateful_metrics = [\"loss\", \"cer\", \"val_loss\", \"val_cer\"]\n",
    "            progbar = tf.keras.utils.Progbar(\n",
    "                self.args.train_steps, interval=0.05,\n",
    "                stateful_metrics=stateful_metrics)\n",
    "\n",
    "            # Training loop\n",
    "            for step, t_batch in enumerate(self.train_dataset):\n",
    "                t_input_ids = t_batch['input_ids']\n",
    "                t_attention_mask = t_batch['attention_mask']\n",
    "                t_labels = t_batch['label_ids']\n",
    "                with tf.GradientTape() as tape:\n",
    "                    t_logits = self.model(\n",
    "                        [t_input_ids, t_attention_mask],\n",
    "                        training=True)\n",
    "                    t_loss = self.loss_fn(\n",
    "                        t_labels, t_logits, sample_weight=t_attention_mask)\n",
    "                    self.cer_metric.update_state(t_labels, t_logits)\n",
    "                gradients = tape.gradient(t_loss, self.model.trainable_weights)\n",
    "                self.optimizer.apply_gradients(zip(gradients, self.model.trainable_weights))\n",
    "                t_cer = self.cer_metric.result()\n",
    "                t_values = [(\"loss\", t_loss), (\"cer\", t_cer)]\n",
    "                progbar.update(step, values=t_values, finalize=False)\n",
    "            \n",
    "            t_labels, t_logits = self.decoder(t_labels, t_logits)\n",
    "            self.cer_metric.reset_state()\n",
    "\n",
    "            # Validation loop\n",
    "            for v_batch in self.val_dataset:\n",
    "                v_input_ids = v_batch['input_ids']\n",
    "                v_attention_mask = v_batch['attention_mask']\n",
    "                v_labels = v_batch['label_ids']\n",
    "                v_logits = self.model(\n",
    "                    [v_input_ids, v_attention_mask],\n",
    "                    training=False)\n",
    "                v_loss = self.loss_fn(\n",
    "                    v_labels, v_logits, sample_weight=v_attention_mask)\n",
    "                self.cer_metric.update_state(v_labels, v_logits)\n",
    "            v_labels, v_logits = self.decoder(v_labels, v_logits)\n",
    "            v_cer = self.cer_metric.result()\n",
    "            v_values = [\n",
    "                (\"loss\", t_loss),\n",
    "                (\"cer\", t_cer),\n",
    "                (\"val_loss\", v_loss),\n",
    "                (\"val_cer\", v_cer)]\n",
    "            progbar.update(self.args.train_steps, values=v_values, finalize=True)\n",
    "            self.cer_metric.reset_state()\n",
    "\n",
    "            # Print sample transcriptions for both loops\n",
    "            self.display(epoch, t_labels, t_logits, v_labels, v_logits)\n",
    "\n",
    "            # Checkpointing\n",
    "            self.ckpt.save(file_prefix=f\"{self.ckpt_dir}/{self.model_name}\")\n",
    "\n",
    "            # Logging\n",
    "            log = f\"{epoch+1},{t_loss},{t_cer},{v_loss},{v_cer}\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(log)\n",
    "\n",
    "            save_path = f\"{self.args.main_dir}/bert_model_weights\"\n",
    "            self.model.save_weights(f\"{save_path}/{self.model_name}_{epoch+1}.h5\")\n",
    "\n",
    "Trainer(args).fit()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b851d2923cfa3a2562599062e05fd9893d86a7c009c64d8ad3756552e4dd5f41"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('tf-gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
