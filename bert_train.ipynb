{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import cutlet\n",
    "import argparse\n",
    "import MeCab\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "from convert_romaji import Romaji2Kana\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Dense, \n",
    "    TimeDistributed)\n",
    "\n",
    "from transformers import (\n",
    "    BertJapaneseTokenizer,\n",
    "    TFBertModel,\n",
    "    GradientAccumulator,\n",
    "    logging)\n",
    "\n",
    "def seed_everything(SEED):\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    print(\"Random seed set.\")\n",
    "\n",
    "seed_everything(42)\n",
    "tf.get_logger().setLevel('FATAL')\n",
    "logging.set_verbosity_error()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=64, buffer_size=1024, epochs=20, learning_rate=1e-05, lr_max=1e-05, lr_min=1e-08, lr_start=1e-08, main_dir='E://Datasets/Decoder_model', n_cycles=0.5, n_samples=1000000, n_shards=20, n_train=800000, n_val=200000, random_state=42, sustain_epochs=0, test_size=0.2, train_steps=12500, val_steps=3125, vocab_size=4000, warmup_epochs=2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ArgParser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Dataset\n",
    "    parser.add_argument(\"--main_dir\", default=\"E://Datasets/Decoder_model\")\n",
    "    parser.add_argument(\"--random_state\", default=42)\n",
    "    parser.add_argument(\"--n_shards\", default=20)\n",
    "    parser.add_argument(\"--n_samples\", default=1000000)\n",
    "    parser.add_argument(\"--test_size\", default=0.2)\n",
    "    parser.add_argument(\"--vocab_size\", default=4000)\n",
    "    parser.add_argument(\"--batch_size\", default=64)\n",
    "    parser.add_argument(\"--buffer_size\", default=1024)\n",
    "\n",
    "    # Scheduler\n",
    "    parser.add_argument(\"--epochs\", default=20)\n",
    "    parser.add_argument(\"--learning_rate\", default=1e-5)\n",
    "    parser.add_argument(\"--lr_start\", default=1e-8)\n",
    "    parser.add_argument(\"--lr_min\", default=1e-8)\n",
    "    parser.add_argument(\"--lr_max\", default=1e-5)\n",
    "    parser.add_argument(\"--n_cycles\", default=0.5)\n",
    "    parser.add_argument(\"--warmup_epochs\", default=2)\n",
    "    parser.add_argument(\"--sustain_epochs\", default=0)\n",
    "\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    n_train = int(args.n_samples * (1 - args.test_size))\n",
    "    n_val = int(args.n_samples * args.test_size)\n",
    "    train_steps = int(np.ceil(n_train / args.batch_size))\n",
    "    val_steps = int(np.ceil(n_val / args.batch_size))\n",
    "    \n",
    "    # Trainer\n",
    "    parser.add_argument(\"--n_train\", default=n_train)\n",
    "    parser.add_argument(\"--n_val\", default=n_val)\n",
    "    parser.add_argument(\"--train_steps\", default=train_steps)  \n",
    "    parser.add_argument(\"--val_steps\", default=val_steps)\n",
    "\n",
    "    return parser.parse_known_args()[0]\n",
    "\n",
    "args = ArgParser()\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset:\n",
    "    def __init__(self):\n",
    "        self.main_dir = \"D:\\School-stuff\\Sem-2\\PR-Project\\HoloASR\\Datasets\"\n",
    "        opus_ja_paths = glob.glob(f\"{self.main_dir}\\OPUS100-dataset\\*.ja\")\n",
    "        tatoeba_ja_paths = glob.glob(f\"{self.main_dir}\\Tatoeba-dataset\\*.ja\")\n",
    "        self.jesc_path = f\"{self.main_dir}/JESC-dataset/raw\"\n",
    "        self.ja_paths = opus_ja_paths + tatoeba_ja_paths\n",
    "        self.kanji_unicode = self.get_kanji_unicode()\n",
    "        self.katsu = cutlet.Cutlet()\n",
    "        self.katsu.use_foreign_spelling = False\n",
    "\n",
    "        tqdm.pandas()\n",
    "        self.data = pd.DataFrame({\"raw_text\": self.get_data()})\n",
    "\n",
    "        # Remove rows that contains non-kanji characters\n",
    "        self.data = self.data[self.data['raw_text'].progress_apply(self.check_kanji)]   \n",
    "\n",
    "        # Remove words within parenthesis\n",
    "        parenthesis =  r\"\\（.*\\）|\\(.*\\)|\\「.*\\」|\\『.*\\』\"\n",
    "        self.data = self.data[~self.data['raw_text'].str.contains(parenthesis)]\n",
    "\n",
    "        # Remove punctuations from sentences\n",
    "        self.data['raw_text'] = self.data['raw_text'].progress_apply(self.clean_kanji)\n",
    "\n",
    "        # Converts kanji to hiragana sentences\n",
    "        self.data['hira_text'] = self.data['raw_text'].progress_apply(self.kanji2hira)\n",
    "\n",
    "        # Remove null rows\n",
    "        self.data = self.data[~(self.data['raw_text']==\"\") | ~(self.data['hira_text']==\"\")]\n",
    "        self.data = self.data.dropna().reset_index(drop=True)\n",
    "\n",
    "        # Generate vocab file\n",
    "        self.vocab_file = r\"E:\\Datasets\\Language_model\\bert_vocab.txt\"\n",
    "        self.get_vocab(self.data)\n",
    "\n",
    "        # Construct tokenizer\n",
    "        self.tokenizer = BertJapaneseTokenizer(\n",
    "            vocab_file=self.vocab_file,\n",
    "            do_lower_case=False,\n",
    "            do_word_tokenize=False,\n",
    "            do_subword_tokenize=True,\n",
    "            word_tokenizer_type=\"mecab\",\n",
    "            subword_tokenizer_type=\"character\")\n",
    "\n",
    "        # Tokenize inputs and labels\n",
    "        self.data['input_ids'] = self.data['hira_text'].progress_apply(\n",
    "            lambda x: self.tokenizer(x).input_ids)\n",
    "        self.data['label_ids'] = self.data['raw_text'].progress_apply(\n",
    "            lambda x: self.tokenizer(x).input_ids)\n",
    "\n",
    "        # Apply padding to either input or labels to same length\n",
    "        new_input_ids, new_label_ids = [], []\n",
    "        for row_idx in tqdm(range(len(self.data)), total=len(self.data)):\n",
    "            input_ids, label_ids = self.pad_longest(row_idx)\n",
    "            new_input_ids.append(input_ids)\n",
    "            new_label_ids.append(label_ids)\n",
    "\n",
    "        self.data['input_ids'] = new_input_ids\n",
    "        self.data['label_ids'] = new_label_ids\n",
    "        self.data['input_len'] = self.data['input_ids'].apply(len)\n",
    "\n",
    "        # Remove rows that has unknown tokens\n",
    "        self.data = self.data[~self.data['input_ids'].apply(\n",
    "            lambda x: self.tokenizer.unk_token_id in x)]\n",
    "        self.data = self.data[~self.data['label_ids'].apply(\n",
    "            lambda x: self.tokenizer.unk_token_id in x)]\n",
    "        self.data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Save to csv\n",
    "        self.data.to_csv(\n",
    "            r\"E:\\Datasets\\Language_model\\bert_data.csv\", \n",
    "            encoding=\"utf-8\", index=False)\n",
    "\n",
    "    def get_kanji_unicode(self):\n",
    "        vocab = set()\n",
    "        with open(f\"{self.main_dir}\\kanji_unicode.txt\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                for char in line.split()[1:]:\n",
    "                    vocab.add(char)\n",
    "        return \"|\".join(sorted(vocab))\n",
    "\n",
    "    def get_data(self):\n",
    "        ja_lines = []\n",
    "        for ja_path in self.ja_paths:\n",
    "            with open(ja_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f.readlines():\n",
    "                    line = line.strip(\"\\n| \")\n",
    "                    ja_lines.append(line)\n",
    "        with open(self.jesc_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            texts = [text.split(\"\\t\") for text in f.readlines()]\n",
    "            for _, line in texts:\n",
    "                line = line.strip(\"\\n| \")\n",
    "                ja_lines.append(line)\n",
    "        return ja_lines\n",
    "\n",
    "    def check_kanji(self, sentence):\n",
    "        pattern = f\"[^{self.kanji_unicode}]\"\n",
    "        if re.match(pattern, sentence) == None:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def clean_kanji(self, sentence):\n",
    "        sentence = \"\".join(sentence.split())\n",
    "        pattern = r\"[・\\。\\！\\.\\？\\、]\"\n",
    "        sentence = re.sub(pattern, \"\", sentence)\n",
    "        return sentence\n",
    "\n",
    "    def kanji2hira(self, sentence):\n",
    "        try:\n",
    "            sentence = self.katsu.romaji(sentence)\n",
    "            sentence = sentence.replace(\" \", \"\").lower()\n",
    "            sentence = Romaji2Kana(sentence)\n",
    "        except:\n",
    "            sentence = None\n",
    "        return sentence\n",
    "\n",
    "    def get_vocab(self, data):\n",
    "        vocab = []\n",
    "        texts = data['raw_text'].tolist() + data['hira_text'].tolist()\n",
    "        for text in tqdm(texts):\n",
    "            for char in text:\n",
    "                vocab.append(char)\n",
    "\n",
    "        tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "        \n",
    "        for i in Counter(vocab).most_common():\n",
    "            if i[0] in self.kanji_unicode:\n",
    "                tokens.append(i[0])\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for token in tokens[:args.vocab_size]:\n",
    "                f.write(token + \"\\n\")    \n",
    "\n",
    "    def pad_longest(self, row):\n",
    "        input_len = len(self.data['input_ids'][row])\n",
    "        label_len = len(self.data['label_ids'][row])\n",
    "        input_ids = self.data['input_ids'][row]\n",
    "        label_ids = self.data['label_ids'][row]\n",
    "        if label_len > input_len:\n",
    "            pad_width = label_len - input_len\n",
    "            input_ids = np.pad(\n",
    "                self.data['input_ids'][row], pad_width=(0, pad_width)).tolist()\n",
    "        elif label_len < input_len:\n",
    "            pad_width = input_len - label_len\n",
    "            label_ids = np.pad(\n",
    "                self.data['label_ids'][row], pad_width=(0, pad_width)).tolist()\n",
    "        return input_ids, label_ids\n",
    "\n",
    "# data = BertDataset().data\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFRWriter():\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.data = self.get_data()\n",
    "\n",
    "    def get_data(self):\n",
    "        tqdm.pandas()\n",
    "        data = pd.read_csv(f\"{self.args.main_dir}/bert_data.csv\")\n",
    "        data = data.dropna().reset_index(drop=True)\n",
    "        data['input_ids'] = data['input_ids'].progress_apply(ast.literal_eval)\n",
    "        data['label_ids'] = data['label_ids'].progress_apply(ast.literal_eval)\n",
    "        data = data.query(f\"input_len <= {data['input_len'].quantile(0.90)}\")\n",
    "        data = data.sample(n=self.args.n_samples, random_state=self.args.random_state)\n",
    "        data = data.sort_values(by=\"input_len\", ascending=True, ignore_index=True)\n",
    "        return data[['input_ids', 'label_ids']]\n",
    "\n",
    "    def _bytes_feature(self, value):\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    def serialize_example(self, *args):\n",
    "        feature = {\n",
    "            'input_ids': self._bytes_feature(args[0]),\n",
    "            'attention_mask': self._bytes_feature(args[1]),\n",
    "            'label_ids': self._bytes_feature(args[2])}\n",
    "\n",
    "        example_proto = tf.train.Example(\n",
    "            features=tf.train.Features(feature=feature))\n",
    "        return example_proto.SerializeToString()\n",
    "\n",
    "    def get_shards(self):\n",
    "        skf = KFold(n_splits=self.args.n_shards, shuffle=False)\n",
    "        return [j for i,j in skf.split(self.data)]\n",
    "\n",
    "    def get_shard_data(self, samples):\n",
    "        for sample in samples:\n",
    "            input_ids = tf.convert_to_tensor(\n",
    "                self.data['input_ids'][sample], dtype=tf.int32)\n",
    "            attention_mask = tf.where(input_ids != 0, x=1, y=0)\n",
    "            label_ids = tf.convert_to_tensor(\n",
    "                self.data['label_ids'][sample], dtype=tf.int32)\n",
    "            yield {\n",
    "                \"input_ids\": tf.io.serialize_tensor(input_ids),\n",
    "                \"attention_mask\": tf.io.serialize_tensor(attention_mask),\n",
    "                \"label_ids\": tf.io.serialize_tensor(label_ids)}\n",
    "\n",
    "    def write(self):\n",
    "        for shard, samples in tqdm(enumerate(self.get_shards()), total=self.args.n_shards):\n",
    "            with tf.io.TFRecordWriter(f\"{self.args.main_dir}/bert_tfrec/shard_{shard+1}.tfrec\") as f:\n",
    "                for sample in self.get_shard_data(samples):\n",
    "                    example = self.serialize_example(\n",
    "                        sample['input_ids'],\n",
    "                        sample['attention_mask'],\n",
    "                        sample['label_ids'])\n",
    "                    f.write(example)\n",
    "\n",
    "# TFRWriter(args).write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, args):\n",
    "        self.files = glob.glob(args.main_dir + \"/bert_tfrec/*.tfrec\")\n",
    "        self.args = args\n",
    "        self.AUTOTUNE = tf.data.AUTOTUNE\n",
    "        self.train_files, self.val_files = train_test_split(\n",
    "            self.files, test_size=args.test_size, shuffle=True,\n",
    "            random_state=args.random_state)\n",
    "        self.train = self.get_train()\n",
    "        self.val = self.get_val()\n",
    "\n",
    "    def read_tfrecord(self, example):\n",
    "        feature_description = {\n",
    "            'input_ids': tf.io.FixedLenFeature([], tf.string),\n",
    "            'attention_mask': tf.io.FixedLenFeature([], tf.string),\n",
    "            'label_ids': tf.io.FixedLenFeature([], tf.string)\n",
    "            }\n",
    "        \n",
    "        example = tf.io.parse_single_example(example, feature_description)\n",
    "        example['input_ids'] = tf.io.parse_tensor(\n",
    "            example['input_ids'], out_type=tf.int32)\n",
    "        example['attention_mask'] = tf.io.parse_tensor(\n",
    "            example['attention_mask'], out_type=tf.int32) \n",
    "        example['label_ids'] = tf.io.parse_tensor(\n",
    "            example['label_ids'], out_type=tf.int32)\n",
    "        return example\n",
    "\n",
    "    def load_dataset(self, files):\n",
    "        ignore_order = tf.data.Options()\n",
    "        ignore_order.experimental_deterministic = False\n",
    "        dataset = tf.data.TFRecordDataset(files)\n",
    "        dataset = dataset.with_options(ignore_order)\n",
    "        dataset = dataset.map(self.read_tfrecord, num_parallel_calls=self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_train(self):\n",
    "        dataset = self.load_dataset(self.train_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_ids': [None],\n",
    "                'attention_mask': [None],\n",
    "                'label_ids': [None]\n",
    "            },\n",
    "            padding_values={\n",
    "                'input_ids': tf.constant(0, dtype=tf.int32),\n",
    "                'attention_mask': tf.constant(0, dtype=tf.int32),\n",
    "                'label_ids': tf.constant(0, dtype=tf.int32)\n",
    "            })        \n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_val(self):\n",
    "        dataset = self.load_dataset(self.val_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_ids': [None],\n",
    "                'attention_mask': [None],\n",
    "                'label_ids': [None]\n",
    "            },\n",
    "            padding_values={\n",
    "                'input_ids': tf.constant(0, dtype=tf.int32),\n",
    "                'attention_mask': tf.constant(0, dtype=tf.int32),\n",
    "                'label_ids': tf.constant(0, dtype=tf.int32)\n",
    "            })\n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "# train = DataLoader(args).train\n",
    "\n",
    "# inputs = next(iter(train))\n",
    "# print(\"input_ids shape:\", inputs['input_ids'].shape)\n",
    "# print(\"attention_mask shape:\", inputs['attention_mask'].shape)\n",
    "# print(\"label_ids shape:\", inputs['label_ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CERMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"CER\", **kwargs):\n",
    "        super(CERMetric, self).__init__(name=name,  **kwargs)\n",
    "        self.accumulator = self.add_weight(name=\"total_cer\", initializer=\"zeros\")\n",
    "        self.counter = self.add_weight(name=\"cer_count\", initializer=\"zeros\")    \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        hypothesis = tf.cast(tf.sparse.from_dense(y_pred), dtype=tf.int32)\n",
    "\n",
    "        # Convert dense to sparse tensor for edit_distance function\n",
    "        truth = tf.RaggedTensor.from_tensor(y_true, padding=0).to_sparse()\n",
    "\n",
    "        # Calculate Levenshtein distance\n",
    "        distance = tf.edit_distance(hypothesis, truth, normalize=True)\n",
    "\n",
    "        # Add distance and number of samples to variables\n",
    "        self.accumulator.assign_add(tf.reduce_sum(distance))\n",
    "        self.counter.assign_add(len(y_true))\n",
    "\n",
    "    def result(self):\n",
    "        # Divides accumulated distance scores against number of samples passed,\n",
    "        # mimics mean reduction over batch\n",
    "        return tf.math.divide_no_nan(self.accumulator, self.counter)   \n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.accumulator.assign(0.0)\n",
    "        self.counter.assign(0.0)\n",
    "\n",
    "class CosineDecayWithWarmup(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "    def __call__(self, epoch):  \n",
    "        if epoch < self.args.warmup_epochs:\n",
    "            lr = ((self.args.lr_max - self.args.lr_start) / self.args.warmup_epochs) * epoch + self.args.lr_start\n",
    "        elif epoch < (self.args.warmup_epochs + self.args.sustain_epochs):\n",
    "            lr = self.args.lr_max\n",
    "        else:\n",
    "            progress = ((epoch - self.args.warmup_epochs - self.args.sustain_epochs) / \n",
    "            (self.args.epochs - self.args.warmup_epochs - self.args.sustain_epochs))\n",
    "            lr = (self.args.lr_max-self.args.lr_min) * (0.5 * (1.0 + tf.math.cos((22/7) * \n",
    "                self.args.n_cycles * 2.0 * progress)))\n",
    "            if self.args.lr_min is not None:\n",
    "                lr = tf.math.maximum(self.args.lr_min, lr)\n",
    "        return lr\n",
    "\n",
    "    def plot(self):\n",
    "        epochs = range(self.args.epochs+1)\n",
    "        lr = [self(epoch) for epoch in epochs]\n",
    "        plt.plot(epochs, lr)\n",
    "        plt.xlabel(\"learning_rate\")\n",
    "        plt.ylabel(\"epochs\")\n",
    "        plt.show()\n",
    "\n",
    "CosineDecayWithWarmup(args).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file created.\n",
      "Starting from epoch 1...\n",
      "Epoch 1/20: Learning rate @ 1.00e-08\n",
      "12500/12500 [==============================] - 4127s 330ms/step - loss: 1.7415 - cer: 0.4927 - val_loss: 1.3679 - val_cer: 0.2569\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Training\n",
      "Target:    リストに載ってるメイズだおねえちゃん\n",
      "Predicted: リストにのってるメズずおおねちゃゃん\n",
      "Target:    うまく履けないの手伝って店長\n",
      "Predicted: うマくけないの手手つってて\n",
      "Target:    先日君たちに説明したように\n",
      "Predicted: 先実く達ちに説明ししたうう\n",
      "Target:    佐藤さんは英語をじょうずに話す\n",
      "Predicted: サささんはえををを上ずずすすす\n",
      "\n",
      "Validation\n",
      "Target:    ドアには内側から鍵が\n",
      "Predicted: ドあにはう違がかかかが\n",
      "Target:    私って自己中だな\n",
      "Predicted: 私ってじ事中だなな\n",
      "Target:    彼女は几帳面でね\n",
      "Predicted: 彼女はきちめでねね\n",
      "Target:    一日いてすぐ帰ります\n",
      "Predicted: 一にいいてぐぐえります\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 2/20: Learning rate @ 5.01e-06\n",
      "12500/12500 [==============================] - 4144s 331ms/step - loss: 1.4558 - cer: 0.3204 - val_loss: 0.9433 - val_cer: 0.1932\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Training\n",
      "Target:    奴は一匹狼だそして死んだ\n",
      "Predicted: 奴は一日ききかだそしししだだ\n",
      "Target:    だが食物の味を楽しんでいた\n",
      "Predicted: だが食物のあをを楽死んいた\n",
      "Target:    彼はその問題をらくらくと解いた\n",
      "Predicted: 彼はその問題をらくくととい\n",
      "Target:    私について何も知らないはずだ\n",
      "Predicted: 私について何も知知ないはずず\n",
      "\n",
      "Validation\n",
      "Target:    ドアには内側から鍵が\n",
      "Predicted: ドアにはう違わからがが\n",
      "Target:    私って自己中だな\n",
      "Predicted: 私って事事中だな\n",
      "Target:    彼女は几帳面でね\n",
      "Predicted: 彼女は気長面でね\n",
      "Target:    一日いてすぐ帰ります\n",
      "Predicted: 一日ちいてぐぐえります\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 3/20: Learning rate @ 9.99e-06\n",
      "12500/12500 [==============================] - 4202s 336ms/step - loss: 0.6398 - cer: 0.2577 - val_loss: 0.7772 - val_cer: 0.1658\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Training\n",
      "Target:    扉押さえて\n",
      "Predicted: 飛ささてて\n",
      "Target:    戦略だと\n",
      "Predicted: 戦力だと\n",
      "Target:    ベッキーはどう\n",
      "Predicted: ベッキーはどう\n",
      "Target:    ではまた後で\n",
      "Predicted: ではまた後で\n",
      "\n",
      "Validation\n",
      "Target:    ドアには内側から鍵が\n",
      "Predicted: ドアにはう違わから鍵\n",
      "Target:    私って自己中だな\n",
      "Predicted: 私って事故中だな\n",
      "Target:    彼女は几帳面でね\n",
      "Predicted: 彼女は貴長面でね\n",
      "Target:    一日いてすぐ帰ります\n",
      "Predicted: 一日ちいてすぐ帰ります\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 4/20: Learning rate @ 9.91e-06\n",
      "12500/12500 [==============================] - 4194s 335ms/step - loss: 0.9292 - cer: 0.2213 - val_loss: 0.6485 - val_cer: 0.1633\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Training\n",
      "Target:    ある日市場を妻と歩いていたら\n",
      "Predicted: ある久示を妻まと歩いいいたら\n",
      "Target:    鬱陶しい鬱陶しい鬱陶しい\n",
      "Predicted: うっとうしううととううううしい\n",
      "Target:    俺も最近のヤマを確認しとく\n",
      "Predicted: 俺も最近の山を確認認しく\n",
      "Target:    ブラジルには別の大学院生と\n",
      "Predicted: ブラジルには別の大学員生と\n",
      "\n",
      "Validation\n",
      "Target:    ドアには内側から鍵が\n",
      "Predicted: ドアにはう違からら鍵\n",
      "Target:    私って自己中だな\n",
      "Predicted: 私って自故中だな\n",
      "Target:    彼女は几帳面でね\n",
      "Predicted: 彼女は貴長面でね\n",
      "Target:    一日いてすぐ帰ります\n",
      "Predicted: 一日ちいてぐぐ帰ります\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 5/20: Learning rate @ 9.69e-06\n",
      "12500/12500 [==============================] - 4184s 335ms/step - loss: 0.5693 - cer: 0.1967 - val_loss: 0.5902 - val_cer: 0.1484\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Training\n",
      "Target:    もうその頃には\n",
      "Predicted: もうその頃には\n",
      "Target:    待って待ってくれ\n",
      "Predicted: 待って待ってくれ\n",
      "Target:    はいかけてみたら\n",
      "Predicted: はいかけてみたら\n",
      "Target:    それは聞きました\n",
      "Predicted: それは聞きました\n",
      "\n",
      "Validation\n",
      "Target:    ドアには内側から鍵が\n",
      "Predicted: ドアにはう違からら鍵\n",
      "Target:    私って自己中だな\n",
      "Predicted: 私って事故中だな\n",
      "Target:    彼女は几帳面でね\n",
      "Predicted: 彼女は貴長面でね\n",
      "Target:    一日いてすぐ帰ります\n",
      "Predicted: 一日位てすぐぐ帰ります\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 6/20: Learning rate @ 9.32e-06\n",
      "12500/12500 [==============================] - 4163s 333ms/step - loss: 0.6604 - cer: 0.1784 - val_loss: 0.5585 - val_cer: 0.1342\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Training\n",
      "Target:    それには準備がいる旅行しよう\n",
      "Predicted: それには準備がいる旅ししう\n",
      "Target:    英語話者に通じなきゃ意味ないぞ\n",
      "Predicted: 英語は者に通通なきゃ味味ないぞ\n",
      "Target:    その後にアンソニーを助けに戻る\n",
      "Predicted: その後にフンンニニー助助け戻る\n",
      "Target:    一覧に名を連ねすらしないのです\n",
      "Predicted: 一乱になをらねららしないのです\n",
      "\n",
      "Validation\n",
      "Target:    ドアには内側から鍵が\n",
      "Predicted: ドアにはう側から鍵が\n",
      "Target:    私って自己中だな\n",
      "Predicted: 私って自故中だな\n",
      "Target:    彼女は几帳面でね\n",
      "Predicted: 彼女は貴長面でね\n",
      "Target:    一日いてすぐ帰ります\n",
      "Predicted: 一日いてすぐぐりりすす\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 7/20: Learning rate @ 8.82e-06\n",
      "12500/12500 [==============================] - 4227s 338ms/step - loss: 0.7142 - cer: 0.1648 - val_loss: 0.4999 - val_cer: 0.1346\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Training\n",
      "Target:    奇跡なんて言葉で片づけちゃダメだ\n",
      "Predicted: 奇跡なんて言葉で片付けちゃダメだ\n",
      "Target:    あぁいえそれはないですが他のところなら\n",
      "Predicted: あいえそれはないですが他のところなら\n",
      "Target:    あの夜にあった事をちゃんと話して\n",
      "Predicted: あの夜に会ったことをちゃんと話てて\n",
      "Target:    一応報告したんだけどさそしたら\n",
      "Predicted: 一応報告したんだけど誘ししたら\n",
      "\n",
      "Validation\n",
      "Target:    ドアには内側から鍵が\n",
      "Predicted: ドアにはう側から鍵が\n",
      "Target:    私って自己中だな\n",
      "Predicted: 私って自故中だな\n",
      "Target:    彼女は几帳面でね\n",
      "Predicted: 彼女は貴長面でね\n",
      "Target:    一日いてすぐ帰ります\n",
      "Predicted: 一日いてすぐ帰りますす\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 8/20: Learning rate @ 8.20e-06\n",
      "10589/12500 [========================>.....] - ETA: 9:48 - loss: 0.4672 - cer: 0.1539"
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.train_dataset = DataLoader(args).train\n",
    "        self.val_dataset = DataLoader(args).val\n",
    "\n",
    "        self.tokenizer = BertJapaneseTokenizer(\n",
    "            vocab_file=f\"{self.args.main_dir}/bert_vocab.txt\",\n",
    "            do_lower_case=False,\n",
    "            do_word_tokenize=False,\n",
    "            do_subword_tokenize=True,\n",
    "            word_tokenizer_type=\"mecab\",\n",
    "            subword_tokenizer_type=\"character\")\n",
    "        \n",
    "        self.model = self.Kana2Kanji(args)\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam(CosineDecayWithWarmup(args))\n",
    "        self.loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=False)\n",
    "        self.cer_metric = CERMetric()\n",
    "\n",
    "        self.model_name = f\"model_{int(self.args.n_samples/1000)}k\"\n",
    "        self.log_path = f\"{self.args.main_dir}/bert_model_weights/{self.model_name}.csv\"\n",
    "        if not os.path.exists(self.log_path):\n",
    "            print(\"Log file created.\")\n",
    "            columns = \"epoch,loss,cer,val_loss,val_cer\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(columns)\n",
    "\n",
    "    def Kana2Kanji(self, args):\n",
    "        input_ids = Input(type_spec=tf.TensorSpec(\n",
    "            shape=(args.batch_size, None), dtype=tf.int32), name=\"input_ids\")\n",
    "        mask = Input(type_spec=tf.TensorSpec(\n",
    "            shape=(args.batch_size, None), dtype=tf.int32), name=\"attention_mask\")\n",
    "\n",
    "        bert = TFBertModel.from_pretrained(\n",
    "            \"cl-tohoku/bert-base-japanese-char\",\n",
    "            output_hidden_states=False,\n",
    "            output_attentions=False,\n",
    "            name=\"bert_model\")\n",
    "\n",
    "        x = bert(input_ids=input_ids, attention_mask=mask).last_hidden_state\n",
    "        x = TimeDistributed(Dense(args.vocab_size, activation=\"softmax\"), name=\"output\")(x, mask=mask)\n",
    "\n",
    "        return tf.keras.Model(inputs=[input_ids, mask], outputs=x, name=\"Kana2Kanji\")\n",
    "\n",
    "    def decoder(self, labels, logits):\n",
    "        labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        logits = tf.argmax(logits, axis=-1)\n",
    "        logits = self.tokenizer.batch_decode(logits, skip_special_tokens=True)\n",
    "        return labels, logits\n",
    "\n",
    "    def display(self, epoch, t_labels, t_logits, v_labels, v_logits):\n",
    "        print(\"-\" * 129)\n",
    "        print(\"Training\")\n",
    "        for y_true, y_pred in zip(t_labels[:4], t_logits[:4]):\n",
    "            print(f\"Target:    {y_true.replace(' ', '')}\")\n",
    "            print(f\"Predicted: {y_pred.replace(' ', '')}\") \n",
    "\n",
    "        print(\"\\nValidation\")\n",
    "        for y_true, y_pred in zip(v_labels[:4], v_logits[:4]):\n",
    "            print(f\"Target:    {y_true.replace(' ', '')}\")\n",
    "            print(f\"Predicted: {y_pred.replace(' ', '')}\")\n",
    "        print(\"-\" * 129)\n",
    "        \n",
    "    def fit(self):\n",
    "        # Checkpointing\n",
    "        self.ckpt_dir = f\"{self.args.main_dir}/bert_checkpoints\"\n",
    "        self.ckpt = tf.train.Checkpoint(self.model)\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(\n",
    "            checkpoint=self.ckpt, directory=self.ckpt_dir, max_to_keep=5)\n",
    "\n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            self.start_epoch = int(self.ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "            self.ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "            print(f\"Resuming from epoch {self.start_epoch + 1}...\")\n",
    "        else:\n",
    "            self.start_epoch = 0\n",
    "            print(\"Starting from epoch 1...\")\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.args.epochs+1):\n",
    "            print(f\"Epoch {epoch+1}/{self.args.epochs}: Learning rate @ {self.optimizer.lr(epoch):.2e}\")\n",
    "            stateful_metrics = [\"loss\", \"cer\", \"val_loss\", \"val_cer\"]\n",
    "            progbar = tf.keras.utils.Progbar(\n",
    "                self.args.train_steps, interval=0.05,\n",
    "                stateful_metrics=stateful_metrics)\n",
    "\n",
    "            # Training loop\n",
    "            for step, t_batch in enumerate(self.train_dataset):\n",
    "                t_input_ids = t_batch['input_ids']\n",
    "                t_attention_mask = t_batch['attention_mask']\n",
    "                t_labels = t_batch['label_ids']\n",
    "                with tf.GradientTape() as tape:\n",
    "                    t_logits = self.model(\n",
    "                        [t_input_ids, t_attention_mask],\n",
    "                        training=True)\n",
    "                    t_loss = self.loss_fn(\n",
    "                        t_labels, t_logits, sample_weight=t_attention_mask)\n",
    "                    self.cer_metric.update_state(t_labels, t_logits)\n",
    "                gradients = tape.gradient(t_loss, self.model.trainable_weights)\n",
    "                self.optimizer.apply_gradients(zip(gradients, self.model.trainable_weights))\n",
    "                t_cer = self.cer_metric.result()\n",
    "                t_values = [(\"loss\", t_loss), (\"cer\", t_cer)]\n",
    "                progbar.update(step, values=t_values, finalize=False)\n",
    "            \n",
    "            t_labels, t_logits = self.decoder(t_labels, t_logits)\n",
    "            self.cer_metric.reset_state()\n",
    "\n",
    "            # Validation loop\n",
    "            for v_batch in self.val_dataset:\n",
    "                v_input_ids = v_batch['input_ids']\n",
    "                v_attention_mask = v_batch['attention_mask']\n",
    "                v_labels = v_batch['label_ids']\n",
    "                v_logits = self.model(\n",
    "                    [v_input_ids, v_attention_mask],\n",
    "                    training=False)\n",
    "                v_loss = self.loss_fn(\n",
    "                    v_labels, v_logits, sample_weight=v_attention_mask)\n",
    "                self.cer_metric.update_state(v_labels, v_logits)\n",
    "            v_labels, v_logits = self.decoder(v_labels, v_logits)\n",
    "            v_cer = self.cer_metric.result()\n",
    "            v_values = [\n",
    "                (\"loss\", t_loss),\n",
    "                (\"cer\", t_cer),\n",
    "                (\"val_loss\", v_loss),\n",
    "                (\"val_cer\", v_cer)]\n",
    "            progbar.update(self.args.train_steps, values=v_values, finalize=True)\n",
    "            self.cer_metric.reset_state()\n",
    "\n",
    "            # Print sample transcriptions for both loops\n",
    "            self.display(epoch, t_labels, t_logits, v_labels, v_logits)\n",
    "\n",
    "            # Checkpointing\n",
    "            self.ckpt.save(file_prefix=f\"{self.ckpt_dir}/{self.model_name}\")\n",
    "\n",
    "            # Logging\n",
    "            log = f\"{epoch+1},{t_loss},{t_cer},{v_loss},{v_cer}\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(log)\n",
    "\n",
    "            save_path = f\"{self.args.main_dir}/bert_model_weights\"\n",
    "            self.model.save_weights(f\"{save_path}/{self.model_name}_{epoch+1}.h5\")\n",
    "\n",
    "Trainer(args).fit()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b851d2923cfa3a2562599062e05fd9893d86a7c009c64d8ad3756552e4dd5f41"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('tf-gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
