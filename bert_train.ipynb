{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import regex as re\n",
    "import ast\n",
    "import glob\n",
    "import random\n",
    "import cutlet\n",
    "import argparse\n",
    "from itertools import groupby\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from convert_romaji import Romaji2Kana\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (Input, Dense, Dropout, TimeDistributed)\n",
    "\n",
    "from transformers import (\n",
    "    BertJapaneseTokenizer,\n",
    "    TFBertModel,\n",
    "    logging)\n",
    "\n",
    "def seed_everything(SEED):\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    print(\"Random seed set.\")\n",
    "\n",
    "seed_everything(42)\n",
    "tf.get_logger().setLevel('FATAL')\n",
    "logging.set_verbosity_error()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=32, buffer_size=1024, epochs=15, learning_rate=5e-05, lr_max=5e-05, lr_min=1e-09, lr_start=1e-09, main_dir='E://Datasets/Decoder_model', n_cycles=0.5, n_samples=500000, n_shards=4, n_train=375000, n_val=125000, random_state=42, sustain_epochs=0, test_size=0.25, train_steps=11719, val_steps=3907, vocab_size=4000, warmup_epochs=3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ArgParser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Dataset\n",
    "    parser.add_argument(\"--main_dir\", default=\"E://Datasets/Decoder_model\")\n",
    "    parser.add_argument(\"--random_state\", default=42)\n",
    "    parser.add_argument(\"--n_shards\", default=4)\n",
    "    parser.add_argument(\"--n_samples\", default=500000)\n",
    "    parser.add_argument(\"--test_size\", default=0.25)\n",
    "    parser.add_argument(\"--vocab_size\", default=4000)\n",
    "    parser.add_argument(\"--batch_size\", default=32)\n",
    "    parser.add_argument(\"--buffer_size\", default=1024)\n",
    "\n",
    "    # Scheduler\n",
    "    parser.add_argument(\"--epochs\", default=15)\n",
    "    parser.add_argument(\"--learning_rate\", default=5e-5)\n",
    "    parser.add_argument(\"--lr_start\", default=1e-9)\n",
    "    parser.add_argument(\"--lr_min\", default=1e-9)\n",
    "    parser.add_argument(\"--lr_max\", default=5e-5)\n",
    "    parser.add_argument(\"--n_cycles\", default=0.5)\n",
    "    parser.add_argument(\"--warmup_epochs\", default=3)\n",
    "    parser.add_argument(\"--sustain_epochs\", default=0)\n",
    "\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    n_train = int(args.n_samples * (1 - args.test_size))\n",
    "    n_val = int(args.n_samples * args.test_size)\n",
    "    train_steps = int(np.ceil(n_train / args.batch_size))\n",
    "    val_steps = int(np.ceil(n_val / args.batch_size))\n",
    "    \n",
    "    # Trainer\n",
    "    parser.add_argument(\"--n_train\", default=n_train)\n",
    "    parser.add_argument(\"--n_val\", default=n_val)\n",
    "    parser.add_argument(\"--train_steps\", default=train_steps)  \n",
    "    parser.add_argument(\"--val_steps\", default=val_steps)\n",
    "\n",
    "    return parser.parse_known_args()[0]\n",
    "\n",
    "args = ArgParser()\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset:\n",
    "    def __init__(self):\n",
    "        self.main_dir = \"D:\\School-stuff\\Sem-2\\PR-Project\\HoloASR\\Datasets\"\n",
    "        opus_ja_paths = glob.glob(f\"{self.main_dir}\\OPUS100-dataset\\*.ja\")\n",
    "        tatoeba_ja_paths = glob.glob(f\"{self.main_dir}\\Tatoeba-dataset\\*.ja\")\n",
    "        self.ja_paths = opus_ja_paths + tatoeba_ja_paths\n",
    "        self.jesc_path = f\"{self.main_dir}/JESC-dataset/raw\"\n",
    "        self.cc100_path = f\"{self.main_dir}/ja(1).txt\"\n",
    "        self.kanji_unicode = self.get_kanji_unicode()\n",
    "        self.katsu = cutlet.Cutlet()\n",
    "        self.katsu.use_foreign_spelling = False\n",
    "\n",
    "        tqdm.pandas()\n",
    "        self.data = pd.DataFrame({\"raw_text\": self.get_data()})\n",
    "\n",
    "        # Remove rows that contains non-kanji characters\n",
    "        self.data = self.data[self.data['raw_text'].progress_apply(self.check_kanji)]   \n",
    "\n",
    "        # Remove words within parenthesis\n",
    "        parenthesis =  r\"\\（.*\\）|\\(.*\\)|\\「.*\\」|\\『.*\\』\"\n",
    "        self.data = self.data[~self.data['raw_text'].str.contains(parenthesis)]\n",
    "\n",
    "        # Remove punctuations from sentences\n",
    "        self.data['raw_text'] = self.data['raw_text'].progress_apply(self.clean_kanji)\n",
    "\n",
    "        # Converts kanji to hiragana sentences\n",
    "        self.data['hira_text'] = self.data['raw_text'].progress_apply(self.kanji2hira)\n",
    "\n",
    "        # Remove null rows\n",
    "        self.data = self.data[~(self.data['raw_text']==\"\") | ~(self.data['hira_text']==\"\")]\n",
    "        self.data = self.data[\n",
    "            (~self.data['raw_text'].duplicated()) & \n",
    "            (~self.data['hira_text'].duplicated())]\n",
    "        self.data = self.data.dropna().reset_index(drop=True)\n",
    "\n",
    "        # Generate vocab file\n",
    "        self.vocab_file = r\"E:\\Datasets\\Decoder_model\\bert_vocab.txt\"\n",
    "        self.get_vocab(self.data)\n",
    "\n",
    "        # Construct tokenizer\n",
    "        self.tokenizer = BertJapaneseTokenizer(\n",
    "            vocab_file=self.vocab_file,\n",
    "            do_lower_case=False,\n",
    "            do_word_tokenize=True,\n",
    "            do_subword_tokenize=True,\n",
    "            word_tokenizer_type=\"mecab\",\n",
    "            subword_tokenizer_type=\"character\")\n",
    "\n",
    "        # Tokenize inputs and labels\n",
    "        self.data['input_ids'] = self.data['hira_text'].progress_apply(\n",
    "            lambda x: self.tokenizer(x).input_ids)\n",
    "        self.data['label_ids'] = self.data['raw_text'].progress_apply(\n",
    "            lambda x: self.tokenizer(x).input_ids)\n",
    "\n",
    "        # Apply padding to either input or labels to same length\n",
    "        new_input_ids, new_label_ids = [], []\n",
    "        for row_idx in tqdm(range(len(self.data)), total=len(self.data)):\n",
    "            input_ids, label_ids = self.pad_longest(row_idx)\n",
    "            new_input_ids.append(input_ids)\n",
    "            new_label_ids.append(label_ids)\n",
    "\n",
    "        self.data['input_ids'] = new_input_ids\n",
    "        self.data['label_ids'] = new_label_ids\n",
    "        self.data['input_len'] = self.data['input_ids'].apply(len)\n",
    "\n",
    "        # Save to csv\n",
    "        self.data.to_csv(\n",
    "            r\"E:\\Datasets\\Decoder_model\\bert_data.csv\", \n",
    "            encoding=\"utf-8\", index=False)\n",
    "\n",
    "    def get_kanji_unicode(self):\n",
    "        vocab = set()\n",
    "        with open(f\"{self.main_dir}\\kanji_unicode.txt\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                for char in line.split()[1:]:\n",
    "                    vocab.add(char)\n",
    "        return \"|\".join(sorted(vocab))\n",
    "\n",
    "    def get_data(self):\n",
    "        ja_lines = []\n",
    "        for ja_path in self.ja_paths:\n",
    "            with open(ja_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f.readlines():\n",
    "                    line = line.strip(\"\\n| \")\n",
    "                    ja_lines.append(line)\n",
    "        with open(self.jesc_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            texts = [text.split(\"\\t\") for text in f.readlines()]\n",
    "            for _, line in texts:\n",
    "                line = line.strip(\"\\n| \")\n",
    "                ja_lines.append(line)\n",
    "        with open(self.cc100_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            texts = random.sample(f.readlines(), 7000000)\n",
    "            for line in texts:\n",
    "                line = line.strip(\"\\n| \")\n",
    "                ja_lines.append(line)            \n",
    "        return ja_lines\n",
    "\n",
    "    def check_kanji(self, sentence):\n",
    "        pattern = f\"[^{self.kanji_unicode}]\"\n",
    "        match = re.findall(pattern, sentence)\n",
    "        if match != []:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def clean_kanji(self, sentence):\n",
    "        sentence = \"\".join(sentence.split())\n",
    "        pattern = f\"[^{self.kanji_unicode}]\"\n",
    "        sentence = re.sub(pattern, \"\", sentence)\n",
    "        return sentence\n",
    "\n",
    "    def kanji2hira(self, sentence):\n",
    "        try:\n",
    "            sentence = self.katsu.romaji(sentence)\n",
    "            sentence = sentence.replace(\" \", \"\")\n",
    "            sentence = sentence.replace(\"。\", \"\").lower()\n",
    "            sentence = Romaji2Kana(sentence)\n",
    "        except:\n",
    "            sentence = None\n",
    "        return sentence\n",
    "\n",
    "    def get_vocab(self, data):\n",
    "        vocab = []\n",
    "        texts = data['raw_text'].tolist() + data['hira_text'].tolist()\n",
    "        for text in tqdm(texts):\n",
    "            for char in text:\n",
    "                vocab.append(char)\n",
    "\n",
    "        tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\"]\n",
    "        \n",
    "        for i in Counter(vocab).most_common():\n",
    "            if i[0] in self.kanji_unicode:\n",
    "                tokens.append(i[0])\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for token in tokens[:args.vocab_size]:\n",
    "                f.write(token + \"\\n\")    \n",
    "\n",
    "    def pad_longest(self, row):\n",
    "        input_len = len(self.data['input_ids'][row])\n",
    "        label_len = len(self.data['label_ids'][row])\n",
    "        input_ids = self.data['input_ids'][row]\n",
    "        label_ids = self.data['label_ids'][row]\n",
    "        if label_len > input_len:\n",
    "            pad_width = label_len - input_len\n",
    "            input_ids = np.pad(\n",
    "                self.data['input_ids'][row], pad_width=(0, pad_width)).tolist()\n",
    "        elif label_len < input_len:\n",
    "            pad_width = input_len - label_len\n",
    "            label_ids = np.pad(\n",
    "                self.data['label_ids'][row], pad_width=(0, pad_width)).tolist()\n",
    "        return input_ids, label_ids\n",
    "\n",
    "# data = BertDataset().data\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_char(text):\n",
    "#     for i, char in enumerate(vocab):\n",
    "#         if char in text:\n",
    "#             return int(i)\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "# with open(r\"E:\\Datasets\\Decoder_model\\bert_vocab.txt\", encoding=\"utf-8\") as f:\n",
    "#     vocab = [v.strip(\"\\n\") for v in f.readlines()[4:][::-1]]\n",
    "\n",
    "# tqdm.pandas()\n",
    "# data = pd.read_csv(r\"E:\\Datasets\\Decoder_model\\bert_data.csv\", encoding=\"utf-8\")\n",
    "# data = data.dropna().reset_index(drop=True)\n",
    "# q1 = data['input_len'].quantile(0.1)\n",
    "# q2 = data['input_len'].quantile(0.9)\n",
    "# data = data[data['input_len'].between(q1, q2)]\n",
    "# data['char'] = data['raw_text'].progress_apply(get_char)\n",
    "# data = data.query(\"raw_text != hira_text\")\n",
    "# data = data.sort_values(by=\"char\").reset_index(drop=True)[:-70000]\n",
    "# data.to_csv(r\"E:\\Datasets\\Decoder_model\\bert_datav2.csv\", index=False, encoding=\"utf-8\")\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(r\"E:\\Datasets\\Decoder_model\\bert_datav2.csv\", encoding=\"utf-8\")\n",
    "# data = data.dropna().reset_index(drop=True)\n",
    "\n",
    "# uniq = []\n",
    "# for text in tqdm(data['raw_text'][:500000]):\n",
    "#     for char in text:\n",
    "#         if char in vocab:\n",
    "#             uniq.append(char)\n",
    "\n",
    "# Counter(uniq).most_common()[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFRWriter():\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.vocab = self.get_vocab()\n",
    "        self.data = self.get_data()\n",
    "\n",
    "    def get_data(self):\n",
    "        tqdm.pandas()\n",
    "        data = pd.read_csv(f\"{self.args.main_dir}/bert_datav2.csv\")\n",
    "        data = data.dropna().reset_index(drop=True)\n",
    "        data = data[:self.args.n_samples]\n",
    "        data['input_ids'] = data['input_ids'].progress_apply(ast.literal_eval)\n",
    "        data['label_ids'] = data['label_ids'].progress_apply(ast.literal_eval)\n",
    "        data = data.sort_values(by=\"input_len\", ascending=True, ignore_index=True)\n",
    "        data.to_csv(f\"{self.args.main_dir}/bert_datav3.csv\", index=False, encoding=\"utf-8\")\n",
    "        return data[['input_ids', 'label_ids', 'char']]\n",
    "\n",
    "    def get_vocab(self):\n",
    "        with open(r\"E:\\Datasets\\Decoder_model\\bert_vocab.txt\", encoding=\"utf-8\") as f:\n",
    "            vocab = {k.strip(\"\\n\"): 0 for k in f.readlines()[4:]}\n",
    "\n",
    "    def _bytes_feature(self, value):\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    def serialize_example(self, *args):\n",
    "        feature = {\n",
    "            'input_ids': self._bytes_feature(args[0]),\n",
    "            'attention_mask': self._bytes_feature(args[1]),\n",
    "            'label_ids': self._bytes_feature(args[2])}\n",
    "\n",
    "        example_proto = tf.train.Example(\n",
    "            features=tf.train.Features(feature=feature))\n",
    "        return example_proto.SerializeToString()\n",
    "\n",
    "    def get_shards(self):\n",
    "        skf = StratifiedKFold(n_splits=self.args.n_shards)\n",
    "        return [j for i,j in skf.split(self.data, self.data['char'])]\n",
    "\n",
    "    def get_shard_data(self, samples):\n",
    "        for sample in samples:\n",
    "            input_ids = tf.convert_to_tensor(\n",
    "                self.data['input_ids'][sample], dtype=tf.int32)\n",
    "            attention_mask = tf.where(input_ids != 0, x=1, y=0)\n",
    "            label_ids = tf.convert_to_tensor(\n",
    "                self.data['label_ids'][sample], dtype=tf.int32)\n",
    "            yield {\n",
    "                \"input_ids\": tf.io.serialize_tensor(input_ids),\n",
    "                \"attention_mask\": tf.io.serialize_tensor(attention_mask),\n",
    "                \"label_ids\": tf.io.serialize_tensor(label_ids)}\n",
    "\n",
    "    def write(self):\n",
    "        for shard, samples in tqdm(enumerate(self.get_shards()), total=self.args.n_shards):\n",
    "            with tf.io.TFRecordWriter(f\"{self.args.main_dir}/bert_tfrec/shard_{shard+1}.tfrec\") as f:\n",
    "                for sample in self.get_shard_data(samples):\n",
    "                    example = self.serialize_example(\n",
    "                        sample['input_ids'],\n",
    "                        sample['attention_mask'],\n",
    "                        sample['label_ids'])\n",
    "                    f.write(example)\n",
    "\n",
    "# TFRWriter(args).write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, args):\n",
    "        self.files = glob.glob(args.main_dir + \"/bert_tfrec/*.tfrec\")\n",
    "        self.args = args\n",
    "        self.AUTOTUNE = tf.data.AUTOTUNE\n",
    "        self.train_files, self.val_files = train_test_split(\n",
    "            self.files, test_size=args.test_size, shuffle=True,\n",
    "            random_state=args.random_state)\n",
    "        self.train = self.get_train()\n",
    "        self.val = self.get_val()\n",
    "\n",
    "    def read_tfrecord(self, example):\n",
    "        feature_description = {\n",
    "            'input_ids': tf.io.FixedLenFeature([], tf.string),\n",
    "            'attention_mask': tf.io.FixedLenFeature([], tf.string),\n",
    "            'label_ids': tf.io.FixedLenFeature([], tf.string)}\n",
    "        \n",
    "        example = tf.io.parse_single_example(example, feature_description)\n",
    "        example['input_ids'] = tf.io.parse_tensor(\n",
    "            example['input_ids'], out_type=tf.int32)\n",
    "        example['attention_mask'] = tf.io.parse_tensor(\n",
    "            example['attention_mask'], out_type=tf.int32) \n",
    "        example['label_ids'] = tf.io.parse_tensor(\n",
    "            example['label_ids'], out_type=tf.int32)\n",
    "        return example\n",
    "\n",
    "    def load_dataset(self, files):\n",
    "        ignore_order = tf.data.Options()\n",
    "        ignore_order.experimental_deterministic = False\n",
    "        dataset = tf.data.TFRecordDataset(files)\n",
    "        dataset = dataset.with_options(ignore_order)\n",
    "        dataset = dataset.map(self.read_tfrecord, num_parallel_calls=self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_train(self):\n",
    "        dataset = self.load_dataset(self.train_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_ids': [None],\n",
    "                'attention_mask': [None],\n",
    "                'label_ids': [None]},\n",
    "            padding_values={\n",
    "                'input_ids': tf.constant(0, dtype=tf.int32),\n",
    "                'attention_mask': tf.constant(0, dtype=tf.int32),\n",
    "                'label_ids': tf.constant(0, dtype=tf.int32)})        \n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_val(self):\n",
    "        dataset = self.load_dataset(self.val_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_ids': [None],\n",
    "                'attention_mask': [None],\n",
    "                'label_ids': [None]},\n",
    "            padding_values={\n",
    "                'input_ids': tf.constant(0, dtype=tf.int32),\n",
    "                'attention_mask': tf.constant(0, dtype=tf.int32),\n",
    "                'label_ids': tf.constant(0, dtype=tf.int32)})\n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "# train = DataLoader(args).train\n",
    "# inputs = next(iter(train))\n",
    "# print(\"input_ids shape:\", inputs['input_ids'].shape)\n",
    "# print(\"attention_mask shape:\", inputs['attention_mask'].shape)\n",
    "# print(\"label_ids shape:\", inputs['label_ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CERMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"CER\", **kwargs):\n",
    "        super(CERMetric, self).__init__(name=name,  **kwargs)\n",
    "        self.accumulator = self.add_weight(name=\"total_cer\", initializer=\"zeros\")\n",
    "        self.counter = self.add_weight(name=\"cer_count\", initializer=\"zeros\")    \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        hypothesis = tf.cast(tf.sparse.from_dense(y_pred), dtype=tf.int32)\n",
    "\n",
    "        # Convert dense to sparse tensor for edit_distance function\n",
    "        truth = tf.RaggedTensor.from_tensor(y_true, padding=0).to_sparse()\n",
    "\n",
    "        # Calculate Levenshtein distance\n",
    "        distance = tf.edit_distance(hypothesis, truth, normalize=True)\n",
    "\n",
    "        # Add distance and number of samples to variables\n",
    "        self.accumulator.assign_add(tf.reduce_sum(distance))\n",
    "        self.counter.assign_add(len(y_true))\n",
    "\n",
    "    def result(self):\n",
    "        # Divides accumulated distance scores against number of samples passed,\n",
    "        # mimics mean reduction over batch\n",
    "        return tf.math.divide_no_nan(self.accumulator, self.counter)   \n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.accumulator.assign(0.0)\n",
    "        self.counter.assign(0.0)\n",
    "\n",
    "class CosineDecayWithWarmup(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "    def __call__(self, epoch):  \n",
    "        if epoch < self.args.warmup_epochs:\n",
    "            lr = ((self.args.lr_max - self.args.lr_start) / self.args.warmup_epochs) * epoch + self.args.lr_start\n",
    "        elif epoch < (self.args.warmup_epochs + self.args.sustain_epochs):\n",
    "            lr = self.args.lr_max\n",
    "        else:\n",
    "            progress = ((epoch - self.args.warmup_epochs - self.args.sustain_epochs) / \n",
    "            (self.args.epochs - self.args.warmup_epochs - self.args.sustain_epochs))\n",
    "            lr = (self.args.lr_max-self.args.lr_min) * (0.5 * (1.0 + tf.math.cos((22/7) * \n",
    "                self.args.n_cycles * 2.0 * progress)))\n",
    "            if self.args.lr_min is not None:\n",
    "                lr = tf.math.maximum(self.args.lr_min, lr)\n",
    "        return lr\n",
    "\n",
    "    def plot(self):\n",
    "        epochs = range(self.args.epochs+1)\n",
    "        lr = [self(epoch) for epoch in epochs]\n",
    "        plt.plot(epochs, lr)\n",
    "        plt.xlabel(\"learning_rate\")\n",
    "        plt.ylabel(\"epochs\")\n",
    "        plt.show()\n",
    "\n",
    "CosineDecayWithWarmup(args).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kana2Kanji(args):\n",
    "    input_ids = Input(type_spec=tf.TensorSpec(\n",
    "        shape=(args.batch_size, None), dtype=tf.int32), name=\"input_ids\")\n",
    "    mask = Input(type_spec=tf.TensorSpec(\n",
    "        shape=(args.batch_size, None), dtype=tf.int32), name=\"attention_mask\")\n",
    "\n",
    "    bert = TFBertModel.from_pretrained(\n",
    "        \"cl-tohoku/bert-base-japanese-char-v2\",\n",
    "        output_hidden_states=False,\n",
    "        output_attentions=False,\n",
    "        num_attention_heads=16,\n",
    "        num_hidden_layers=16,\n",
    "        name=\"bert_model\")\n",
    "\n",
    "    x = bert(input_ids=input_ids, attention_mask=mask).last_hidden_state\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = TimeDistributed(\n",
    "        Dense(\n",
    "            args.vocab_size, \n",
    "            activation=\"softmax\"\n",
    "            ), \n",
    "        name=\"output\")(x, mask=mask)\n",
    "    return tf.keras.Model(inputs=[input_ids, mask], outputs=x, name=\"Kana2Kanji\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from epoch 4...\n",
      "Epoch 4/15: Learning rate @ 5.00e-05\n",
      "11719/11719 [==============================] - 4038s 344ms/step - loss: 1.1136 - cer: 0.2252 - val_loss: 0.7276 - val_cer: 0.1700\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Training\n",
      "Target:    そういえば今日は昭一君の命日だったね\n",
      "Predicted: そういえば今日は正一君の命日だったね\n",
      "Target:    あのプラトンだって同じことを嘆いていたのだから\n",
      "Predicted: あのプラトンだって同じことを嘆いててたののから\n",
      "Target:    ペテロはいま舟や積荷のことなど頭になかった\n",
      "Predicted: ペテロは今船やにににことなど頭頭なかっっ\n",
      "Target:    越中舟橋駅前ある舟橋郵便局\n",
      "Predicted: え中不不橋駅前前アフ橋有便便\n",
      "\n",
      "Validation\n",
      "Target:    怠惰がしばしば貧乏の原因になる\n",
      "Predicted: 怠惰がしばしば貧乏の原因になる\n",
      "Target:    大きな噴火にならないことが一番だが\n",
      "Predicted: 大きな噴火にならないことが一番だが\n",
      "Target:    成虫はこれらの木の森林に棲みます\n",
      "Predicted: 成虫はこれらの木の森林にみみます\n",
      "Target:    今全国ではの耕地が放棄されている\n",
      "Predicted: 今全国ではのコ置が蜂棄されている\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 5/15: Learning rate @ 4.91e-05\n",
      "11719/11719 [==============================] - 4011s 342ms/step - loss: 0.5444 - cer: 0.1993 - val_loss: 0.5950 - val_cer: 0.1515\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Training\n",
      "Target:    養蜂家はどんな仕事をしているのでしょうか\n",
      "Predicted: 養蜂家はどんな仕事をしているのでしょうか\n",
      "Target:    この言葉は男を侮辱するのに使われる\n",
      "Predicted: この言葉は男を侮辱するのに使われるる\n",
      "Target:    往にして過去が最も有効な武器なのである\n",
      "Predicted: 往にして過去最もも友好な武ななのあある\n",
      "Target:    ここの前のページと次のページの畳み掛けが\n",
      "Predicted: ここの前のページと次のページの畳かがが\n",
      "\n",
      "Validation\n",
      "Target:    怠惰がしばしば貧乏の原因になる\n",
      "Predicted: 怠惰がしばしば貧乏の原因になる\n",
      "Target:    大きな噴火にならないことが一番だが\n",
      "Predicted: 大きな噴火にならないことが一番だが\n",
      "Target:    成虫はこれらの木の森林に棲みます\n",
      "Predicted: 成虫はこれらの木の森林に住みます\n",
      "Target:    今全国ではの耕地が放棄されている\n",
      "Predicted: 今全国ではのコーが放棄されている\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 6/15: Learning rate @ 4.66e-05\n",
      "11719/11719 [==============================] - 4071s 347ms/step - loss: 0.9058 - cer: 0.1965 - val_loss: 0.5336 - val_cer: 0.1496\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Training\n",
      "Target:    第回稲垣杯水泳競技大会が終了しました\n",
      "Predicted: 第回稲木倍水水響議隊会が終了しまたた\n",
      "Target:    明後日から東京藝大卒業作品展が始まります\n",
      "Predicted: 明後日地らと虚と大大業作品品ががまります\n",
      "Target:    戦場で勇猛果敢に突撃する情景が目に浮かびます\n",
      "Predicted: 洗浄で猛猛果敢に突撃する条計が目に浮かびます\n",
      "Target:    私はその場しのぎの一時的な謙虚さを潔しとしない\n",
      "Predicted: 私はそのばしの議の時的的虚虚虚をを義義しししいい\n",
      "\n",
      "Validation\n",
      "Target:    怠惰がしばしば貧乏の原因になる\n",
      "Predicted: 怠惰がしばしば貧乏の原因になる\n",
      "Target:    大きな噴火にならないことが一番だが\n",
      "Predicted: 大きな噴火にならないことが一番だが\n",
      "Target:    成虫はこれらの木の森林に棲みます\n",
      "Predicted: 生虫はこれらの木の森林に棲みます\n",
      "Target:    今全国ではの耕地が放棄されている\n",
      "Predicted: 今前国ではのコーが蜂棄されている\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 7/15: Learning rate @ 4.27e-05\n",
      "11719/11719 [==============================] - 4128s 352ms/step - loss: 0.5895 - cer: 0.1800 - val_loss: 0.3765 - val_cer: 0.1390\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Training\n",
      "Target:    急遽体調不良で休むのは初めてのこと\n",
      "Predicted: 急遽隊長不良で休むのは初めててことと\n",
      "Target:    今後とも戦国大戦を宜しくお願い致します\n",
      "Predicted: 今後とも戦国戦戦をししくお願いたします\n",
      "Target:    さかなクンの家には水槽がつあり魚を飼っています\n",
      "Predicted: 坂君のの家は水槽がつつりギ魚をっていいす\n",
      "Target:    形蒸気機関車号機年米国ポーター製\n",
      "Predicted: 形蒸気機関車号記念米国ポーター製\n",
      "\n",
      "Validation\n",
      "Target:    怠惰がしばしば貧乏の原因になる\n",
      "Predicted: 怠惰がしばしば貧乏の原因になる\n",
      "Target:    大きな噴火にならないことが一番だが\n",
      "Predicted: 大きな噴火にならないことが一番だが\n",
      "Target:    成虫はこれらの木の森林に棲みます\n",
      "Predicted: 成虫はこれらの木の森林にみみます\n",
      "Target:    今全国ではの耕地が放棄されている\n",
      "Predicted: 今前国ではの耕ーが放棄されている\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 8/15: Learning rate @ 3.75e-05\n",
      "11719/11719 [==============================] - 4115s 351ms/step - loss: 0.9260 - cer: 0.1671 - val_loss: 0.4200 - val_cer: 0.1328\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Training\n",
      "Target:    寒いんで賄いは熱です寒いんで暖かくして下さいね\n",
      "Predicted: 寒ムで賄賄は熱です寒寒んで暖かくててくささい\n",
      "Target:    事業費は主に企業団体からの寄付で賄われました\n",
      "Predicted: 事業日は主に企業団体からの寄附で賄われました\n",
      "Target:    次に目覚めたとき彼は彼女の眷属に生まれ変わっていた\n",
      "Predicted: 次に目覚めたとき彼彼彼女の眷にに生まれ変わっていた\n",
      "Target:    然后因为眷恋人类会住在旧民宅的地板下\n",
      "Predicted: 然后元为是恋人就在住在的明民的的治至\n",
      "\n",
      "Validation\n",
      "Target:    怠惰がしばしば貧乏の原因になる\n",
      "Predicted: 怠惰がしばしば貧乏の原因になる\n",
      "Target:    大きな噴火にならないことが一番だが\n",
      "Predicted: 大きな噴火にならないことが一番だが\n",
      "Target:    成虫はこれらの木の森林に棲みます\n",
      "Predicted: 成虫はこれらの木の森林に棲みます\n",
      "Target:    今全国ではの耕地が放棄されている\n",
      "Predicted: 今全国ではの耕地が放棄されている\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 9/15: Learning rate @ 3.15e-05\n",
      "11719/11719 [==============================] - 4093s 349ms/step - loss: 0.5764 - cer: 0.1550 - val_loss: 0.3808 - val_cer: 0.1304\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Training\n",
      "Target:    不耐症や過敏症はグルテンだけに限りません\n",
      "Predicted: 不対症や過敏症はグルルテンだに限りままん\n",
      "Target:    一般的に鳥はトウガラシに敏感ではありません\n",
      "Predicted: 一般的に取りは唐辛しに敏感ははあままんん\n",
      "Target:    農業家畜省がそのプログラムを提供する\n",
      "Predicted: 農業家畜生がそのプググラムを提供する\n",
      "Target:    でそんなわけでランクが非常に肥大してきてしまった\n",
      "Predicted: でそんなわでララククが非に肥肥大してててしまった\n",
      "\n",
      "Validation\n",
      "Target:    怠惰がしばしば貧乏の原因になる\n",
      "Predicted: 怠惰がしばしば貧乏の原因になる\n",
      "Target:    大きな噴火にならないことが一番だが\n",
      "Predicted: 大きな噴火にならないことが一番だが\n",
      "Target:    成虫はこれらの木の森林に棲みます\n",
      "Predicted: 成虫はこれらの木の森林に棲みます\n",
      "Target:    今全国ではの耕地が放棄されている\n",
      "Predicted: 今全国ではの耕置が放棄されている\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 10/15: Learning rate @ 2.50e-05\n",
      "11719/11719 [==============================] - 4277s 365ms/step - loss: 0.5110 - cer: 0.1460 - val_loss: 0.3516 - val_cer: 0.1321\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Training\n",
      "Target:    陰窩には常在菌としていろいろな細菌がいます\n",
      "Predicted: カげにには錠材ととていいいいなな菌いいますす\n",
      "Target:    年バスティーユ監獄でひとりの囚人が急死した\n",
      "Predicted: 年バスティィユ監獄でで人人の囚人が休止した\n",
      "Target:    経膣分娩では赤ちゃんは膣を通って出てきます\n",
      "Predicted: 啓膣分娩では赤ちゃんは膣を通って出てきます\n",
      "Target:    おそらく顎の関節に何か異常がおきたのでしょう\n",
      "Predicted: 恐そらくアの節にににん情がが起たたのししょょ\n",
      "\n",
      "Validation\n",
      "Target:    怠惰がしばしば貧乏の原因になる\n",
      "Predicted: 怠惰がしばしば貧乏の原因になる\n",
      "Target:    大きな噴火にならないことが一番だが\n",
      "Predicted: 大きな噴火にならないことが一番だが\n",
      "Target:    成虫はこれらの木の森林に棲みます\n",
      "Predicted: 成虫はこれらの木の森林に棲みます\n",
      "Target:    今全国ではの耕地が放棄されている\n",
      "Predicted: 今全国ではの耕地が放棄されている\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 11/15: Learning rate @ 1.85e-05\n",
      "11719/11719 [==============================] - 4907s 419ms/step - loss: 0.8189 - cer: 0.1374 - val_loss: 0.3034 - val_cer: 0.1238\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Training\n",
      "Target:    郊外の林での戦闘シーンも相当な迫力\n",
      "Predicted: 郊外の林での戦闘し員も相当な白\n",
      "Target:    私達は車に押し込まれ郊外へ連れて行かれた\n",
      "Predicted: 私たちは車にし込込れ郊外へ連連て行行かた\n",
      "Target:    書道が趣味との事字のきれいな女性に惹かれます\n",
      "Predicted: 書動が趣味とのこじのききいなな性性惹かかれます\n",
      "Target:    幸橋の辺りで井の頭恩賜公園と離れる\n",
      "Predicted: 幸い橋の辺りりでのの白恩恩賜園園とれるるる\n",
      "\n",
      "Validation\n",
      "Target:    怠惰がしばしば貧乏の原因になる\n",
      "Predicted: 怠惰がしばしば貧乏の原因になる\n",
      "Target:    大きな噴火にならないことが一番だが\n",
      "Predicted: 大きな噴火にならないことが一番だが\n",
      "Target:    成虫はこれらの木の森林に棲みます\n",
      "Predicted: 生虫はこれらの木の森林に棲みます\n",
      "Target:    今全国ではの耕地が放棄されている\n",
      "Predicted: 今全国ではのコーチが放棄されいる\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 12/15: Learning rate @ 1.25e-05\n",
      "10213/11719 [=========================>....] - ETA: 8:47 - loss: 0.4541 - cer: 0.1313"
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.train_dataset = DataLoader(args).train\n",
    "        self.val_dataset = DataLoader(args).val\n",
    "\n",
    "        self.tokenizer = BertJapaneseTokenizer(\n",
    "            vocab_file=f\"{self.args.main_dir}/bert_vocab.txt\",\n",
    "            do_lower_case=False,\n",
    "            do_word_tokenize=True,\n",
    "            do_subword_tokenize=True,\n",
    "            word_tokenizer_type=\"mecab\",\n",
    "            subword_tokenizer_type=\"character\")\n",
    "        \n",
    "        self.model = self.Kana2Kanji(args)\n",
    "        \n",
    "        schedule = CosineDecayWithWarmup(args)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(schedule)\n",
    "        self.loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=False)\n",
    "        self.cer_metric = CERMetric()\n",
    "\n",
    "        self.model_name = f\"model_{int(self.args.n_samples/1000)}k_v2\"\n",
    "        self.log_path = f\"{self.args.main_dir}/bert_model_weights/{self.model_name}.csv\"\n",
    "        if not os.path.exists(self.log_path):\n",
    "            print(\"Log file created.\")\n",
    "            columns = \"epoch,loss,cer,val_loss,val_cer\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(columns)\n",
    "\n",
    "    def Kana2Kanji(self, args):\n",
    "        input_ids = Input(type_spec=tf.TensorSpec(\n",
    "            shape=(args.batch_size, None), dtype=tf.int32), name=\"input_ids\")\n",
    "        mask = Input(type_spec=tf.TensorSpec(\n",
    "            shape=(args.batch_size, None), dtype=tf.int32), name=\"attention_mask\")\n",
    "\n",
    "        bert = TFBertModel.from_pretrained(\n",
    "            \"cl-tohoku/bert-base-japanese-char-v2\",\n",
    "            output_hidden_states=False,\n",
    "            output_attentions=False,\n",
    "            num_attention_heads=16,\n",
    "            num_hidden_layers=16,\n",
    "            name=\"bert_model\")\n",
    "\n",
    "        x = bert(input_ids=input_ids, attention_mask=mask).last_hidden_state\n",
    "        x = Dropout(0.1)(x)\n",
    "        x = TimeDistributed(\n",
    "            Dense(\n",
    "                args.vocab_size, \n",
    "                activation=\"softmax\"\n",
    "                ), \n",
    "            name=\"output\")(x, mask=mask)\n",
    "        return tf.keras.Model(inputs=[input_ids, mask], outputs=x, name=\"Kana2Kanji\")\n",
    "\n",
    "    def display(self, epoch, t_labels, t_logits, v_labels, v_logits):\n",
    "        t_labels = self.tokenizer.batch_decode(t_labels, skip_special_tokens=True)\n",
    "        t_logits = self.tokenizer.batch_decode(t_logits, skip_special_tokens=True)\n",
    "        v_labels = self.tokenizer.batch_decode(v_labels, skip_special_tokens=True)\n",
    "        v_logits = self.tokenizer.batch_decode(v_logits, skip_special_tokens=True)\n",
    "\n",
    "        print(\"-\" * 129)\n",
    "        print(\"Training\")\n",
    "        for y_true, y_pred in zip(t_labels[:4], t_logits[:4]):\n",
    "            print(f\"Target:    {y_true.replace(' ', '')}\")\n",
    "            print(f\"Predicted: {y_pred.replace(' ', '')}\")\n",
    "\n",
    "        print(\"\\nValidation\")\n",
    "        for y_true, y_pred in zip(v_labels[:4], v_logits[:4]):\n",
    "            print(f\"Target:    {y_true.replace(' ', '')}\")\n",
    "            print(f\"Predicted: {y_pred.replace(' ', '')}\")\n",
    "        print(\"-\" * 129)\n",
    "        \n",
    "    def fit(self):\n",
    "        # Checkpointing\n",
    "        self.ckpt_dir = f\"{self.args.main_dir}/bert_checkpoints\"\n",
    "        self.ckpt = tf.train.Checkpoint(self.model)\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(\n",
    "            checkpoint=self.ckpt, directory=self.ckpt_dir, max_to_keep=5)\n",
    "\n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            self.start_epoch = int(self.ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "            self.ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "            print(f\"Resuming from epoch {self.start_epoch + 1}...\")\n",
    "        else:\n",
    "            self.start_epoch = 0\n",
    "            print(\"Starting from epoch 1...\")\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.args.epochs+1):\n",
    "            print(f\"Epoch {epoch+1}/{self.args.epochs}: Learning rate @ {self.optimizer.lr(epoch):.2e}\")\n",
    "            stateful_metrics = [\"loss\", \"cer\", \"val_loss\", \"val_cer\"]\n",
    "            progbar = tf.keras.utils.Progbar(\n",
    "                self.args.train_steps, interval=0.05,\n",
    "                stateful_metrics=stateful_metrics)\n",
    "\n",
    "            # Training loop\n",
    "            for step, t_batch in enumerate(self.train_dataset):\n",
    "                t_input_ids = t_batch['input_ids']\n",
    "                t_attention_mask = t_batch['attention_mask']\n",
    "                t_labels = t_batch['label_ids']\n",
    "                with tf.GradientTape() as tape:\n",
    "                    t_logits = self.model(\n",
    "                        [t_input_ids, t_attention_mask],\n",
    "                        training=True)                 \n",
    "                    t_loss = self.loss_fn(\n",
    "                        t_labels, t_logits, sample_weight=t_attention_mask)\n",
    "                gradients = tape.gradient(t_loss, self.model.trainable_weights)\n",
    "                self.optimizer.apply_gradients(zip(gradients, self.model.trainable_weights))\n",
    "                t_logits = tf.argmax(t_logits, axis=-1)\n",
    "                self.cer_metric.update_state(t_labels, t_logits)\n",
    "                t_cer = self.cer_metric.result()\n",
    "                t_values = [(\"loss\", t_loss), (\"cer\", t_cer)]\n",
    "                progbar.update(step, values=t_values, finalize=False)\n",
    "            \n",
    "            self.cer_metric.reset_state()\n",
    "\n",
    "            # Validation loop\n",
    "            for v_batch in self.val_dataset:\n",
    "                v_input_ids = v_batch['input_ids']\n",
    "                v_attention_mask = v_batch['attention_mask']\n",
    "                v_labels = v_batch['label_ids']\n",
    "                v_logits = self.model(\n",
    "                    [v_input_ids, v_attention_mask],\n",
    "                    training=False)\n",
    "                v_loss = self.loss_fn(\n",
    "                    v_labels, v_logits, sample_weight=v_attention_mask)\n",
    "                v_logits = tf.argmax(v_logits, axis=-1)    \n",
    "                self.cer_metric.update_state(v_labels, v_logits)\n",
    "                \n",
    "            v_cer = self.cer_metric.result()\n",
    "            v_values = [\n",
    "                (\"loss\", t_loss),\n",
    "                (\"cer\", t_cer),\n",
    "                (\"val_loss\", v_loss),\n",
    "                (\"val_cer\", v_cer)]\n",
    "            progbar.update(self.args.train_steps, values=v_values, finalize=True)\n",
    "            self.cer_metric.reset_state()\n",
    "\n",
    "            # Print sample transcriptions for both loops\n",
    "            self.display(epoch, t_labels, t_logits, v_labels, v_logits)\n",
    "\n",
    "            # Checkpointing\n",
    "            self.ckpt.save(file_prefix=f\"{self.ckpt_dir}/{self.model_name}\")\n",
    "\n",
    "            # Logging\n",
    "            log = f\"{epoch+1},{t_loss},{t_cer},{v_loss},{v_cer}\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(log)\n",
    "\n",
    "            save_path = f\"{self.args.main_dir}/bert_model_weights\"\n",
    "            self.model.save_weights(f\"{save_path}/{self.model_name}_{epoch+1}.h5\")\n",
    "\n",
    "Trainer(args).fit()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b851d2923cfa3a2562599062e05fd9893d86a7c009c64d8ad3756552e4dd5f41"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('tf-gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
