{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import regex as re\n",
    "import ast\n",
    "import glob\n",
    "import random\n",
    "import cutlet\n",
    "import argparse\n",
    "from itertools import groupby\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from convert_romaji import Romaji2Kana\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (Input, Dense, Dropout, TimeDistributed)\n",
    "\n",
    "from transformers import (\n",
    "    BertJapaneseTokenizer,\n",
    "    TFBertModel,\n",
    "    logging)\n",
    "\n",
    "def seed_everything(SEED):\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    print(\"Random seed set.\")\n",
    "\n",
    "seed_everything(42)\n",
    "tf.get_logger().setLevel('FATAL')\n",
    "logging.set_verbosity_error()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=64, buffer_size=1024, epochs=20, learning_rate=5e-05, lr_max=5e-05, lr_min=1e-09, lr_start=1e-09, main_dir='E://Datasets/Decoder_model', n_cycles=0.5, n_samples=500000, n_shards=10, n_train=450000, n_val=50000, random_state=42, sustain_epochs=0, test_size=0.1, train_steps=7032, val_steps=782, vocab_size=4000, warmup_epochs=3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ArgParser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Dataset\n",
    "    parser.add_argument(\"--main_dir\", default=\"E://Datasets/Decoder_model\")\n",
    "    parser.add_argument(\"--random_state\", default=42)\n",
    "    parser.add_argument(\"--n_shards\", default=10)\n",
    "    parser.add_argument(\"--n_samples\", default=500000)\n",
    "    parser.add_argument(\"--test_size\", default=0.1)\n",
    "    parser.add_argument(\"--vocab_size\", default=4000)\n",
    "    parser.add_argument(\"--batch_size\", default=64)\n",
    "    parser.add_argument(\"--buffer_size\", default=1024)\n",
    "\n",
    "    # Scheduler\n",
    "    parser.add_argument(\"--epochs\", default=20)\n",
    "    parser.add_argument(\"--learning_rate\", default=5e-5)\n",
    "    parser.add_argument(\"--lr_start\", default=1e-9)\n",
    "    parser.add_argument(\"--lr_min\", default=1e-9)\n",
    "    parser.add_argument(\"--lr_max\", default=5e-5)\n",
    "    parser.add_argument(\"--n_cycles\", default=0.5)\n",
    "    parser.add_argument(\"--warmup_epochs\", default=3)\n",
    "    parser.add_argument(\"--sustain_epochs\", default=0)\n",
    "\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    n_train = int(args.n_samples * (1 - args.test_size))\n",
    "    n_val = int(args.n_samples * args.test_size)\n",
    "    train_steps = int(np.ceil(n_train / args.batch_size))\n",
    "    val_steps = int(np.ceil(n_val / args.batch_size))\n",
    "    \n",
    "    # Trainer\n",
    "    parser.add_argument(\"--n_train\", default=n_train)\n",
    "    parser.add_argument(\"--n_val\", default=n_val)\n",
    "    parser.add_argument(\"--train_steps\", default=train_steps)  \n",
    "    parser.add_argument(\"--val_steps\", default=val_steps)\n",
    "\n",
    "    return parser.parse_known_args()[0]\n",
    "\n",
    "args = ArgParser()\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset:\n",
    "    def __init__(self):\n",
    "        self.main_dir = \"D:\\School-stuff\\Sem-2\\PR-Project\\HoloASR\\Datasets\"\n",
    "        opus_ja_paths = glob.glob(f\"{self.main_dir}\\OPUS100-dataset\\*.ja\")\n",
    "        tatoeba_ja_paths = glob.glob(f\"{self.main_dir}\\Tatoeba-dataset\\*.ja\")\n",
    "        self.ja_paths = opus_ja_paths + tatoeba_ja_paths\n",
    "        self.jesc_path = f\"{self.main_dir}/JESC-dataset/raw\"\n",
    "        self.cc100_path = f\"{self.main_dir}/ja(1).txt\"\n",
    "        self.kanji_unicode = self.get_kanji_unicode()\n",
    "        self.katsu = cutlet.Cutlet()\n",
    "        self.katsu.use_foreign_spelling = False\n",
    "\n",
    "        tqdm.pandas()\n",
    "        self.data = pd.DataFrame({\"raw_text\": self.get_data()})\n",
    "\n",
    "        # Remove rows that contains non-kanji characters\n",
    "        self.data = self.data[self.data['raw_text'].progress_apply(self.check_kanji)]   \n",
    "\n",
    "        # Remove words within parenthesis\n",
    "        parenthesis =  r\"\\（.*\\）|\\(.*\\)|\\「.*\\」|\\『.*\\』\"\n",
    "        self.data = self.data[~self.data['raw_text'].str.contains(parenthesis)]\n",
    "\n",
    "        # Remove punctuations from sentences\n",
    "        self.data['raw_text'] = self.data['raw_text'].progress_apply(self.clean_kanji)\n",
    "\n",
    "        # Converts kanji to hiragana sentences\n",
    "        self.data['hira_text'] = self.data['raw_text'].progress_apply(self.kanji2hira)\n",
    "\n",
    "        # Remove null rows\n",
    "        self.data = self.data[~(self.data['raw_text']==\"\") | ~(self.data['hira_text']==\"\")]\n",
    "        self.data = self.data[\n",
    "            (~self.data['raw_text'].duplicated()) & \n",
    "            (~self.data['hira_text'].duplicated())]\n",
    "        self.data = self.data.dropna().reset_index(drop=True)\n",
    "\n",
    "        # Generate vocab file\n",
    "        self.vocab_file = r\"E:\\Datasets\\Decoder_model\\bert_vocab.txt\"\n",
    "        self.get_vocab(self.data)\n",
    "\n",
    "        # Construct tokenizer\n",
    "        self.tokenizer = BertJapaneseTokenizer(\n",
    "            vocab_file=self.vocab_file,\n",
    "            do_lower_case=False,\n",
    "            do_word_tokenize=True,\n",
    "            do_subword_tokenize=True,\n",
    "            word_tokenizer_type=\"mecab\",\n",
    "            subword_tokenizer_type=\"character\")\n",
    "\n",
    "        # Tokenize inputs and labels\n",
    "        self.data['input_ids'] = self.data['hira_text'].progress_apply(\n",
    "            lambda x: self.tokenizer(x).input_ids)\n",
    "        self.data['label_ids'] = self.data['raw_text'].progress_apply(\n",
    "            lambda x: self.tokenizer(x).input_ids)\n",
    "\n",
    "        # Apply padding to either input or labels to same length\n",
    "        new_input_ids, new_label_ids = [], []\n",
    "        for row_idx in tqdm(range(len(self.data)), total=len(self.data)):\n",
    "            input_ids, label_ids = self.pad_longest(row_idx)\n",
    "            new_input_ids.append(input_ids)\n",
    "            new_label_ids.append(label_ids)\n",
    "\n",
    "        self.data['input_ids'] = new_input_ids\n",
    "        self.data['label_ids'] = new_label_ids\n",
    "        self.data['input_len'] = self.data['input_ids'].apply(len)\n",
    "\n",
    "        # Save to csv\n",
    "        self.data.to_csv(\n",
    "            r\"E:\\Datasets\\Decoder_model\\bert_data.csv\", \n",
    "            encoding=\"utf-8\", index=False)\n",
    "\n",
    "    def get_kanji_unicode(self):\n",
    "        vocab = set()\n",
    "        with open(f\"{self.main_dir}\\kanji_unicode.txt\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                for char in line.split()[1:]:\n",
    "                    vocab.add(char)\n",
    "        return \"|\".join(sorted(vocab))\n",
    "\n",
    "    def get_data(self):\n",
    "        ja_lines = []\n",
    "        for ja_path in self.ja_paths:\n",
    "            with open(ja_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f.readlines():\n",
    "                    line = line.strip(\"\\n| \")\n",
    "                    ja_lines.append(line)\n",
    "        with open(self.jesc_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            texts = [text.split(\"\\t\") for text in f.readlines()]\n",
    "            for _, line in texts:\n",
    "                line = line.strip(\"\\n| \")\n",
    "                ja_lines.append(line)\n",
    "        with open(self.cc100_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            texts = random.sample(f.readlines(), 7000000)\n",
    "            for line in texts:\n",
    "                line = line.strip(\"\\n| \")\n",
    "                ja_lines.append(line)            \n",
    "        return ja_lines\n",
    "\n",
    "    def check_kanji(self, sentence):\n",
    "        pattern = f\"[^{self.kanji_unicode}]\"\n",
    "        match = re.findall(pattern, sentence)\n",
    "        if match != []:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def clean_kanji(self, sentence):\n",
    "        sentence = \"\".join(sentence.split())\n",
    "        pattern = f\"[^{self.kanji_unicode}]\"\n",
    "        sentence = re.sub(pattern, \"\", sentence)\n",
    "        return sentence\n",
    "\n",
    "    def kanji2hira(self, sentence):\n",
    "        try:\n",
    "            sentence = self.katsu.romaji(sentence)\n",
    "            sentence = sentence.replace(\" \", \"\")\n",
    "            sentence = sentence.replace(\"。\", \"\").lower()\n",
    "            sentence = Romaji2Kana(sentence)\n",
    "        except:\n",
    "            sentence = None\n",
    "        return sentence\n",
    "\n",
    "    def get_vocab(self, data):\n",
    "        vocab = []\n",
    "        texts = data['raw_text'].tolist() + data['hira_text'].tolist()\n",
    "        for text in tqdm(texts):\n",
    "            for char in text:\n",
    "                vocab.append(char)\n",
    "\n",
    "        tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\"]\n",
    "        \n",
    "        for i in Counter(vocab).most_common():\n",
    "            if i[0] in self.kanji_unicode:\n",
    "                tokens.append(i[0])\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for token in tokens[:args.vocab_size]:\n",
    "                f.write(token + \"\\n\")    \n",
    "\n",
    "    def pad_longest(self, row):\n",
    "        input_len = len(self.data['input_ids'][row])\n",
    "        label_len = len(self.data['label_ids'][row])\n",
    "        input_ids = self.data['input_ids'][row]\n",
    "        label_ids = self.data['label_ids'][row]\n",
    "        if label_len > input_len:\n",
    "            pad_width = label_len - input_len\n",
    "            input_ids = np.pad(\n",
    "                self.data['input_ids'][row], pad_width=(0, pad_width)).tolist()\n",
    "        elif label_len < input_len:\n",
    "            pad_width = input_len - label_len\n",
    "            label_ids = np.pad(\n",
    "                self.data['label_ids'][row], pad_width=(0, pad_width)).tolist()\n",
    "        return input_ids, label_ids\n",
    "\n",
    "# data = BertDataset().data\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_char(text):\n",
    "#     for i, char in enumerate(vocab):\n",
    "#         if char in text:\n",
    "#             return int(i)\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "# with open(r\"E:\\Datasets\\Decoder_model\\bert_vocab.txt\", encoding=\"utf-8\") as f:\n",
    "#     vocab = [v.strip(\"\\n\") for v in f.readlines()[4:][::-1]]\n",
    "\n",
    "# tqdm.pandas()\n",
    "# data = pd.read_csv(r\"E:\\Datasets\\Decoder_model\\bert_data.csv\", encoding=\"utf-8\")\n",
    "# data = data.dropna().reset_index(drop=True)\n",
    "# q1 = data['input_len'].quantile(0.1)\n",
    "# q2 = data['input_len'].quantile(0.9)\n",
    "# data = data[data['input_len'].between(q1, q2)]\n",
    "# data['char'] = data['raw_text'].progress_apply(get_char)\n",
    "# data = data.query(\"raw_text != hira_text\")\n",
    "# data = data.sort_values(by=\"char\").reset_index(drop=True)[:-70000]\n",
    "# data.to_csv(r\"E:\\Datasets\\Decoder_model\\bert_datav2.csv\", index=False, encoding=\"utf-8\")\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(r\"E:\\Datasets\\Decoder_model\\bert_datav2.csv\", encoding=\"utf-8\")\n",
    "# data = data.dropna().reset_index(drop=True)\n",
    "\n",
    "# uniq = []\n",
    "# for text in tqdm(data['raw_text'][:500000]):\n",
    "#     for char in text:\n",
    "#         if char in vocab:\n",
    "#             uniq.append(char)\n",
    "\n",
    "# Counter(uniq).most_common()[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFRWriter():\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.vocab = self.get_vocab()\n",
    "        self.data = self.get_data()\n",
    "\n",
    "    def get_data(self):\n",
    "        tqdm.pandas()\n",
    "        data = pd.read_csv(f\"{self.args.main_dir}/bert_datav2.csv\")\n",
    "        data = data.dropna().reset_index(drop=True)\n",
    "        data = data[:self.args.n_samples]\n",
    "        data['input_ids'] = data['input_ids'].progress_apply(ast.literal_eval)\n",
    "        data['label_ids'] = data['label_ids'].progress_apply(ast.literal_eval)\n",
    "        data = data.sort_values(by=\"input_len\", ascending=True, ignore_index=True)\n",
    "        data.to_csv(f\"{self.args.main_dir}/bert_datav3.csv\", index=False, encoding=\"utf-8\")\n",
    "        return data[['input_ids', 'label_ids', 'char']]\n",
    "\n",
    "    def get_vocab(self):\n",
    "        with open(r\"E:\\Datasets\\Decoder_model\\bert_vocab.txt\", encoding=\"utf-8\") as f:\n",
    "            vocab = {k.strip(\"\\n\"): 0 for k in f.readlines()[4:]}\n",
    "\n",
    "    def _bytes_feature(self, value):\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    def serialize_example(self, *args):\n",
    "        feature = {\n",
    "            'input_ids': self._bytes_feature(args[0]),\n",
    "            'attention_mask': self._bytes_feature(args[1]),\n",
    "            'label_ids': self._bytes_feature(args[2])}\n",
    "\n",
    "        example_proto = tf.train.Example(\n",
    "            features=tf.train.Features(feature=feature))\n",
    "        return example_proto.SerializeToString()\n",
    "\n",
    "    def get_shards(self):\n",
    "        skf = StratifiedKFold(n_splits=self.args.n_shards)\n",
    "        return [j for i,j in skf.split(self.data, self.data['char'])]\n",
    "\n",
    "    def get_shard_data(self, samples):\n",
    "        for sample in samples:\n",
    "            input_ids = tf.convert_to_tensor(\n",
    "                self.data['input_ids'][sample], dtype=tf.int32)\n",
    "            attention_mask = tf.where(input_ids != 0, x=1, y=0)\n",
    "            label_ids = tf.convert_to_tensor(\n",
    "                self.data['label_ids'][sample], dtype=tf.int32)\n",
    "            yield {\n",
    "                \"input_ids\": tf.io.serialize_tensor(input_ids),\n",
    "                \"attention_mask\": tf.io.serialize_tensor(attention_mask),\n",
    "                \"label_ids\": tf.io.serialize_tensor(label_ids)}\n",
    "\n",
    "    def write(self):\n",
    "        for shard, samples in tqdm(enumerate(self.get_shards()), total=self.args.n_shards):\n",
    "            with tf.io.TFRecordWriter(f\"{self.args.main_dir}/bert_tfrec/shard_{shard+1}.tfrec\") as f:\n",
    "                for sample in self.get_shard_data(samples):\n",
    "                    example = self.serialize_example(\n",
    "                        sample['input_ids'],\n",
    "                        sample['attention_mask'],\n",
    "                        sample['label_ids'])\n",
    "                    f.write(example)\n",
    "\n",
    "# TFRWriter(args).write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, args):\n",
    "        self.files = glob.glob(args.main_dir + \"/bert_tfrec/*.tfrec\")\n",
    "        self.args = args\n",
    "        self.AUTOTUNE = tf.data.AUTOTUNE\n",
    "        self.train_files, self.val_files = train_test_split(\n",
    "            self.files, test_size=args.test_size, shuffle=True,\n",
    "            random_state=args.random_state)\n",
    "        self.train = self.get_train()\n",
    "        self.val = self.get_val()\n",
    "\n",
    "    def read_tfrecord(self, example):\n",
    "        feature_description = {\n",
    "            'input_ids': tf.io.FixedLenFeature([], tf.string),\n",
    "            'attention_mask': tf.io.FixedLenFeature([], tf.string),\n",
    "            'label_ids': tf.io.FixedLenFeature([], tf.string)}\n",
    "        \n",
    "        example = tf.io.parse_single_example(example, feature_description)\n",
    "        example['input_ids'] = tf.io.parse_tensor(\n",
    "            example['input_ids'], out_type=tf.int32)\n",
    "        example['attention_mask'] = tf.io.parse_tensor(\n",
    "            example['attention_mask'], out_type=tf.int32) \n",
    "        example['label_ids'] = tf.io.parse_tensor(\n",
    "            example['label_ids'], out_type=tf.int32)\n",
    "        return example\n",
    "\n",
    "    def load_dataset(self, files):\n",
    "        ignore_order = tf.data.Options()\n",
    "        ignore_order.experimental_deterministic = False\n",
    "        dataset = tf.data.TFRecordDataset(files)\n",
    "        dataset = dataset.with_options(ignore_order)\n",
    "        dataset = dataset.map(self.read_tfrecord, num_parallel_calls=self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_train(self):\n",
    "        dataset = self.load_dataset(self.train_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_ids': [None],\n",
    "                'attention_mask': [None],\n",
    "                'label_ids': [None]},\n",
    "            padding_values={\n",
    "                'input_ids': tf.constant(0, dtype=tf.int32),\n",
    "                'attention_mask': tf.constant(0, dtype=tf.int32),\n",
    "                'label_ids': tf.constant(0, dtype=tf.int32)})        \n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_val(self):\n",
    "        dataset = self.load_dataset(self.val_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_ids': [None],\n",
    "                'attention_mask': [None],\n",
    "                'label_ids': [None]},\n",
    "            padding_values={\n",
    "                'input_ids': tf.constant(0, dtype=tf.int32),\n",
    "                'attention_mask': tf.constant(0, dtype=tf.int32),\n",
    "                'label_ids': tf.constant(0, dtype=tf.int32)})\n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "# train = DataLoader(args).train\n",
    "# inputs = next(iter(train))\n",
    "# print(\"input_ids shape:\", inputs['input_ids'].shape)\n",
    "# print(\"attention_mask shape:\", inputs['attention_mask'].shape)\n",
    "# print(\"label_ids shape:\", inputs['label_ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CERMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"CER\", **kwargs):\n",
    "        super(CERMetric, self).__init__(name=name,  **kwargs)\n",
    "        self.accumulator = self.add_weight(name=\"total_cer\", initializer=\"zeros\")\n",
    "        self.counter = self.add_weight(name=\"cer_count\", initializer=\"zeros\")    \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        hypothesis = tf.cast(tf.sparse.from_dense(y_pred), dtype=tf.int32)\n",
    "\n",
    "        # Convert dense to sparse tensor for edit_distance function\n",
    "        truth = tf.RaggedTensor.from_tensor(y_true, padding=0).to_sparse()\n",
    "\n",
    "        # Calculate Levenshtein distance\n",
    "        distance = tf.edit_distance(hypothesis, truth, normalize=True)\n",
    "\n",
    "        # Add distance and number of samples to variables\n",
    "        self.accumulator.assign_add(tf.reduce_sum(distance))\n",
    "        self.counter.assign_add(len(y_true))\n",
    "\n",
    "    def result(self):\n",
    "        # Divides accumulated distance scores against number of samples passed,\n",
    "        # mimics mean reduction over batch\n",
    "        return tf.math.divide_no_nan(self.accumulator, self.counter)   \n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.accumulator.assign(0.0)\n",
    "        self.counter.assign(0.0)\n",
    "\n",
    "class CosineDecayWithWarmup(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "    def __call__(self, epoch):  \n",
    "        if epoch < self.args.warmup_epochs:\n",
    "            lr = ((self.args.lr_max - self.args.lr_start) / self.args.warmup_epochs) * epoch + self.args.lr_start\n",
    "        elif epoch < (self.args.warmup_epochs + self.args.sustain_epochs):\n",
    "            lr = self.args.lr_max\n",
    "        else:\n",
    "            progress = ((epoch - self.args.warmup_epochs - self.args.sustain_epochs) / \n",
    "            (self.args.epochs - self.args.warmup_epochs - self.args.sustain_epochs))\n",
    "            lr = (self.args.lr_max-self.args.lr_min) * (0.5 * (1.0 + tf.math.cos((22/7) * \n",
    "                self.args.n_cycles * 2.0 * progress)))\n",
    "            if self.args.lr_min is not None:\n",
    "                lr = tf.math.maximum(self.args.lr_min, lr)\n",
    "        return lr\n",
    "\n",
    "    def plot(self):\n",
    "        epochs = range(self.args.epochs+1)\n",
    "        lr = [self(epoch) for epoch in epochs]\n",
    "        plt.plot(epochs, lr)\n",
    "        plt.xlabel(\"learning_rate\")\n",
    "        plt.ylabel(\"epochs\")\n",
    "        plt.show()\n",
    "\n",
    "CosineDecayWithWarmup(args).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kana2Kanji(args):\n",
    "    input_ids = Input(type_spec=tf.TensorSpec(\n",
    "        shape=(args.batch_size, None), dtype=tf.int32), name=\"input_ids\")\n",
    "    mask = Input(type_spec=tf.TensorSpec(\n",
    "        shape=(args.batch_size, None), dtype=tf.int32), name=\"attention_mask\")\n",
    "\n",
    "    bert = TFBertModel.from_pretrained(\n",
    "        \"cl-tohoku/bert-base-japanese-char-v2\",\n",
    "        output_hidden_states=False,\n",
    "        output_attentions=False,\n",
    "        num_attention_heads=16,\n",
    "        num_hidden_layers=16,\n",
    "        name=\"bert_model\")\n",
    "\n",
    "    x = bert(input_ids=input_ids, attention_mask=mask).last_hidden_state\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = TimeDistributed(\n",
    "        Dense(\n",
    "            args.vocab_size, \n",
    "            activation=\"softmax\"\n",
    "            ), \n",
    "        name=\"output\")(x, mask=mask)\n",
    "    return tf.keras.Model(inputs=[input_ids, mask], outputs=x, name=\"Kana2Kanji\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from epoch 4...\n",
      "Epoch 4/20: Learning rate @ 5.00e-05\n",
      "7032/7032 [==============================] - 2319s 330ms/step - loss: 0.6557 - cer: 0.2588 - val_loss: 1.2419 - val_cer: 0.2005\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Training\n",
      "Target:    パジャマも長袖に交換\n",
      "Predicted: パジャマも長袖に交換\n",
      "Target:    歩き方と腸の働き\n",
      "Predicted: 歩き方と長の働き\n",
      "Target:    振袖にサイズはありますか\n",
      "Predicted: 振袖にサイズはありますか\n",
      "Target:    北陸最大規模の古墳\n",
      "Predicted: 北陸砕大規模の古墳\n",
      "\n",
      "Validation\n",
      "Target:    僕はちぃちゃんが落ち着くまで舌を舐めてもらいました\n",
      "Predicted: 僕はちちちゃん落落ち着まで舌舌を舐めてもらいしたた\n",
      "Target:    次に上から誰かが私の顔を覗ぎこみます\n",
      "Predicted: 次に上から誰かが私の顔を狙いぎ込みます\n",
      "Target:    年種田仁はまたしても怪我で不振に陥ります\n",
      "Predicted: 年タネた人はまししても怪がで不心に陥ります\n",
      "Target:    引用元登山が趣味これって山舐めすぎでしょ\n",
      "Predicted: 引用元と山が味込れれって山めすででし\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 5/20: Learning rate @ 4.96e-05\n",
      "7032/7032 [==============================] - 2289s 325ms/step - loss: 0.8626 - cer: 0.2299 - val_loss: 1.1185 - val_cer: 0.1836\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Training\n",
      "Target:    通常胆嚢に入った形で出回っている\n",
      "Predicted: 通常担堪に入っ形形ででっってるる\n",
      "Target:    の縦長ディスプレイやダブルレンズカメラを搭載\n",
      "Predicted: の縦長いススレイヤダブルレンズカメラを載載\n",
      "Target:    法律を学び法廷弁護士として認められた\n",
      "Predicted: 法律を学び法廷弁護士として認認めれた\n",
      "Target:    では日記における宛先読み手とは誰でしょうか\n",
      "Predicted: では日記における宛先読てととは誰でしょうか\n",
      "\n",
      "Validation\n",
      "Target:    僕はちぃちゃんが落ち着くまで舌を舐めてもらいました\n",
      "Predicted: 僕はちちちゃんが落ち着ままで舌を舐めてもらいいした\n",
      "Target:    次に上から誰かが私の顔を覗ぎこみます\n",
      "Predicted: 次に上から誰かが私の顔を狙いぎ込みます\n",
      "Target:    年種田仁はまたしても怪我で不振に陥ります\n",
      "Predicted: 年種たた人はまししても怪我不不心に陥ります\n",
      "Target:    引用元登山が趣味これって山舐めすぎでしょ\n",
      "Predicted: 引用元と山が趣込これってやめすででし\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 6/20: Learning rate @ 4.83e-05\n",
      "3695/7032 [==============>...............] - ETA: 17:59 - loss: 1.2577 - cer: 0.2334"
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.train_dataset = DataLoader(args).train\n",
    "        self.val_dataset = DataLoader(args).val\n",
    "\n",
    "        self.tokenizer = BertJapaneseTokenizer(\n",
    "            vocab_file=f\"{self.args.main_dir}/bert_vocab.txt\",\n",
    "            do_lower_case=False,\n",
    "            do_word_tokenize=True,\n",
    "            do_subword_tokenize=True,\n",
    "            word_tokenizer_type=\"mecab\",\n",
    "            subword_tokenizer_type=\"character\")\n",
    "        \n",
    "        self.model = self.Kana2Kanji(args)\n",
    "        \n",
    "        schedule = CosineDecayWithWarmup(args)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(schedule)\n",
    "        self.loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=False)\n",
    "        self.cer_metric = CERMetric()\n",
    "\n",
    "        self.model_name = f\"model_{int(self.args.n_samples/1000)}k_v4\"\n",
    "        self.log_path = f\"{self.args.main_dir}/bert_model_weights/{self.model_name}.csv\"\n",
    "        if not os.path.exists(self.log_path):\n",
    "            print(\"Log file created.\")\n",
    "            columns = \"epoch,loss,cer,val_loss,val_cer\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(columns)\n",
    "\n",
    "    def Kana2Kanji(self, args):\n",
    "        input_ids = Input(type_spec=tf.TensorSpec(\n",
    "            shape=(args.batch_size, None), dtype=tf.int32), name=\"input_ids\")\n",
    "        mask = Input(type_spec=tf.TensorSpec(\n",
    "            shape=(args.batch_size, None), dtype=tf.int32), name=\"attention_mask\")\n",
    "\n",
    "        bert = TFBertModel.from_pretrained(\n",
    "            \"cl-tohoku/bert-base-japanese-char-v2\",\n",
    "            output_hidden_states=False,\n",
    "            output_attentions=False,\n",
    "            name=\"bert_model\")\n",
    "\n",
    "        x = bert(input_ids=input_ids, attention_mask=mask).last_hidden_state\n",
    "        x = Dropout(0.2)(x)\n",
    "        x = TimeDistributed(\n",
    "            Dense(\n",
    "                args.vocab_size, \n",
    "                activation=\"softmax\"), \n",
    "            name=\"output\")(x, mask=mask)\n",
    "        return tf.keras.Model(inputs=[input_ids, mask], outputs=x, name=\"Kana2Kanji\")\n",
    "\n",
    "    def display(self, epoch, t_labels, t_logits, v_labels, v_logits):\n",
    "        t_labels = self.tokenizer.batch_decode(t_labels, skip_special_tokens=True)\n",
    "        t_logits = self.tokenizer.batch_decode(t_logits, skip_special_tokens=True)\n",
    "        v_labels = self.tokenizer.batch_decode(v_labels, skip_special_tokens=True)\n",
    "        v_logits = self.tokenizer.batch_decode(v_logits, skip_special_tokens=True)\n",
    "\n",
    "        print(\"-\" * 129)\n",
    "        print(\"Training\")\n",
    "        for y_true, y_pred in zip(t_labels[:4], t_logits[:4]):\n",
    "            print(f\"Target:    {y_true.replace(' ', '')}\")\n",
    "            print(f\"Predicted: {y_pred.replace(' ', '')}\")\n",
    "\n",
    "        print(\"\\nValidation\")\n",
    "        for y_true, y_pred in zip(v_labels[:4], v_logits[:4]):\n",
    "            print(f\"Target:    {y_true.replace(' ', '')}\")\n",
    "            print(f\"Predicted: {y_pred.replace(' ', '')}\")\n",
    "        print(\"-\" * 129)\n",
    "        \n",
    "    def fit(self):\n",
    "        # Checkpointing\n",
    "        self.ckpt_dir = f\"{self.args.main_dir}/bert_checkpoints\"\n",
    "        self.ckpt = tf.train.Checkpoint(self.model)\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(\n",
    "            checkpoint=self.ckpt, directory=self.ckpt_dir, max_to_keep=5)\n",
    "\n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            self.start_epoch = int(self.ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "            self.ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "            print(f\"Resuming from epoch {self.start_epoch + 1}...\")\n",
    "        else:\n",
    "            self.start_epoch = 0\n",
    "            print(\"Starting from epoch 1...\")\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.args.epochs+1):\n",
    "            print(f\"Epoch {epoch+1}/{self.args.epochs}: Learning rate @ {self.optimizer.lr(epoch):.2e}\")\n",
    "            stateful_metrics = [\"loss\", \"cer\", \"val_loss\", \"val_cer\"]\n",
    "            progbar = tf.keras.utils.Progbar(\n",
    "                self.args.train_steps, interval=0.05,\n",
    "                stateful_metrics=stateful_metrics)\n",
    "\n",
    "            # Training loop\n",
    "            for step, t_batch in enumerate(self.train_dataset):\n",
    "                t_input_ids = t_batch['input_ids']\n",
    "                t_attention_mask = t_batch['attention_mask']\n",
    "                t_labels = t_batch['label_ids']\n",
    "                with tf.GradientTape() as tape:\n",
    "                    t_logits = self.model(\n",
    "                        [t_input_ids, t_attention_mask],\n",
    "                        training=True)                 \n",
    "                    t_loss = self.loss_fn(\n",
    "                        t_labels, t_logits, sample_weight=t_attention_mask)\n",
    "                gradients = tape.gradient(t_loss, self.model.trainable_weights)\n",
    "                self.optimizer.apply_gradients(zip(gradients, self.model.trainable_weights))\n",
    "                t_logits = tf.argmax(t_logits, axis=-1)\n",
    "                self.cer_metric.update_state(t_labels, t_logits)\n",
    "                t_cer = self.cer_metric.result()\n",
    "                t_values = [(\"loss\", t_loss), (\"cer\", t_cer)]\n",
    "                progbar.update(step, values=t_values, finalize=False)\n",
    "            \n",
    "            self.cer_metric.reset_state()\n",
    "\n",
    "            # Validation loop\n",
    "            for v_batch in self.val_dataset:\n",
    "                v_input_ids = v_batch['input_ids']\n",
    "                v_attention_mask = v_batch['attention_mask']\n",
    "                v_labels = v_batch['label_ids']\n",
    "                v_logits = self.model(\n",
    "                    [v_input_ids, v_attention_mask],\n",
    "                    training=False)\n",
    "                v_loss = self.loss_fn(\n",
    "                    v_labels, v_logits, sample_weight=v_attention_mask)\n",
    "                v_logits = tf.argmax(v_logits, axis=-1)    \n",
    "                self.cer_metric.update_state(v_labels, v_logits)\n",
    "                \n",
    "            v_cer = self.cer_metric.result()\n",
    "            v_values = [\n",
    "                (\"loss\", t_loss),\n",
    "                (\"cer\", t_cer),\n",
    "                (\"val_loss\", v_loss),\n",
    "                (\"val_cer\", v_cer)]\n",
    "            progbar.update(self.args.train_steps, values=v_values, finalize=True)\n",
    "            self.cer_metric.reset_state()\n",
    "\n",
    "            # Print sample transcriptions for both loops\n",
    "            self.display(epoch, t_labels, t_logits, v_labels, v_logits)\n",
    "\n",
    "            # Checkpointing\n",
    "            self.ckpt.save(file_prefix=f\"{self.ckpt_dir}/{self.model_name}\")\n",
    "\n",
    "            # Logging\n",
    "            log = f\"{epoch+1},{t_loss},{t_cer},{v_loss},{v_cer}\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(log)\n",
    "\n",
    "            save_path = f\"{self.args.main_dir}/bert_model_weights\"\n",
    "            self.model.save_weights(f\"{save_path}/{self.model_name}_{epoch+1}.h5\")\n",
    "\n",
    "Trainer(args).fit()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b851d2923cfa3a2562599062e05fd9893d86a7c009c64d8ad3756552e4dd5f41"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('tf-gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
