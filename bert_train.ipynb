{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import cutlet\n",
    "import argparse\n",
    "import MeCab\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "from convert_romaji import Romaji2Kana\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Dense, \n",
    "    TimeDistributed)\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "from transformers import (\n",
    "    BertJapaneseTokenizer,\n",
    "    TFBertModel,\n",
    "    GradientAccumulator,\n",
    "    logging)\n",
    "\n",
    "# policy = mixed_precision.Policy('mixed_float16')\n",
    "# mixed_precision.set_global_policy(policy)\n",
    "\n",
    "def seed_everything(SEED):\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    print(\"Random seed set.\")\n",
    "\n",
    "seed_everything(42)\n",
    "tf.get_logger().setLevel('FATAL')\n",
    "logging.set_verbosity_error()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=64, buffer_size=1024, epochs=5, learning_rate=1e-08, lr_max=1e-08, lr_min=1e-10, lr_start=1e-10, main_dir='E://Datasets/Decoder_model', n_cycles=0.5, n_samples=2500000, n_shards=20, n_train=1500000, n_val=1000000, random_state=42, sustain_epochs=0, test_size=0.4, train_steps=23438, val_steps=15625, vocab_size=6144, warmup_epochs=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ArgParser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Dataset\n",
    "    parser.add_argument(\"--main_dir\", default=\"E://Datasets/Decoder_model\")\n",
    "    parser.add_argument(\"--random_state\", default=42)\n",
    "    parser.add_argument(\"--n_shards\", default=20)\n",
    "    parser.add_argument(\"--n_samples\", default=2500000)\n",
    "    parser.add_argument(\"--test_size\", default=0.4)\n",
    "    parser.add_argument(\"--vocab_size\", default=6144)\n",
    "    parser.add_argument(\"--batch_size\", default=64)\n",
    "    parser.add_argument(\"--buffer_size\", default=1024)\n",
    "\n",
    "    # Scheduler\n",
    "    parser.add_argument(\"--epochs\", default=5)\n",
    "    parser.add_argument(\"--learning_rate\", default=1e-8)\n",
    "    parser.add_argument(\"--lr_start\", default=1e-10)\n",
    "    parser.add_argument(\"--lr_min\", default=1e-10)\n",
    "    parser.add_argument(\"--lr_max\", default=1e-8)\n",
    "    parser.add_argument(\"--n_cycles\", default=0.5)\n",
    "    parser.add_argument(\"--warmup_epochs\", default=0)\n",
    "    parser.add_argument(\"--sustain_epochs\", default=0)\n",
    "\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    n_train = int(args.n_samples * (1 - args.test_size))\n",
    "    n_val = int(args.n_samples * args.test_size)\n",
    "    train_steps = int(np.ceil(n_train / args.batch_size))\n",
    "    val_steps = int(np.ceil(n_val / args.batch_size))\n",
    "    \n",
    "    # Trainer\n",
    "    parser.add_argument(\"--n_train\", default=n_train)\n",
    "    parser.add_argument(\"--n_val\", default=n_val)\n",
    "    parser.add_argument(\"--train_steps\", default=train_steps)  \n",
    "    parser.add_argument(\"--val_steps\", default=val_steps)\n",
    "\n",
    "    return parser.parse_known_args()[0]\n",
    "\n",
    "args = ArgParser()\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset:\n",
    "    def __init__(self):\n",
    "        self.main_dir = \"D:\\School-stuff\\Sem-2\\PR-Project\\HoloASR\\Datasets\"\n",
    "        opus_ja_paths = glob.glob(f\"{self.main_dir}\\OPUS100-dataset\\*.ja\")\n",
    "        tatoeba_ja_paths = glob.glob(f\"{self.main_dir}\\Tatoeba-dataset\\*.ja\")\n",
    "        self.jesc_path = f\"{self.main_dir}/JESC-dataset/raw\"\n",
    "        self.ja_paths = opus_ja_paths + tatoeba_ja_paths\n",
    "        self.kanji_unicode = self.get_kanji_unicode()\n",
    "        self.katsu = cutlet.Cutlet()\n",
    "        self.katsu.use_foreign_spelling = False\n",
    "\n",
    "        tqdm.pandas()\n",
    "        self.data = pd.DataFrame({\"raw_text\": self.get_data()})\n",
    "\n",
    "        # Remove rows that contains non-kanji characters\n",
    "        self.data = self.data[self.data['raw_text'].progress_apply(self.check_kanji)]   \n",
    "\n",
    "        # Remove words within parenthesis\n",
    "        parenthesis =  r\"\\（.*\\）|\\(.*\\)|\\「.*\\」|\\『.*\\』\"\n",
    "        self.data = self.data[~self.data['raw_text'].str.contains(parenthesis)]\n",
    "\n",
    "        # Remove punctuations from sentences\n",
    "        self.data['raw_text'] = self.data['raw_text'].progress_apply(self.clean_kanji)\n",
    "\n",
    "        # Converts kanji to hiragana sentences\n",
    "        self.data['hira_text'] = self.data['raw_text'].progress_apply(self.kanji2hira)\n",
    "\n",
    "        # Remove null rows\n",
    "        self.data = self.data[~(self.data['raw_text']==\"\") | ~(self.data['hira_text']==\"\")]\n",
    "        self.data = self.data.dropna().reset_index(drop=True)\n",
    "\n",
    "        # Generate vocab file\n",
    "        self.vocab_file = r\"E:\\Datasets\\Language_model\\bert_vocab.txt\"\n",
    "        self.get_vocab(self.data)\n",
    "\n",
    "        # Construct tokenizer\n",
    "        self.tokenizer = BertJapaneseTokenizer(\n",
    "            vocab_file=self.vocab_file,\n",
    "            do_lower_case=False,\n",
    "            do_word_tokenize=False,\n",
    "            do_subword_tokenize=True,\n",
    "            word_tokenizer_type=\"mecab\",\n",
    "            subword_tokenizer_type=\"character\")\n",
    "\n",
    "        # Tokenize inputs and labels\n",
    "        self.data['input_ids'] = self.data['hira_text'].progress_apply(\n",
    "            lambda x: self.tokenizer(x).input_ids)\n",
    "        self.data['label_ids'] = self.data['raw_text'].progress_apply(\n",
    "            lambda x: self.tokenizer(x).input_ids)\n",
    "\n",
    "        # Apply padding to either input or labels to same length\n",
    "        new_input_ids, new_label_ids = [], []\n",
    "        for row_idx in tqdm(range(len(self.data)), total=len(self.data)):\n",
    "            input_ids, label_ids = self.pad_longest(row_idx)\n",
    "            new_input_ids.append(input_ids)\n",
    "            new_label_ids.append(label_ids)\n",
    "\n",
    "        self.data['input_ids'] = new_input_ids\n",
    "        self.data['label_ids'] = new_label_ids\n",
    "        self.data['input_len'] = self.data['input_ids'].apply(len)\n",
    "\n",
    "        # Remove rows that has unknown tokens\n",
    "        self.data = self.data[~self.data['input_ids'].apply(\n",
    "            lambda x: self.tokenizer.unk_token_id in x)]\n",
    "        self.data = self.data[~self.data['label_ids'].apply(\n",
    "            lambda x: self.tokenizer.unk_token_id in x)]\n",
    "        self.data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Save to csv\n",
    "        self.data.to_csv(\n",
    "            r\"E:\\Datasets\\Language_model\\bert_data.csv\", \n",
    "            encoding=\"utf-8\", index=False)\n",
    "\n",
    "    def get_kanji_unicode(self):\n",
    "        vocab = set()\n",
    "        with open(f\"{self.main_dir}\\kanji_unicode.txt\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                for char in line.split()[1:]:\n",
    "                    vocab.add(char)\n",
    "        return \"|\".join(sorted(vocab))\n",
    "\n",
    "    def get_data(self):\n",
    "        ja_lines = []\n",
    "        for ja_path in self.ja_paths:\n",
    "            with open(ja_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f.readlines():\n",
    "                    line = line.strip(\"\\n| \")\n",
    "                    ja_lines.append(line)\n",
    "        with open(self.jesc_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            texts = [text.split(\"\\t\") for text in f.readlines()]\n",
    "            for _, line in texts:\n",
    "                line = line.strip(\"\\n| \")\n",
    "                ja_lines.append(line)\n",
    "        return ja_lines\n",
    "\n",
    "    def check_kanji(self, sentence):\n",
    "        pattern = f\"[^{self.kanji_unicode}]\"\n",
    "        if re.match(pattern, sentence) == None:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def clean_kanji(self, sentence):\n",
    "        sentence = \"\".join(sentence.split())\n",
    "        pattern = r\"[・\\。\\！\\.\\？\\、]\"\n",
    "        sentence = re.sub(pattern, \"\", sentence)\n",
    "        return sentence\n",
    "\n",
    "    def kanji2hira(self, sentence):\n",
    "        try:\n",
    "            sentence = self.katsu.romaji(sentence)\n",
    "            sentence = sentence.replace(\" \", \"\").lower()\n",
    "            sentence = Romaji2Kana(sentence)\n",
    "        except:\n",
    "            sentence = None\n",
    "        return sentence\n",
    "\n",
    "    def get_vocab(self, data):\n",
    "        vocab = []\n",
    "        texts = data['raw_text'].tolist() + data['hira_text'].tolist()\n",
    "        for text in tqdm(texts):\n",
    "            for char in text:\n",
    "                vocab.append(char)\n",
    "\n",
    "        tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "        \n",
    "        for i in Counter(vocab).most_common():\n",
    "            if i[0] in self.kanji_unicode:\n",
    "                tokens.append(i[0])\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for token in tokens[:args.vocab_size]:\n",
    "                f.write(token + \"\\n\")    \n",
    "\n",
    "    def pad_longest(self, row):\n",
    "        input_len = len(self.data['input_ids'][row])\n",
    "        label_len = len(self.data['label_ids'][row])\n",
    "        input_ids = self.data['input_ids'][row]\n",
    "        label_ids = self.data['label_ids'][row]\n",
    "        if label_len > input_len:\n",
    "            pad_width = label_len - input_len\n",
    "            input_ids = np.pad(\n",
    "                self.data['input_ids'][row], pad_width=(0, pad_width)).tolist()\n",
    "        elif label_len < input_len:\n",
    "            pad_width = input_len - label_len\n",
    "            label_ids = np.pad(\n",
    "                self.data['label_ids'][row], pad_width=(0, pad_width)).tolist()\n",
    "        return input_ids, label_ids\n",
    "\n",
    "# data = BertDataset().data\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFRWriter():\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.data = self.get_data()\n",
    "\n",
    "    def get_data(self):\n",
    "        tqdm.pandas()\n",
    "        data = pd.read_csv(f\"{self.args.main_dir}/bert_data.csv\")\n",
    "        data = data.dropna().reset_index(drop=True)\n",
    "        data['input_ids'] = data['input_ids'].progress_apply(ast.literal_eval)\n",
    "        data['label_ids'] = data['label_ids'].progress_apply(ast.literal_eval)\n",
    "        data = data.query(f\"input_len <= {data['input_len'].quantile(0.90)}\")\n",
    "        data = data.sample(n=self.args.n_samples, random_state=self.args.random_state)\n",
    "        data = data.sort_values(by=\"input_len\", ascending=True, ignore_index=True)\n",
    "        return data[['input_ids', 'label_ids']]\n",
    "\n",
    "    def _bytes_feature(self, value):\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    def serialize_example(self, *args):\n",
    "        feature = {\n",
    "            'input_ids': self._bytes_feature(args[0]),\n",
    "            'attention_mask': self._bytes_feature(args[1]),\n",
    "            'label_ids': self._bytes_feature(args[2])}\n",
    "\n",
    "        example_proto = tf.train.Example(\n",
    "            features=tf.train.Features(feature=feature))\n",
    "        return example_proto.SerializeToString()\n",
    "\n",
    "    def get_shards(self):\n",
    "        skf = KFold(n_splits=self.args.n_shards, shuffle=False)\n",
    "        return [j for i,j in skf.split(self.data)]\n",
    "\n",
    "    def get_shard_data(self, samples):\n",
    "        for sample in samples:\n",
    "            input_ids = tf.convert_to_tensor(\n",
    "                self.data['input_ids'][sample], dtype=tf.int32)\n",
    "            attention_mask = tf.where(input_ids != 0, x=1, y=0)\n",
    "            label_ids = tf.convert_to_tensor(\n",
    "                self.data['label_ids'][sample], dtype=tf.int32)\n",
    "            yield {\n",
    "                \"input_ids\": tf.io.serialize_tensor(input_ids),\n",
    "                \"attention_mask\": tf.io.serialize_tensor(attention_mask),\n",
    "                \"label_ids\": tf.io.serialize_tensor(label_ids)}\n",
    "\n",
    "    def write(self):\n",
    "        for shard, samples in tqdm(enumerate(self.get_shards()), total=self.args.n_shards):\n",
    "            with tf.io.TFRecordWriter(f\"{self.args.main_dir}/bert_tfrec/shard_{shard+1}.tfrec\") as f:\n",
    "                for sample in self.get_shard_data(samples):\n",
    "                    example = self.serialize_example(\n",
    "                        sample['input_ids'],\n",
    "                        sample['attention_mask'],\n",
    "                        sample['label_ids'])\n",
    "                    f.write(example)\n",
    "\n",
    "# TFRWriter(args).write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, args):\n",
    "        self.files = glob.glob(args.main_dir + \"/bert_tfrec/*.tfrec\")\n",
    "        self.args = args\n",
    "        self.AUTOTUNE = tf.data.AUTOTUNE\n",
    "        self.train_files, self.val_files = train_test_split(\n",
    "            self.files, test_size=args.test_size, shuffle=True,\n",
    "            random_state=args.random_state)\n",
    "        self.train = self.get_train()\n",
    "        self.val = self.get_val()\n",
    "\n",
    "    def read_tfrecord(self, example):\n",
    "        feature_description = {\n",
    "            'input_ids': tf.io.FixedLenFeature([], tf.string),\n",
    "            'attention_mask': tf.io.FixedLenFeature([], tf.string),\n",
    "            'label_ids': tf.io.FixedLenFeature([], tf.string)}\n",
    "        \n",
    "        example = tf.io.parse_single_example(example, feature_description)\n",
    "        example['input_ids'] = tf.io.parse_tensor(\n",
    "            example['input_ids'], out_type=tf.int32)\n",
    "        example['attention_mask'] = tf.io.parse_tensor(\n",
    "            example['attention_mask'], out_type=tf.int32) \n",
    "        example['label_ids'] = tf.io.parse_tensor(\n",
    "            example['label_ids'], out_type=tf.int32)\n",
    "        return example\n",
    "\n",
    "    def load_dataset(self, files):\n",
    "        ignore_order = tf.data.Options()\n",
    "        ignore_order.experimental_deterministic = False\n",
    "        dataset = tf.data.TFRecordDataset(files)\n",
    "        dataset = dataset.with_options(ignore_order)\n",
    "        dataset = dataset.map(self.read_tfrecord, num_parallel_calls=self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_train(self):\n",
    "        dataset = self.load_dataset(self.train_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_ids': [None],\n",
    "                'attention_mask': [None],\n",
    "                'label_ids': [None]},\n",
    "            padding_values={\n",
    "                'input_ids': tf.constant(0, dtype=tf.int32),\n",
    "                'attention_mask': tf.constant(0, dtype=tf.int32),\n",
    "                'label_ids': tf.constant(0, dtype=tf.int32)})        \n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_val(self):\n",
    "        dataset = self.load_dataset(self.val_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_ids': [None],\n",
    "                'attention_mask': [None],\n",
    "                'label_ids': [None]},\n",
    "            padding_values={\n",
    "                'input_ids': tf.constant(0, dtype=tf.int32),\n",
    "                'attention_mask': tf.constant(0, dtype=tf.int32),\n",
    "                'label_ids': tf.constant(0, dtype=tf.int32)})\n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "# train = DataLoader(args).train\n",
    "# inputs = next(iter(train))\n",
    "# print(\"input_ids shape:\", inputs['input_ids'].shape)\n",
    "# print(\"attention_mask shape:\", inputs['attention_mask'].shape)\n",
    "# print(\"label_ids shape:\", inputs['label_ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CERMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"CER\", **kwargs):\n",
    "        super(CERMetric, self).__init__(name=name,  **kwargs)\n",
    "        self.accumulator = self.add_weight(name=\"total_cer\", initializer=\"zeros\")\n",
    "        self.counter = self.add_weight(name=\"cer_count\", initializer=\"zeros\")    \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        hypothesis = tf.cast(tf.sparse.from_dense(y_pred), dtype=tf.int32)\n",
    "\n",
    "        # Convert dense to sparse tensor for edit_distance function\n",
    "        truth = tf.RaggedTensor.from_tensor(y_true, padding=0).to_sparse()\n",
    "\n",
    "        # Calculate Levenshtein distance\n",
    "        distance = tf.edit_distance(hypothesis, truth, normalize=True)\n",
    "\n",
    "        # Add distance and number of samples to variables\n",
    "        self.accumulator.assign_add(tf.reduce_sum(distance))\n",
    "        self.counter.assign_add(len(y_true))\n",
    "\n",
    "    def result(self):\n",
    "        # Divides accumulated distance scores against number of samples passed,\n",
    "        # mimics mean reduction over batch\n",
    "        return tf.math.divide_no_nan(self.accumulator, self.counter)   \n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.accumulator.assign(0.0)\n",
    "        self.counter.assign(0.0)\n",
    "\n",
    "class CosineDecayWithWarmup(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "    def __call__(self, epoch):  \n",
    "        if epoch < self.args.warmup_epochs:\n",
    "            lr = ((self.args.lr_max - self.args.lr_start) / self.args.warmup_epochs) * epoch + self.args.lr_start\n",
    "        elif epoch < (self.args.warmup_epochs + self.args.sustain_epochs):\n",
    "            lr = self.args.lr_max\n",
    "        else:\n",
    "            progress = ((epoch - self.args.warmup_epochs - self.args.sustain_epochs) / \n",
    "            (self.args.epochs - self.args.warmup_epochs - self.args.sustain_epochs))\n",
    "            lr = (self.args.lr_max-self.args.lr_min) * (0.5 * (1.0 + tf.math.cos((22/7) * \n",
    "                self.args.n_cycles * 2.0 * progress)))\n",
    "            if self.args.lr_min is not None:\n",
    "                lr = tf.math.maximum(self.args.lr_min, lr)\n",
    "        return lr\n",
    "\n",
    "    def plot(self):\n",
    "        epochs = range(self.args.epochs+1)\n",
    "        lr = [self(epoch) for epoch in epochs]\n",
    "        plt.plot(epochs, lr)\n",
    "        plt.xlabel(\"learning_rate\")\n",
    "        plt.ylabel(\"epochs\")\n",
    "        plt.show()\n",
    "\n",
    "CosineDecayWithWarmup(args).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from epoch 2...\n",
      "Epoch 2/5: Learning rate @ 8.95e-09\n",
      "23438/23438 [==============================] - 7436s 317ms/step - loss: 0.4693 - cer: 0.1515 - val_loss: 0.5806 - val_cer: 0.1329\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Training\n",
      "Target:    これまで見たものから仮定してみよう\n",
      "Predicted: これまで見たものから家定してみよう\n",
      "Target:    何もお返しするものがないからって\n",
      "Predicted: 何もお返しするものがないからって\n",
      "Target:    彼が外出してるほうがいいのよ\n",
      "Predicted: 彼が外出してる方がいいのよ\n",
      "Target:    早樹は講義よりベルリッツだって\n",
      "Predicted: 早野は講義よりベルリッツだって\n",
      "\n",
      "Validation\n",
      "Target:    彼の骨は壊れやすかったから\n",
      "Predicted: 彼の骨は壊れやすかかか\n",
      "Target:    注意したが体を壊した\n",
      "Predicted: 注意したが体を壊した\n",
      "Target:    工事現場そのものだけでなく\n",
      "Predicted: 浩時現場そのものだけでなく\n",
      "Target:    強力な酸で溶かすんだ\n",
      "Predicted: 協力なさんでかすんだ\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 3/5: Learning rate @ 6.48e-09\n",
      "23437/23438 [============================>.] - ETA: 0s - loss: 0.5377 - cer: 0.1376"
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.train_dataset = DataLoader(args).train\n",
    "        self.val_dataset = DataLoader(args).val\n",
    "\n",
    "        self.tokenizer = BertJapaneseTokenizer(\n",
    "            vocab_file=f\"{self.args.main_dir}/bert_vocab.txt\",\n",
    "            do_lower_case=False,\n",
    "            do_word_tokenize=False,\n",
    "            do_subword_tokenize=True,\n",
    "            word_tokenizer_type=\"mecab\",\n",
    "            subword_tokenizer_type=\"character\")\n",
    "        \n",
    "        self.model = self.Kana2Kanji(args)\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam(CosineDecayWithWarmup(args))\n",
    "        self.loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=False)\n",
    "        self.cer_metric = CERMetric()\n",
    "\n",
    "        self.model_name = f\"model_{int(self.args.n_samples/1000)}k\"\n",
    "        self.log_path = f\"{self.args.main_dir}/bert_model_weights/{self.model_name}.csv\"\n",
    "        if not os.path.exists(self.log_path):\n",
    "            print(\"Log file created.\")\n",
    "            columns = \"epoch,loss,cer,val_loss,val_cer\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(columns)\n",
    "\n",
    "    def Kana2Kanji(self, args):\n",
    "        input_ids = Input(type_spec=tf.TensorSpec(\n",
    "            shape=(args.batch_size, None), dtype=tf.int32), name=\"input_ids\")\n",
    "        mask = Input(type_spec=tf.TensorSpec(\n",
    "            shape=(args.batch_size, None), dtype=tf.int32), name=\"attention_mask\")\n",
    "\n",
    "        bert = TFBertModel.from_pretrained(\n",
    "            \"cl-tohoku/bert-base-japanese-char\",\n",
    "            output_hidden_states=False,\n",
    "            output_attentions=False,\n",
    "            # num_hidden_layers=20,\n",
    "            name=\"bert_model\")\n",
    "\n",
    "        x = bert(input_ids=input_ids, attention_mask=mask).last_hidden_state\n",
    "        x = TimeDistributed(Dense(args.vocab_size, activation=\"softmax\"), name=\"output\")(x, mask=mask)\n",
    "\n",
    "        return tf.keras.Model(inputs=[input_ids, mask], outputs=x, name=\"Kana2Kanji\")\n",
    "\n",
    "    def decoder(self, labels, logits):\n",
    "        labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        logits = tf.argmax(logits, axis=-1)\n",
    "        logits = self.tokenizer.batch_decode(logits, skip_special_tokens=True)\n",
    "        return labels, logits\n",
    "\n",
    "    def display(self, epoch, t_labels, t_logits, v_labels, v_logits):\n",
    "        print(\"-\" * 129)\n",
    "        print(\"Training\")\n",
    "        for y_true, y_pred in zip(t_labels[:4], t_logits[:4]):\n",
    "            print(f\"Target:    {y_true.replace(' ', '')}\")\n",
    "            print(f\"Predicted: {y_pred.replace(' ', '')}\") \n",
    "\n",
    "        print(\"\\nValidation\")\n",
    "        for y_true, y_pred in zip(v_labels[:4], v_logits[:4]):\n",
    "            print(f\"Target:    {y_true.replace(' ', '')}\")\n",
    "            print(f\"Predicted: {y_pred.replace(' ', '')}\")\n",
    "        print(\"-\" * 129)\n",
    "        \n",
    "    def fit(self):\n",
    "        # Checkpointing\n",
    "        self.ckpt_dir = f\"{self.args.main_dir}/bert_checkpoints\"\n",
    "        self.ckpt = tf.train.Checkpoint(self.model)\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(\n",
    "            checkpoint=self.ckpt, directory=self.ckpt_dir, max_to_keep=5)\n",
    "\n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            self.start_epoch = int(self.ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "            self.ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "            print(f\"Resuming from epoch {self.start_epoch + 1}...\")\n",
    "        else:\n",
    "            self.start_epoch = 0\n",
    "            print(\"Starting from epoch 1...\")\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.args.epochs+1):\n",
    "            print(f\"Epoch {epoch+1}/{self.args.epochs}: Learning rate @ {self.optimizer.lr(epoch):.2e}\")\n",
    "            stateful_metrics = [\"loss\", \"cer\", \"val_loss\", \"val_cer\"]\n",
    "            progbar = tf.keras.utils.Progbar(\n",
    "                self.args.train_steps, interval=0.05,\n",
    "                stateful_metrics=stateful_metrics)\n",
    "\n",
    "            # Training loop\n",
    "            for step, t_batch in enumerate(self.train_dataset):\n",
    "                t_input_ids = t_batch['input_ids']\n",
    "                t_attention_mask = t_batch['attention_mask']\n",
    "                t_labels = t_batch['label_ids']\n",
    "                with tf.GradientTape() as tape:\n",
    "                    t_logits = self.model(\n",
    "                        [t_input_ids, t_attention_mask],\n",
    "                        training=True)\n",
    "                    t_loss = self.loss_fn(\n",
    "                        t_labels, t_logits, sample_weight=t_attention_mask)\n",
    "                self.cer_metric.update_state(t_labels, t_logits)\n",
    "                gradients = tape.gradient(t_loss, self.model.trainable_weights)\n",
    "                self.optimizer.apply_gradients(zip(gradients, self.model.trainable_weights))\n",
    "                t_cer = self.cer_metric.result()\n",
    "                t_values = [(\"loss\", t_loss), (\"cer\", t_cer)]\n",
    "                progbar.update(step, values=t_values, finalize=False)\n",
    "            \n",
    "            t_labels, t_logits = self.decoder(t_labels, t_logits)\n",
    "            self.cer_metric.reset_state()\n",
    "\n",
    "            # Validation loop\n",
    "            for v_batch in self.val_dataset:\n",
    "                v_input_ids = v_batch['input_ids']\n",
    "                v_attention_mask = v_batch['attention_mask']\n",
    "                v_labels = v_batch['label_ids']\n",
    "                v_logits = self.model(\n",
    "                    [v_input_ids, v_attention_mask],\n",
    "                    training=False)\n",
    "                v_loss = self.loss_fn(\n",
    "                    v_labels, v_logits, sample_weight=v_attention_mask)\n",
    "                self.cer_metric.update_state(v_labels, v_logits)\n",
    "            v_labels, v_logits = self.decoder(v_labels, v_logits)\n",
    "            v_cer = self.cer_metric.result()\n",
    "            v_values = [\n",
    "                (\"loss\", t_loss),\n",
    "                (\"cer\", t_cer),\n",
    "                (\"val_loss\", v_loss),\n",
    "                (\"val_cer\", v_cer)]\n",
    "            progbar.update(self.args.train_steps, values=v_values, finalize=True)\n",
    "            self.cer_metric.reset_state()\n",
    "\n",
    "            # Print sample transcriptions for both loops\n",
    "            self.display(epoch, t_labels, t_logits, v_labels, v_logits)\n",
    "\n",
    "            # Checkpointing\n",
    "            self.ckpt.save(file_prefix=f\"{self.ckpt_dir}/{self.model_name}\")\n",
    "\n",
    "            # Logging\n",
    "            log = f\"{epoch+1},{t_loss},{t_cer},{v_loss},{v_cer}\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(log)\n",
    "\n",
    "            save_path = f\"{self.args.main_dir}/bert_model_weights\"\n",
    "            self.model.save_weights(f\"{save_path}/{self.model_name}_{epoch+1}.h5\")\n",
    "\n",
    "Trainer(args).fit()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b851d2923cfa3a2562599062e05fd9893d86a7c009c64d8ad3756552e4dd5f41"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('tf-gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
