{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import glob\n",
    "import random\n",
    "import cutlet\n",
    "import argparse\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from convert_romaji import Romaji2Kana\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Dense, \n",
    "    TimeDistributed)\n",
    "\n",
    "from transformers import (\n",
    "    BertJapaneseTokenizer,\n",
    "    TFBertModel,\n",
    "    logging)\n",
    "\n",
    "def seed_everything(SEED):\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    print(\"Random seed set.\")\n",
    "\n",
    "seed_everything(42)\n",
    "tf.get_logger().setLevel('FATAL')\n",
    "logging.set_verbosity_error()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=64, buffer_size=1024, epochs=30, learning_rate=5e-05, lr_max=5e-05, lr_min=1e-09, lr_start=1e-09, main_dir='E://Datasets/Decoder_model', n_cycles=0.5, n_samples=500000, n_shards=5, n_train=400000, n_val=100000, random_state=42, sustain_epochs=0, test_size=0.2, train_steps=6250, val_steps=1563, vocab_size=6144, warmup_epochs=3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ArgParser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Dataset\n",
    "    parser.add_argument(\"--main_dir\", default=\"E://Datasets/Decoder_model\")\n",
    "    parser.add_argument(\"--random_state\", default=42)\n",
    "    parser.add_argument(\"--n_shards\", default=5)\n",
    "    parser.add_argument(\"--n_samples\", default=500000)\n",
    "    parser.add_argument(\"--test_size\", default=0.2)\n",
    "    parser.add_argument(\"--vocab_size\", default=6144)\n",
    "    parser.add_argument(\"--batch_size\", default=64)\n",
    "    parser.add_argument(\"--buffer_size\", default=1024)\n",
    "\n",
    "    # Scheduler\n",
    "    parser.add_argument(\"--epochs\", default=30)\n",
    "    parser.add_argument(\"--learning_rate\", default=5e-5)\n",
    "    parser.add_argument(\"--lr_start\", default=1e-9)\n",
    "    parser.add_argument(\"--lr_min\", default=1e-9)\n",
    "    parser.add_argument(\"--lr_max\", default=5e-5)\n",
    "    parser.add_argument(\"--n_cycles\", default=0.5)\n",
    "    parser.add_argument(\"--warmup_epochs\", default=3)\n",
    "    parser.add_argument(\"--sustain_epochs\", default=0)\n",
    "\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    n_train = int(args.n_samples * (1 - args.test_size))\n",
    "    n_val = int(args.n_samples * args.test_size)\n",
    "    train_steps = int(np.ceil(n_train / args.batch_size))\n",
    "    val_steps = int(np.ceil(n_val / args.batch_size))\n",
    "    \n",
    "    # Trainer\n",
    "    parser.add_argument(\"--n_train\", default=n_train)\n",
    "    parser.add_argument(\"--n_val\", default=n_val)\n",
    "    parser.add_argument(\"--train_steps\", default=train_steps)  \n",
    "    parser.add_argument(\"--val_steps\", default=val_steps)\n",
    "\n",
    "    return parser.parse_known_args()[0]\n",
    "\n",
    "args = ArgParser()\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset:\n",
    "    def __init__(self):\n",
    "        self.main_dir = \"D:\\School-stuff\\Sem-2\\PR-Project\\HoloASR\\Datasets\"\n",
    "        opus_ja_paths = glob.glob(f\"{self.main_dir}\\OPUS100-dataset\\*.ja\")\n",
    "        tatoeba_ja_paths = glob.glob(f\"{self.main_dir}\\Tatoeba-dataset\\*.ja\")\n",
    "        self.ja_paths = opus_ja_paths + tatoeba_ja_paths\n",
    "        self.jesc_path = f\"{self.main_dir}/JESC-dataset/raw\"\n",
    "        self.cc100_path = f\"{self.main_dir}/ja(1).txt\"\n",
    "        self.kanji_unicode = self.get_kanji_unicode()\n",
    "        self.katsu = cutlet.Cutlet()\n",
    "        self.katsu.use_foreign_spelling = False\n",
    "\n",
    "        tqdm.pandas()\n",
    "        self.data = pd.DataFrame({\"raw_text\": self.get_data()})\n",
    "\n",
    "        # Remove rows that contains non-kanji characters\n",
    "        self.data = self.data[self.data['raw_text'].progress_apply(self.check_kanji)]   \n",
    "\n",
    "        # Remove words within parenthesis\n",
    "        parenthesis =  r\"\\（.*\\）|\\(.*\\)|\\「.*\\」|\\『.*\\』\"\n",
    "        self.data = self.data[~self.data['raw_text'].str.contains(parenthesis)]\n",
    "\n",
    "        # Remove punctuations from sentences\n",
    "        self.data['raw_text'] = self.data['raw_text'].progress_apply(self.clean_kanji)\n",
    "\n",
    "        # Converts kanji to hiragana sentences\n",
    "        self.data['hira_text'] = self.data['raw_text'].progress_apply(self.kanji2hira)\n",
    "\n",
    "        # Remove null rows\n",
    "        self.data = self.data[~(self.data['raw_text']==\"\") | ~(self.data['hira_text']==\"\")]\n",
    "        self.data = self.data[\n",
    "            (~self.data['raw_text'].duplicated()) & \n",
    "            (~self.data['hira_text'].duplicated())]\n",
    "        self.data = self.data.dropna().reset_index(drop=True)\n",
    "\n",
    "        # Generate vocab file\n",
    "        self.vocab_file = r\"E:\\Datasets\\Decoder_model\\bert_vocab.txt\"\n",
    "        self.get_vocab(self.data)\n",
    "\n",
    "        # Construct tokenizer\n",
    "        self.tokenizer = BertJapaneseTokenizer(\n",
    "            vocab_file=self.vocab_file,\n",
    "            do_lower_case=False,\n",
    "            do_word_tokenize=True,\n",
    "            do_subword_tokenize=True,\n",
    "            word_tokenizer_type=\"mecab\",\n",
    "            subword_tokenizer_type=\"character\")\n",
    "\n",
    "        # Tokenize inputs and labels\n",
    "        self.data['input_ids'] = self.data['hira_text'].progress_apply(\n",
    "            lambda x: self.tokenizer(x).input_ids)\n",
    "        self.data['label_ids'] = self.data['raw_text'].progress_apply(\n",
    "            lambda x: self.tokenizer(x).input_ids)\n",
    "\n",
    "        # Apply padding to either input or labels to same length\n",
    "        new_input_ids, new_label_ids = [], []\n",
    "        for row_idx in tqdm(range(len(self.data)), total=len(self.data)):\n",
    "            input_ids, label_ids = self.pad_longest(row_idx)\n",
    "            new_input_ids.append(input_ids)\n",
    "            new_label_ids.append(label_ids)\n",
    "\n",
    "        self.data['input_ids'] = new_input_ids\n",
    "        self.data['label_ids'] = new_label_ids\n",
    "        self.data['input_len'] = self.data['input_ids'].apply(len)\n",
    "\n",
    "        # Save to csv\n",
    "        self.data.to_csv(\n",
    "            r\"E:\\Datasets\\Decoder_model\\bert_data.csv\", \n",
    "            encoding=\"utf-8\", index=False)\n",
    "\n",
    "    def get_kanji_unicode(self):\n",
    "        vocab = set()\n",
    "        with open(f\"{self.main_dir}\\kanji_unicode.txt\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                for char in line.split()[1:]:\n",
    "                    vocab.add(char)\n",
    "        return \"|\".join(sorted(vocab))\n",
    "\n",
    "    def get_data(self):\n",
    "        ja_lines = []\n",
    "        for ja_path in self.ja_paths:\n",
    "            with open(ja_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f.readlines():\n",
    "                    line = line.strip(\"\\n| \")\n",
    "                    ja_lines.append(line)\n",
    "        with open(self.jesc_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            texts = [text.split(\"\\t\") for text in f.readlines()]\n",
    "            for _, line in texts:\n",
    "                line = line.strip(\"\\n| \")\n",
    "                ja_lines.append(line)\n",
    "        with open(self.cc100_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            texts = random.sample(f.readlines(), 5000000)\n",
    "            for line in texts:\n",
    "                line = line.strip(\"\\n| \")\n",
    "                ja_lines.append(line)            \n",
    "        return ja_lines\n",
    "\n",
    "    def check_kanji(self, sentence):\n",
    "        pattern = f\"[^{self.kanji_unicode}]\"\n",
    "        match = re.match(pattern, sentence)\n",
    "        if match == None:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def clean_kanji(self, sentence):\n",
    "        sentence = \"\".join(sentence.split())\n",
    "        pattern = f\"[^{self.kanji_unicode}]\"\n",
    "        sentence = re.sub(pattern, \"\", sentence)\n",
    "        return sentence\n",
    "\n",
    "    def kanji2hira(self, sentence):\n",
    "        try:\n",
    "            sentence = self.katsu.romaji(sentence)\n",
    "            sentence = sentence.replace(\" \", \"\").lower()\n",
    "            sentence = Romaji2Kana(sentence)\n",
    "        except:\n",
    "            sentence = None\n",
    "        return sentence\n",
    "\n",
    "    def get_vocab(self, data):\n",
    "        vocab = []\n",
    "        texts = data['raw_text'].tolist() + data['hira_text'].tolist()\n",
    "        for text in tqdm(texts):\n",
    "            for char in text:\n",
    "                vocab.append(char)\n",
    "\n",
    "        tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\"]\n",
    "        \n",
    "        for i in Counter(vocab).most_common():\n",
    "            if i[0] in self.kanji_unicode:\n",
    "                tokens.append(i[0])\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for token in tokens[:args.vocab_size]:\n",
    "                f.write(token + \"\\n\")    \n",
    "\n",
    "    def pad_longest(self, row):\n",
    "        input_len = len(self.data['input_ids'][row])\n",
    "        label_len = len(self.data['label_ids'][row])\n",
    "        input_ids = self.data['input_ids'][row]\n",
    "        label_ids = self.data['label_ids'][row]\n",
    "        if label_len > input_len:\n",
    "            pad_width = label_len - input_len\n",
    "            input_ids = np.pad(\n",
    "                self.data['input_ids'][row], pad_width=(0, pad_width)).tolist()\n",
    "        elif label_len < input_len:\n",
    "            pad_width = input_len - label_len\n",
    "            label_ids = np.pad(\n",
    "                self.data['label_ids'][row], pad_width=(0, pad_width)).tolist()\n",
    "        return input_ids, label_ids\n",
    "\n",
    "# data = BertDataset().data\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_char(text):\n",
    "#     for i, char in enumerate(vocab):\n",
    "#         if char in text:\n",
    "#             return i\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "# with open(r\"E:\\Datasets\\Decoder_model\\bert_vocab.txt\", encoding=\"utf-8\") as f:\n",
    "#     vocab = [v.strip(\"\\n\") for v in f.readlines()[4:][::-1]]\n",
    "\n",
    "# tqdm.pandas()\n",
    "# data = pd.read_csv(r\"E:\\Datasets\\Decoder_model\\bert_data.csv\", encoding=\"utf-8\")\n",
    "# data = data.dropna().reset_index(drop=True)\n",
    "# q1 = data['input_len'].quantile(0.1)\n",
    "# q2 = data['input_len'].quantile(0.95)\n",
    "# data = data[data['input_len'].between(q1, q2)]\n",
    "# data['char'] = data['raw_text'].progress_apply(get_char)\n",
    "# data = data.query(\"raw_text != hira_text\")\n",
    "# data = data.sort_values(by=\"char\").reset_index(drop=True)[:-70000]\n",
    "# data.to_csv(r\"E:\\Datasets\\Decoder_model\\bert_datav2.csv\", index=False, encoding=\"utf-8\")\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFRWriter():\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.vocab = self.get_vocab()\n",
    "        self.data = self.get_data()\n",
    "\n",
    "    def get_data(self):\n",
    "        tqdm.pandas()\n",
    "        data = pd.read_csv(f\"{self.args.main_dir}/bert_datav2.csv\")\n",
    "        data = data.dropna().reset_index(drop=True)\n",
    "        data = data[:self.args.n_samples]\n",
    "        data['input_ids'] = data['input_ids'].progress_apply(ast.literal_eval)\n",
    "        data['label_ids'] = data['label_ids'].progress_apply(ast.literal_eval)\n",
    "        data = data.sort_values(by=\"input_len\", ascending=True, ignore_index=True)\n",
    "        data.to_csv(f\"{self.args.main_dir}/bert_datav3.csv\", index=False, encoding=\"utf-8\")\n",
    "        return data[['input_ids', 'label_ids', 'char']]\n",
    "\n",
    "    def get_vocab(self):\n",
    "        with open(r\"E:\\Datasets\\Decoder_model\\bert_vocab.txt\", encoding=\"utf-8\") as f:\n",
    "            vocab = {k.strip(\"\\n\"): 0 for k in f.readlines()[4:]}\n",
    "\n",
    "    def _bytes_feature(self, value):\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    def serialize_example(self, *args):\n",
    "        feature = {\n",
    "            'input_ids': self._bytes_feature(args[0]),\n",
    "            'attention_mask': self._bytes_feature(args[1]),\n",
    "            'label_ids': self._bytes_feature(args[2])}\n",
    "\n",
    "        example_proto = tf.train.Example(\n",
    "            features=tf.train.Features(feature=feature))\n",
    "        return example_proto.SerializeToString()\n",
    "\n",
    "    def get_shards(self):\n",
    "        skf = StratifiedKFold(n_splits=self.args.n_shards)\n",
    "        return [j for i,j in skf.split(self.data, self.data['char'])]\n",
    "\n",
    "    def get_shard_data(self, samples):\n",
    "        for sample in samples:\n",
    "            input_ids = tf.convert_to_tensor(\n",
    "                self.data['input_ids'][sample], dtype=tf.int32)\n",
    "            attention_mask = tf.where(input_ids != 0, x=1, y=0)\n",
    "            label_ids = tf.convert_to_tensor(\n",
    "                self.data['label_ids'][sample], dtype=tf.int32)\n",
    "            yield {\n",
    "                \"input_ids\": tf.io.serialize_tensor(input_ids),\n",
    "                \"attention_mask\": tf.io.serialize_tensor(attention_mask),\n",
    "                \"label_ids\": tf.io.serialize_tensor(label_ids)}\n",
    "\n",
    "    def write(self):\n",
    "        for shard, samples in tqdm(enumerate(self.get_shards()), total=self.args.n_shards):\n",
    "            with tf.io.TFRecordWriter(f\"{self.args.main_dir}/bert_tfrec/shard_{shard+1}.tfrec\") as f:\n",
    "                for sample in self.get_shard_data(samples):\n",
    "                    example = self.serialize_example(\n",
    "                        sample['input_ids'],\n",
    "                        sample['attention_mask'],\n",
    "                        sample['label_ids'])\n",
    "                    f.write(example)\n",
    "\n",
    "# TFRWriter(args).write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, args):\n",
    "        self.files = glob.glob(args.main_dir + \"/bert_tfrec/*.tfrec\")\n",
    "        self.args = args\n",
    "        self.AUTOTUNE = tf.data.AUTOTUNE\n",
    "        self.train_files, self.val_files = train_test_split(\n",
    "            self.files, test_size=args.test_size, shuffle=True,\n",
    "            random_state=args.random_state)\n",
    "        self.train = self.get_train()\n",
    "        self.val = self.get_val()\n",
    "\n",
    "    def read_tfrecord(self, example):\n",
    "        feature_description = {\n",
    "            'input_ids': tf.io.FixedLenFeature([], tf.string),\n",
    "            'attention_mask': tf.io.FixedLenFeature([], tf.string),\n",
    "            'label_ids': tf.io.FixedLenFeature([], tf.string)}\n",
    "        \n",
    "        example = tf.io.parse_single_example(example, feature_description)\n",
    "        example['input_ids'] = tf.io.parse_tensor(\n",
    "            example['input_ids'], out_type=tf.int32)\n",
    "        example['attention_mask'] = tf.io.parse_tensor(\n",
    "            example['attention_mask'], out_type=tf.int32) \n",
    "        example['label_ids'] = tf.io.parse_tensor(\n",
    "            example['label_ids'], out_type=tf.int32)\n",
    "        return example\n",
    "\n",
    "    def load_dataset(self, files):\n",
    "        ignore_order = tf.data.Options()\n",
    "        ignore_order.experimental_deterministic = False\n",
    "        dataset = tf.data.TFRecordDataset(files)\n",
    "        dataset = dataset.with_options(ignore_order)\n",
    "        dataset = dataset.map(self.read_tfrecord, num_parallel_calls=self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_train(self):\n",
    "        dataset = self.load_dataset(self.train_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_ids': [None],\n",
    "                'attention_mask': [None],\n",
    "                'label_ids': [None]},\n",
    "            padding_values={\n",
    "                'input_ids': tf.constant(0, dtype=tf.int32),\n",
    "                'attention_mask': tf.constant(0, dtype=tf.int32),\n",
    "                'label_ids': tf.constant(0, dtype=tf.int32)})        \n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_val(self):\n",
    "        dataset = self.load_dataset(self.val_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_ids': [None],\n",
    "                'attention_mask': [None],\n",
    "                'label_ids': [None]},\n",
    "            padding_values={\n",
    "                'input_ids': tf.constant(0, dtype=tf.int32),\n",
    "                'attention_mask': tf.constant(0, dtype=tf.int32),\n",
    "                'label_ids': tf.constant(0, dtype=tf.int32)})\n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "# train = DataLoader(args).train\n",
    "# inputs = next(iter(train))\n",
    "# print(\"input_ids shape:\", inputs['input_ids'].shape)\n",
    "# print(\"attention_mask shape:\", inputs['attention_mask'].shape)\n",
    "# print(\"label_ids shape:\", inputs['label_ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CERMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"CER\", **kwargs):\n",
    "        super(CERMetric, self).__init__(name=name,  **kwargs)\n",
    "        self.accumulator = self.add_weight(name=\"total_cer\", initializer=\"zeros\")\n",
    "        self.counter = self.add_weight(name=\"cer_count\", initializer=\"zeros\")    \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        hypothesis = tf.cast(tf.sparse.from_dense(y_pred), dtype=tf.int32)\n",
    "\n",
    "        # Convert dense to sparse tensor for edit_distance function\n",
    "        truth = tf.RaggedTensor.from_tensor(y_true, padding=0).to_sparse()\n",
    "\n",
    "        # Calculate Levenshtein distance\n",
    "        distance = tf.edit_distance(hypothesis, truth, normalize=True)\n",
    "\n",
    "        # Add distance and number of samples to variables\n",
    "        self.accumulator.assign_add(tf.reduce_sum(distance))\n",
    "        self.counter.assign_add(len(y_true))\n",
    "\n",
    "    def result(self):\n",
    "        # Divides accumulated distance scores against number of samples passed,\n",
    "        # mimics mean reduction over batch\n",
    "        return tf.math.divide_no_nan(self.accumulator, self.counter)   \n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.accumulator.assign(0.0)\n",
    "        self.counter.assign(0.0)\n",
    "\n",
    "class CosineDecayWithWarmup(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "    def __call__(self, epoch):  \n",
    "        if epoch < self.args.warmup_epochs:\n",
    "            lr = ((self.args.lr_max - self.args.lr_start) / self.args.warmup_epochs) * epoch + self.args.lr_start\n",
    "        elif epoch < (self.args.warmup_epochs + self.args.sustain_epochs):\n",
    "            lr = self.args.lr_max\n",
    "        else:\n",
    "            progress = ((epoch - self.args.warmup_epochs - self.args.sustain_epochs) / \n",
    "            (self.args.epochs - self.args.warmup_epochs - self.args.sustain_epochs))\n",
    "            lr = (self.args.lr_max-self.args.lr_min) * (0.5 * (1.0 + tf.math.cos((22/7) * \n",
    "                self.args.n_cycles * 2.0 * progress)))\n",
    "            if self.args.lr_min is not None:\n",
    "                lr = tf.math.maximum(self.args.lr_min, lr)\n",
    "        return lr\n",
    "\n",
    "    def plot(self):\n",
    "        epochs = range(self.args.epochs+1)\n",
    "        lr = [self(epoch) for epoch in epochs]\n",
    "        plt.plot(epochs, lr)\n",
    "        plt.xlabel(\"learning_rate\")\n",
    "        plt.ylabel(\"epochs\")\n",
    "        plt.show()\n",
    "\n",
    "CosineDecayWithWarmup(args).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from epoch 1...\n",
      "Epoch 1/30: Learning rate @ 1.00e-09\n",
      "4418/6250 [====================>.........] - ETA: 8:55 - loss: 1.7776 - cer: 0.6364"
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.train_dataset = DataLoader(args).train\n",
    "        self.val_dataset = DataLoader(args).val\n",
    "\n",
    "        self.tokenizer = BertJapaneseTokenizer(\n",
    "            vocab_file=f\"{self.args.main_dir}/bert_vocab.txt\",\n",
    "            do_lower_case=False,\n",
    "            do_word_tokenize=True,\n",
    "            do_subword_tokenize=True,\n",
    "            word_tokenizer_type=\"mecab\",\n",
    "            subword_tokenizer_type=\"character\")\n",
    "        \n",
    "        self.model = self.Kana2Kanji(args)\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam(CosineDecayWithWarmup(args))\n",
    "        self.loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=False)\n",
    "        self.cer_metric = CERMetric()\n",
    "\n",
    "        self.model_name = f\"model_{int(self.args.n_samples/1000)}k\"\n",
    "        self.log_path = f\"{self.args.main_dir}/bert_model_weights/{self.model_name}.csv\"\n",
    "        if not os.path.exists(self.log_path):\n",
    "            print(\"Log file created.\")\n",
    "            columns = \"epoch,loss,cer,val_loss,val_cer\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(columns)\n",
    "\n",
    "    def Kana2Kanji(self, args):\n",
    "        input_ids = Input(type_spec=tf.TensorSpec(\n",
    "            shape=(args.batch_size, None), dtype=tf.int32), name=\"input_ids\")\n",
    "        mask = Input(type_spec=tf.TensorSpec(\n",
    "            shape=(args.batch_size, None), dtype=tf.int32), name=\"attention_mask\")\n",
    "\n",
    "        bert = TFBertModel.from_pretrained(\n",
    "            \"cl-tohoku/bert-base-japanese-char-v2\",\n",
    "            output_hidden_states=False,\n",
    "            output_attentions=False,\n",
    "            name=\"bert_model\")\n",
    "\n",
    "        x = bert(input_ids=input_ids, attention_mask=mask).last_hidden_state\n",
    "        x = TimeDistributed(Dense(args.vocab_size, activation=\"softmax\"), name=\"output\")(x, mask=mask)\n",
    "\n",
    "        return tf.keras.Model(inputs=[input_ids, mask], outputs=x, name=\"Kana2Kanji\")\n",
    "\n",
    "    def display(self, epoch, t_labels, t_logits, v_labels, v_logits):\n",
    "        t_labels = self.tokenizer.batch_decode(t_labels, skip_special_tokens=True)\n",
    "        t_logits = self.tokenizer.batch_decode(t_logits, skip_special_tokens=True)\n",
    "        v_labels = self.tokenizer.batch_decode(v_labels, skip_special_tokens=True)\n",
    "        v_logits = self.tokenizer.batch_decode(v_logits, skip_special_tokens=True)\n",
    "\n",
    "        print(\"-\" * 129)\n",
    "        print(\"Training\")\n",
    "        for y_true, y_pred in zip(t_labels[:4], t_logits[:4]):\n",
    "            print(f\"Target:    {y_true.replace(' ', '')}\")\n",
    "            print(f\"Predicted: {y_pred.replace(' ', '')}\")\n",
    "\n",
    "        print(\"\\nValidation\")\n",
    "        for y_true, y_pred in zip(v_labels[:4], v_logits[:4]):\n",
    "            print(f\"Target:    {y_true.replace(' ', '')}\")\n",
    "            print(f\"Predicted: {y_pred.replace(' ', '')}\")\n",
    "        print(\"-\" * 129)\n",
    "        \n",
    "    def fit(self):\n",
    "        # Checkpointing\n",
    "        self.ckpt_dir = f\"{self.args.main_dir}/bert_checkpoints\"\n",
    "        self.ckpt = tf.train.Checkpoint(self.model)\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(\n",
    "            checkpoint=self.ckpt, directory=self.ckpt_dir, max_to_keep=5)\n",
    "\n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            self.start_epoch = int(self.ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "            self.ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "            print(f\"Resuming from epoch {self.start_epoch + 1}...\")\n",
    "        else:\n",
    "            self.start_epoch = 0\n",
    "            print(\"Starting from epoch 1...\")\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.args.epochs+1):\n",
    "            print(f\"Epoch {epoch+1}/{self.args.epochs}: Learning rate @ {self.optimizer.lr(epoch):.2e}\")\n",
    "            stateful_metrics = [\"loss\", \"cer\", \"val_loss\", \"val_cer\"]\n",
    "            progbar = tf.keras.utils.Progbar(\n",
    "                self.args.train_steps, interval=0.05,\n",
    "                stateful_metrics=stateful_metrics)\n",
    "\n",
    "            # Training loop\n",
    "            for step, t_batch in enumerate(self.train_dataset):\n",
    "                t_input_ids = t_batch['input_ids']\n",
    "                t_attention_mask = t_batch['attention_mask']\n",
    "                t_labels = t_batch['label_ids']\n",
    "                with tf.GradientTape() as tape:\n",
    "                    t_logits = self.model(\n",
    "                        [t_input_ids, t_attention_mask],\n",
    "                        training=True)\n",
    "                    t_loss = self.loss_fn(\n",
    "                        t_labels, t_logits, sample_weight=t_attention_mask)\n",
    "                \n",
    "                gradients = tape.gradient(t_loss, self.model.trainable_weights)\n",
    "                self.optimizer.apply_gradients(zip(gradients, self.model.trainable_weights))\n",
    "\n",
    "                t_logits = tf.argmax(t_logits, axis=-1)\n",
    "                self.cer_metric.update_state(t_labels, t_logits)\n",
    "                t_cer = self.cer_metric.result()\n",
    "                t_values = [(\"loss\", t_loss), (\"cer\", t_cer)]\n",
    "                progbar.update(step, values=t_values, finalize=False)\n",
    "            \n",
    "            self.cer_metric.reset_state()\n",
    "\n",
    "            # Validation loop\n",
    "            for v_batch in self.val_dataset:\n",
    "                v_input_ids = v_batch['input_ids']\n",
    "                v_attention_mask = v_batch['attention_mask']\n",
    "                v_labels = v_batch['label_ids']\n",
    "                v_logits = self.model(\n",
    "                    [v_input_ids, v_attention_mask],\n",
    "                    training=False)\n",
    "                v_loss = self.loss_fn(\n",
    "                    v_labels, v_logits, sample_weight=v_attention_mask)\n",
    "                v_logits = tf.argmax(v_logits, axis=-1)\n",
    "                self.cer_metric.update_state(v_labels, v_logits)\n",
    "                \n",
    "            v_cer = self.cer_metric.result()\n",
    "            v_values = [\n",
    "                (\"loss\", t_loss),\n",
    "                (\"cer\", t_cer),\n",
    "                (\"val_loss\", v_loss),\n",
    "                (\"val_cer\", v_cer)]\n",
    "            progbar.update(self.args.train_steps, values=v_values, finalize=True)\n",
    "            self.cer_metric.reset_state()\n",
    "\n",
    "            # Print sample transcriptions for both loops\n",
    "            self.display(epoch, t_labels, t_logits, v_labels, v_logits)\n",
    "\n",
    "            # Checkpointing\n",
    "            self.ckpt.save(file_prefix=f\"{self.ckpt_dir}/{self.model_name}\")\n",
    "\n",
    "            # Logging\n",
    "            log = f\"{epoch+1},{t_loss},{t_cer},{v_loss},{v_cer}\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(log)\n",
    "\n",
    "            save_path = f\"{self.args.main_dir}/bert_model_weights\"\n",
    "            self.model.save_weights(f\"{save_path}/{self.model_name}_{epoch+1}.h5\")\n",
    "\n",
    "Trainer(args).fit()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b851d2923cfa3a2562599062e05fd9893d86a7c009c64d8ad3756552e4dd5f41"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('tf-gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
