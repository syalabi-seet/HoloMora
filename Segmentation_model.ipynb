{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import glob\r\n",
    "import logging\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from tqdm.notebook import tqdm\r\n",
    "\r\n",
    "import librosa\r\n",
    "import torchaudio\r\n",
    "import torchaudio.functional as F\r\n",
    "import torchaudio.transforms as T\r\n",
    "\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "from torch.utils.data import Dataset\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "import pytorch_lightning as pl\r\n",
    "\r\n",
    "from argparse import ArgumentParser"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Arguments"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class parser(ArgumentParser):\r\n",
    "    def __init__(self):\r\n",
    "        super(parser, self).__init__()\r\n",
    "        self.add_argument('--rnn_input_size', \r\n",
    "            dest='rnn_input_size', type=int, default=50)\r\n",
    "        self.add_argument('--rnn_hidden_size', \r\n",
    "            dest='rnn_hidden_size', type=int, default=200)   \r\n",
    "        self.add_argument('--device', \r\n",
    "            dest='device', type=str, default='cuda')\r\n",
    "        self.add_argument('--rnn_layers',\r\n",
    "            dest='rnn_layers', type=int, default=2)\r\n",
    "        self.add_argument('--rnn_dropout',\r\n",
    "            dest='rnn_dropout', type=int, default=0.3)\r\n",
    "        self.add_argument('--n_classes', \r\n",
    "            dest='n_classes', type=int, default=5)\r\n",
    "        self.add_argument('--min_seg_size',\r\n",
    "            dest='min_seg_size', type=int, default=1)\r\n",
    "        self.add_argument('--max_seg_size',\r\n",
    "            dest='max_seg_size', type=int, default=100)\r\n",
    "        self.add_argument('--main_dir',\r\n",
    "            dest='main_dir', type=str, default=\"TIMIT-dataset\\data\")\r\n",
    "\r\n",
    "args = parser().parse_known_args()[0]\r\n",
    "args"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Namespace(device='cuda', max_seg_size=100, min_seg_size=1, n_classes=5, rnn_dropout=0.3, rnn_hidden_size=200, rnn_input_size=50, rnn_layers=2)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class TIMITDataset(Dataset):\r\n",
    "    def __init__(self, main_dir, mode=\"partial\"):\r\n",
    "        super(TIMITDataset, self).__init__()\r\n",
    "        paths = glob.glob(f\"{main_dir}\\*\\*\\*.wav\")\r\n",
    "        self.sample_paths = [os.path.splitext(path)[0] \r\n",
    "                for path in paths if not path.endswith(\".WAV.wav\")]\r\n",
    "        self.dictionary = self.phoneme_dict()\r\n",
    "        self.mode = mode\r\n",
    "\r\n",
    "    def dataframe(self):\r\n",
    "        def sentence_type(sample_name):\r\n",
    "            if sample_name.startswith(\"SA\"):\r\n",
    "                return \"dialect\"\r\n",
    "            elif sample_name.startswith(\"SX\"):\r\n",
    "                return \"compact\"\r\n",
    "            else:\r\n",
    "                return \"diverse\"   \r\n",
    "\r\n",
    "        sample_dict = {}\r\n",
    "        for i, sample in tqdm(\r\n",
    "                enumerate(self.sample_paths), total=len(self.sample_paths),\r\n",
    "                desc=\"Generating\"):\r\n",
    "\r\n",
    "            _, _, dialect, speaker_id, sample_name = sample.split(\"\\\\\")\r\n",
    "            S, mfcc, frames, phonemes = self.process_file(sample)\r\n",
    "            \r\n",
    "            if self.mode == \"full\":\r\n",
    "                sample_type = sentence_type(sample_name)\r\n",
    "                sample_rate = librosa.get_samplerate(sample + \".wav\")\r\n",
    "                with open(sample + \".txt\", 'r') as f:\r\n",
    "                    context = f.read().split()\r\n",
    "                    n_frames, text = context[1], \" \".join(context[2::])\r\n",
    "                sample_dict[i] = (sample_name, dialect, speaker_id,\r\n",
    "                    sample_type, sample_rate, n_frames, text, sample, S, \r\n",
    "                    mfcc, frames, phonemes)\r\n",
    "                columns = [\"sample_name\", \"dialect\", \"speaker_id\", \r\n",
    "                    \"sample_type\", \"sample_rate\", \"n_frames\", \"text\", \r\n",
    "                    \"sample_path\", \"spec_array\", \"mfcc_array\", \"frame_array\", \r\n",
    "                    \"phoneme_array\"]\r\n",
    "            elif self.mode == \"partial\":\r\n",
    "                sample_dict[i] = (sample_name, speaker_id, S, mfcc, frames, phonemes)\r\n",
    "                columns = [\"sample_name\", \"speaker_id\", \"spec_array\", \"mfcc_array\",\r\n",
    "                    \"frame_array\", \"phoneme_array\"]\r\n",
    "            else:\r\n",
    "                logging.error(\"Invalid mode, only full or partial allowed.\")\r\n",
    "                break\r\n",
    "\r\n",
    "        return pd.DataFrame.from_dict(\r\n",
    "            sample_dict, orient=\"index\", columns=columns)\r\n",
    "\r\n",
    "\r\n",
    "    def phoneme_dict(self):\r\n",
    "        phonemes = set()\r\n",
    "        for sample in self.sample_paths:\r\n",
    "            with open(sample + \".phn\", \"r\") as f:\r\n",
    "                for line in f.readlines():\r\n",
    "                    phonemes.add(line.split()[-1])\r\n",
    "        return {phoneme: i for i, phoneme in enumerate(phonemes)}\r\n",
    "\r\n",
    "\r\n",
    "    def spectral_features(self, path, type):\r\n",
    "        y, sr = librosa.load(path, librosa.get_samplerate(path))\r\n",
    "        S = librosa.feature.melspectrogram(\r\n",
    "                y=y, sr=sr, n_mels=128, fmax=8000)\r\n",
    "        if type==\"mel\":\r\n",
    "            S = librosa.power_to_db(S, ref=np.max)\r\n",
    "        elif type==\"mfcc\":\r\n",
    "            S = librosa.feature.mfcc(\r\n",
    "                S=S, n_mfcc=128).T\r\n",
    "        else:\r\n",
    "            logging.error(\"Invalid type input, only mel or mfcc allowed.\")\r\n",
    "        return torch.from_numpy(S)\r\n",
    "\r\n",
    "\r\n",
    "    def process_file(self, path):\r\n",
    "        wav_path = path + \".wav\"\r\n",
    "        phn_path = path + \".phn\"\r\n",
    "        with open(phn_path, \"r\") as f:\r\n",
    "            frames, phonemes = [0], []\r\n",
    "            for line in f.readlines():\r\n",
    "                _, time, phoneme = line.split(\" \")\r\n",
    "                frames.append(int(time))\r\n",
    "                phonemes.append(phoneme.strip('\\n'))\r\n",
    "        frames = torch.Tensor(frames)\r\n",
    "        phoneme = np.array(phonemes)   \r\n",
    "\r\n",
    "        mfcc = self.spectral_features(wav_path, type=\"mel\")\r\n",
    "        S = self.spectral_features(wav_path, type=\"mfcc\")\r\n",
    "        return S, mfcc, frames, phonemes\r\n",
    "\r\n",
    "\r\n",
    "    def split_dataset(self):\r\n",
    "        data = self.dataframe()\r\n",
    "        train, test = train_test_split(\r\n",
    "            data, test_size=0.25, shuffle=True,\r\n",
    "            stratify=data['speaker_id'], random_state=42)\r\n",
    "        return train, test\r\n",
    "\r\n",
    "dataset = TIMITDataset(main_dir=\"TIMIT-dataset\\data\", mode=\"partial\")\r\n",
    "train, test = dataset.split_dataset()\r\n",
    "train"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating:   0%|          | 0/6300 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c39cc090d23e46c3bc3ff4143d1683d9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     sample_name speaker_id  \\\n",
       "3687       SX319      FJCS0   \n",
       "6171         SA2      MKDD0   \n",
       "905        SX158      MDEM0   \n",
       "334       SI2327      MPSW0   \n",
       "397        SX309      MRSO0   \n",
       "...          ...        ...   \n",
       "3991         SA2      MDHL0   \n",
       "1352      SI1298      MRJM1   \n",
       "5689        SX43      MNLS0   \n",
       "1779       SX393      MAPV0   \n",
       "47         SX339      FDML0   \n",
       "\n",
       "                                             spec_array  \\\n",
       "3687  [[tensor(1.2060e-05), tensor(8.8126e-06), tens...   \n",
       "6171  [[tensor(7.4580e-06), tensor(6.2724e-06), tens...   \n",
       "905   [[tensor(0.0003), tensor(0.0002), tensor(-0.00...   \n",
       "334   [[tensor(1.0861e-05), tensor(1.1324e-05), tens...   \n",
       "397   [[tensor(1.7078e-05), tensor(1.3041e-05), tens...   \n",
       "...                                                 ...   \n",
       "3991  [[tensor(0.0003), tensor(9.2377e-05), tensor(-...   \n",
       "1352  [[tensor(3.3208e-06), tensor(1.6774e-06), tens...   \n",
       "5689  [[tensor(3.2796e-05), tensor(2.5904e-05), tens...   \n",
       "1779  [[tensor(1.0439e-05), tensor(7.2835e-06), tens...   \n",
       "47    [[tensor(4.6177e-06), tensor(2.8388e-06), tens...   \n",
       "\n",
       "                                             mfcc_array  \\\n",
       "3687  [[tensor(-65.7270), tensor(-65.1304), tensor(-...   \n",
       "6171  [[tensor(-50.0056), tensor(-52.0300), tensor(-...   \n",
       "905   [[tensor(-74.3413), tensor(-76.5413), tensor(-...   \n",
       "334   [[tensor(-55.5831), tensor(-57.2442), tensor(-...   \n",
       "397   [[tensor(-52.4337), tensor(-51.8094), tensor(-...   \n",
       "...                                                 ...   \n",
       "3991  [[tensor(-64.3449), tensor(-64.8335), tensor(-...   \n",
       "1352  [[tensor(-58.9032), tensor(-58.2838), tensor(-...   \n",
       "5689  [[tensor(-57.8896), tensor(-59.7843), tensor(-...   \n",
       "1779  [[tensor(-66.2206), tensor(-67.2553), tensor(-...   \n",
       "47    [[tensor(-63.9279), tensor(-59.9461), tensor(-...   \n",
       "\n",
       "                                            frame_array  \\\n",
       "3687  [tensor(0.), tensor(4360.), tensor(5320.), ten...   \n",
       "6171  [tensor(0.), tensor(2180.), tensor(2450.), ten...   \n",
       "905   [tensor(0.), tensor(2600.), tensor(2799.), ten...   \n",
       "334   [tensor(0.), tensor(1949.), tensor(3270.), ten...   \n",
       "397   [tensor(0.), tensor(1960.), tensor(2597.), ten...   \n",
       "...                                                 ...   \n",
       "3991  [tensor(0.), tensor(2170.), tensor(2750.), ten...   \n",
       "1352  [tensor(0.), tensor(1086.), tensor(1437.), ten...   \n",
       "5689  [tensor(0.), tensor(2321.), tensor(3103.), ten...   \n",
       "1779  [tensor(0.), tensor(2310.), tensor(4840.), ten...   \n",
       "47    [tensor(0.), tensor(790.), tensor(2255.), tens...   \n",
       "\n",
       "                                          phoneme_array  \n",
       "3687  [h#, ah, bcl, b, ih, gcl, g, ow, tcl, q, ay, d...  \n",
       "6171  [h#, d, ow, n, q, ae, s, kcl, m, iy, tcl, t, i...  \n",
       "905   [h#, dh, ix, dcl, d, r, ah, n, kcl, k, er, dcl...  \n",
       "334   [h#, em, s, aa, r, ix, v, ix, dx, ae, sh, epi,...  \n",
       "397   [h#, dh, ax, pcl, p, r, uw, f, epi, dh, eh, q,...  \n",
       "...                                                 ...  \n",
       "3991  [h#, d, ow, n, q, ae, s, kcl, m, iy, tcl, t, i...  \n",
       "1352  [h#, b, ax-h, tcl, t, w, iy, n, m, iy, dx, iy,...  \n",
       "5689  [h#, q, eh, l, dcl, d, axr, l, iy, pcl, p, iy,...  \n",
       "1779  [h#, sh, iy, y, ux, z, ix, z, bcl, b, ow, th, ...  \n",
       "47    [h#, dcl, d, ih, dcl, sh, ao, n, kcl, k, ae, t...  \n",
       "\n",
       "[4725 rows x 6 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_name</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>spec_array</th>\n",
       "      <th>mfcc_array</th>\n",
       "      <th>frame_array</th>\n",
       "      <th>phoneme_array</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3687</th>\n",
       "      <td>SX319</td>\n",
       "      <td>FJCS0</td>\n",
       "      <td>[[tensor(1.2060e-05), tensor(8.8126e-06), tens...</td>\n",
       "      <td>[[tensor(-65.7270), tensor(-65.1304), tensor(-...</td>\n",
       "      <td>[tensor(0.), tensor(4360.), tensor(5320.), ten...</td>\n",
       "      <td>[h#, ah, bcl, b, ih, gcl, g, ow, tcl, q, ay, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6171</th>\n",
       "      <td>SA2</td>\n",
       "      <td>MKDD0</td>\n",
       "      <td>[[tensor(7.4580e-06), tensor(6.2724e-06), tens...</td>\n",
       "      <td>[[tensor(-50.0056), tensor(-52.0300), tensor(-...</td>\n",
       "      <td>[tensor(0.), tensor(2180.), tensor(2450.), ten...</td>\n",
       "      <td>[h#, d, ow, n, q, ae, s, kcl, m, iy, tcl, t, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>SX158</td>\n",
       "      <td>MDEM0</td>\n",
       "      <td>[[tensor(0.0003), tensor(0.0002), tensor(-0.00...</td>\n",
       "      <td>[[tensor(-74.3413), tensor(-76.5413), tensor(-...</td>\n",
       "      <td>[tensor(0.), tensor(2600.), tensor(2799.), ten...</td>\n",
       "      <td>[h#, dh, ix, dcl, d, r, ah, n, kcl, k, er, dcl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>SI2327</td>\n",
       "      <td>MPSW0</td>\n",
       "      <td>[[tensor(1.0861e-05), tensor(1.1324e-05), tens...</td>\n",
       "      <td>[[tensor(-55.5831), tensor(-57.2442), tensor(-...</td>\n",
       "      <td>[tensor(0.), tensor(1949.), tensor(3270.), ten...</td>\n",
       "      <td>[h#, em, s, aa, r, ix, v, ix, dx, ae, sh, epi,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>SX309</td>\n",
       "      <td>MRSO0</td>\n",
       "      <td>[[tensor(1.7078e-05), tensor(1.3041e-05), tens...</td>\n",
       "      <td>[[tensor(-52.4337), tensor(-51.8094), tensor(-...</td>\n",
       "      <td>[tensor(0.), tensor(1960.), tensor(2597.), ten...</td>\n",
       "      <td>[h#, dh, ax, pcl, p, r, uw, f, epi, dh, eh, q,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3991</th>\n",
       "      <td>SA2</td>\n",
       "      <td>MDHL0</td>\n",
       "      <td>[[tensor(0.0003), tensor(9.2377e-05), tensor(-...</td>\n",
       "      <td>[[tensor(-64.3449), tensor(-64.8335), tensor(-...</td>\n",
       "      <td>[tensor(0.), tensor(2170.), tensor(2750.), ten...</td>\n",
       "      <td>[h#, d, ow, n, q, ae, s, kcl, m, iy, tcl, t, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352</th>\n",
       "      <td>SI1298</td>\n",
       "      <td>MRJM1</td>\n",
       "      <td>[[tensor(3.3208e-06), tensor(1.6774e-06), tens...</td>\n",
       "      <td>[[tensor(-58.9032), tensor(-58.2838), tensor(-...</td>\n",
       "      <td>[tensor(0.), tensor(1086.), tensor(1437.), ten...</td>\n",
       "      <td>[h#, b, ax-h, tcl, t, w, iy, n, m, iy, dx, iy,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5689</th>\n",
       "      <td>SX43</td>\n",
       "      <td>MNLS0</td>\n",
       "      <td>[[tensor(3.2796e-05), tensor(2.5904e-05), tens...</td>\n",
       "      <td>[[tensor(-57.8896), tensor(-59.7843), tensor(-...</td>\n",
       "      <td>[tensor(0.), tensor(2321.), tensor(3103.), ten...</td>\n",
       "      <td>[h#, q, eh, l, dcl, d, axr, l, iy, pcl, p, iy,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>SX393</td>\n",
       "      <td>MAPV0</td>\n",
       "      <td>[[tensor(1.0439e-05), tensor(7.2835e-06), tens...</td>\n",
       "      <td>[[tensor(-66.2206), tensor(-67.2553), tensor(-...</td>\n",
       "      <td>[tensor(0.), tensor(2310.), tensor(4840.), ten...</td>\n",
       "      <td>[h#, sh, iy, y, ux, z, ix, z, bcl, b, ow, th, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>SX339</td>\n",
       "      <td>FDML0</td>\n",
       "      <td>[[tensor(4.6177e-06), tensor(2.8388e-06), tens...</td>\n",
       "      <td>[[tensor(-63.9279), tensor(-59.9461), tensor(-...</td>\n",
       "      <td>[tensor(0.), tensor(790.), tensor(2255.), tens...</td>\n",
       "      <td>[h#, dcl, d, ih, dcl, sh, ao, n, kcl, k, ae, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4725 rows × 6 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class Segmentor(nn.Module):\r\n",
    "    def __init__(self, args):\r\n",
    "        super(Segmentor, self).__init__()\r\n",
    "        self.rnn = nn.LSTM(\r\n",
    "            input_size=args.rnn_input_size,\r\n",
    "            hidden_size=args.rnn_hidden_size,\r\n",
    "            num_layers=args.rnn_layers,\r\n",
    "            bias=True,\r\n",
    "            batch_first=True,\r\n",
    "            dropout=args.rnn_dropout,\r\n",
    "            bidirectional=True,\r\n",
    "            proj_size=0)\r\n",
    "\r\n",
    "        self.scorer = nn.Sequential(\r\n",
    "            nn.PReLU(),\r\n",
    "            nn.Linear(2 * 3 * args.rnn_hidden_size, 100),\r\n",
    "            nn.PReLU(),\r\n",
    "            nn.Linear(100, 1))          \r\n",
    "\r\n",
    "        self.classifier = nn.Sequential(\r\n",
    "            nn.PReLU(),\r\n",
    "            nn.Linear(2 * args.rnn_hidden_size, args.n_classes * 2),\r\n",
    "            nn.PReLU(),\r\n",
    "            nn.Linear(args.n_classes * 2, args.n_classes))\r\n",
    "\r\n",
    "        self.bi_classifier = nn.Sequential(\r\n",
    "            nn.PReLU(),\r\n",
    "            nn.Linear(2 * args.rnn_hidden_size, args.n_classes * 2),\r\n",
    "            nn.PReLU(),\r\n",
    "            nn.Linear(args.n_classes * 2, 2))\r\n",
    "\r\n",
    "    def compute_phi(self, rnn_out, rnn_cum):\r\n",
    "        batch_size, seq_len, feat_dim = rnn_out.shape\r\n",
    "        out_dim = (batch_size, seq_len, seq_len, feat_dim)\r\n",
    "\r\n",
    "        a = rnn_cum.repeat(1, seq_len, 1)\r\n",
    "        b = rnn_cum.repeat(1, 1, seq_len).view(batch_size, -1, feat_dim)\r\n",
    "        c = a.sub(b).view(out_dim)\r\n",
    "        d = rnn_out.repeat(1, 1, seq_len).view(out_dim)\r\n",
    "        e = rnn_out.repeat(1, seq_len, 1).view(out_dim)\r\n",
    "        return torch.cat([c, d, e], dim=-1)\r\n",
    "\r\n",
    "    def compute_scores(self, phi):\r\n",
    "        return self.scorer(phi).squeeze(-1)\r\n",
    "\r\n",
    "    def compute_segmentation_score(self, scores, segments):\r\n",
    "        out_scores = torch.zeros((scores.shape[0])).to(scores.device)\r\n",
    "        for seg_idx, segment in enumerate(segments):\r\n",
    "            score = 0\r\n",
    "            for start, end in zip(segment[:-1], segment[1:]):\r\n",
    "                score += scores[seg_idx, start, end]\r\n",
    "            out_scores[seg_idx] = score\r\n",
    "        return out_scores\r\n",
    "\r\n",
    "    def segment_search(self, scores, lengths):\r\n",
    "        batch_size, max_length = scores.shape[:2]\r\n",
    "        scores, lengths = scores.to('cpu'), lengths.to('cpu')\r\n",
    "        best_scores = torch.zeros(batch_size, max_length)\r\n",
    "        segments = [[[0]] for _ in range(batch_size)]\r\n",
    "\r\n",
    "        for i in range(1, max_length):\r\n",
    "            start_idx = max(0, i - args.max_seg_size)\r\n",
    "            end_idx = i\r\n",
    "            current_scores = torch.zeros(batch_size, end_idx - start_idx)\r\n",
    "            for j in range(start_idx, end_idx):\r\n",
    "                current_scores[:, j - start_idx] = \\\r\n",
    "                    best_scores[:, j] + scores[:, j, i]\r\n",
    "            max_scores, k = torch.max(current_scores, 1)\r\n",
    "            k += start_idx\r\n",
    "            best_scores[:, i] = max_scores\r\n",
    "            for batch_idx in range(batch_size):\r\n",
    "                current_segment = segments[batch_idx][k[batch_idx]] + [i]\r\n",
    "                segments[batch_idx].append(current_segment)\r\n",
    "\r\n",
    "        pred_segment = []\r\n",
    "        for i, segment in enumerate(segments):\r\n",
    "            last_idx = lengths[i].item() - 1\r\n",
    "            pred_segment.append(segment[last_idx])\r\n",
    "        \r\n",
    "        return pred_segment\r\n",
    "\r\n",
    "    def forward(self, x, length, gt_seg=None):\r\n",
    "        results = {}\r\n",
    "        x = nn.utils.rnn.pack_padded_sequence(\r\n",
    "            x, length, batch_first=True, enforce_sorted=False)\r\n",
    "        rnn_out, _ = self.rnn(x)\r\n",
    "        rnn_out, _ = nn_utils.rnn.pad_packed_sequence(\r\n",
    "            rnn_out, batch_first=True)\r\n",
    "        rnn_cum = torch.cumsum(rnn_out, dim=1)\r\n",
    "        phi = self.compute_phi(rnn_out, rnn_cum)\r\n",
    "\r\n",
    "        results['clf_out'] = self.classifier(rnn_out)\r\n",
    "        results['bi_clf_out'] = self.bi_classifier(rnn_out)\r\n",
    "\r\n",
    "        scores = self.compute_scores(phi)\r\n",
    "        results['pred'] = self.segment_search(scores, length)\r\n",
    "        results['pred_scores'] = self.compute_segmentation_score(\r\n",
    "            scores, results['pred'])\r\n",
    "        \r\n",
    "        if gt_seg is not None:\r\n",
    "            results['gt_scores'] = self.compute_segmentation_score(\r\n",
    "                scores, gt_seg)\r\n",
    "        \r\n",
    "        return results"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class Solver(pl.LightningModule):\r\n",
    "    def __init__(self, args):\r\n",
    "        super(Solver, self).__init__()\r\n",
    "\r\n",
    "        self.train, self.test = TIMITDataset(args.main_dir, )\r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('py-gpu': conda)"
  },
  "interpreter": {
   "hash": "eaf12c11910b5724464628557fb6b4134fca282894ab291643cad5d07db96546"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}