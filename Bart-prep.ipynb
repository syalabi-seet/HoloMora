{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import cutlet\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "from transformers import T5Tokenizer, TFT5ForConditionalGeneration\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# katsu = cutlet.Cutlet()\n",
    "# katsu.use_foreign_spelling = False\n",
    "\n",
    "# JA_unicode = []\n",
    "# with open(\"E:\\Datasets\\ASR-dataset\\JA_unicode.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     lines = f.readlines()\n",
    "#     for line in lines:\n",
    "#         line = line.split()[1:]\n",
    "#         for char in line:\n",
    "#             JA_unicode.append(char)\n",
    "\n",
    "# def clean_kanji(sentence):\n",
    "#     symbols = f\"[^{JA_unicode}]\"\n",
    "#     sentence = re.sub(symbols, \"\", sentence).strip()\n",
    "#     symbols = f\"[（.*?）！-～.,;..._。、-〿・■（）：ㇰ-ㇿ㈠-㉃㊀-㋾㌀-㍿「」『』→ー -~‘–※π—ゐ’“”]\"\n",
    "#     sentence = re.sub(symbols, \"\", sentence).strip()\n",
    "#     return sentence\n",
    "\n",
    "# def clean_romaji(sentence):\n",
    "#     sentence = sentence.strip()\n",
    "#     sentence = re.sub(r'[.,\"\\'\\/?]', \"\", sentence)\n",
    "#     sentence = sentence.split()\n",
    "#     for i, mora in enumerate(sentence):\n",
    "#         if (mora == \"n\") | (mora == \"u\") & (i < len(sentence) - 1):\n",
    "#             prev_mora = sentence.pop(i-1)\n",
    "#             sentence[i-1] = \"\".join([prev_mora, mora])\n",
    "#     sentence = \" \".join(sentence)\n",
    "#     return sentence\n",
    "\n",
    "# def kanji2romaji(text):\n",
    "#     try:\n",
    "#         new_line = clean_kanji(text)\n",
    "#         new_line = katsu.romaji(new_line)\n",
    "#         new_line = clean_romaji(new_line)\n",
    "#     except:\n",
    "#         new_line = None\n",
    "#     return new_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_dir = \"D:\\School-stuff\\Sem-2\\PR-Project\\HoloASR\\Datasets\"\n",
    "# opus_ja_paths = glob.glob(f\"{main_dir}\\OPUS100-dataset\\*.ja\")\n",
    "# tatoeba_ja_paths = glob.glob(f\"{main_dir}\\Tatoeba-dataset\\*.ja\")\n",
    "# coursera_ja_paths = glob.glob(f\"{main_dir}\\Coursera-dataset\\*.ja.txt\")\n",
    "\n",
    "# ja_paths = opus_ja_paths + tatoeba_ja_paths + coursera_ja_paths\n",
    "\n",
    "# ja_lines, en_lines = [], []\n",
    "# for ja_path in ja_paths:\n",
    "#     if ja_path.endswith(\".ja\"):\n",
    "#         en_path = ja_path.rsplit(\".\", 1)[0] + \".en\"\n",
    "#     else:\n",
    "#         en_path = ja_path.replace(\"ja\", \"en\")\n",
    "#     with open(ja_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         lines = [line.strip(\"\\n\") for line in f.readlines()]\n",
    "#         ja_lines.extend(lines)\n",
    "#     with open(en_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         lines = [line.strip(\"\\n\") for line in f.readlines()]\n",
    "#         en_lines.extend(lines)\n",
    "\n",
    "# tqdm.pandas()\n",
    "# data = pd.DataFrame({'ja_raw': ja_lines, 'en': en_lines})\n",
    "# data['ja_ro'] = data['ja_raw'].progress_apply(kanji2romaji)\n",
    "# data = data[data['ja_ro'].notnull()].reset_index(drop=True)\n",
    "# data.to_csv(\n",
    "#     r\"E:\\Datasets\\ASR-dataset\\tokenizer_text\\tokenizer_text.csv\", \n",
    "#     index=False, encoding=\"utf-8\")\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(r\"E:\\Datasets\\ASR-dataset\\tokenizer_text\\tokenizer_text.csv\")\n",
    "# data = data.dropna().reset_index(drop=True)\n",
    "\n",
    "# with open(r\"E:\\Datasets\\ASR-dataset\\tokenizer_text\\tokenizer_text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for row in tqdm(data.iterrows(), total=len(data)):\n",
    "#         idx, (_, en, ja) = row\n",
    "#         row =  \" \".join([en, ja]) + \"\\n\"\n",
    "#         f.write(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_file = r\"E:\\Datasets\\ASR-dataset\\tokenizer_text\\tokenizer_text.txt\"\n",
    "# vocab_size = 32128\n",
    "# spm.SentencePieceTrainer.train(\n",
    "#     '--input={} --model_prefix=t5 --vocab_size={} --pad_id=0 --unk_id=1 --bos_id=-1 --eos_id=2 --pad_piece=<pad> --unk_piece=<unk> --eos_piece=</s>'.format(\n",
    "#         text_file, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ArgParser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--random_state\", default=42)\n",
    "\n",
    "    parser.add_argument(\"--main_dir\", default=\"E://Datasets/ASR-dataset\")\n",
    "    parser.add_argument(\"--n_shards\", default=10)\n",
    "    parser.add_argument(\"--test_size\", default=0.1)\n",
    "    parser.add_argument(\"--batch_size\", default=4)\n",
    "    parser.add_argument(\"--buffer_size\", default=512)\n",
    "\n",
    "    parser.add_argument(\"--learning_rate\", default=3e-4)\n",
    "\n",
    "    return parser.parse_known_args()[0]\n",
    "\n",
    "args = ArgParser()\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFRWriter():\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.tokenizer = T5Tokenizer(vocab_file=\"t5.model\")\n",
    "        self.task_prefix = \"translate Japanese to English: \"\n",
    "        self.data = self.get_data()    \n",
    "\n",
    "    def get_data(self):\n",
    "        tqdm.pandas()\n",
    "        data = pd.read_csv(\n",
    "            os.path.join(self.args.main_dir, r\"tokenizer_text\\tokenizer_text.csv\"), \n",
    "            encoding=\"utf-8\")\n",
    "        data = data.dropna().reset_index(drop=True)[['ja_ro', 'en']]\n",
    "        data['ja_token'] = data['ja_ro'].progress_apply(\n",
    "            lambda x: self.tokenizer(self.task_prefix + x).input_ids)\n",
    "        data['en_token'] = data['en'].progress_apply(\n",
    "            lambda x: self.tokenizer(x).input_ids)\n",
    "        data['ja_len'] = data['ja_token'].apply(len)\n",
    "        data = data.query(\"ja_len <= 45\")\n",
    "        data = data.sort_values(by=\"ja_len\", ignore_index=True, ascending=True)\n",
    "        return data\n",
    "\n",
    "    def _bytes_feature(self, value):\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    def serialize_example(self, *args):\n",
    "        feature = {\n",
    "            'input_ids': self._bytes_feature(args[0]),\n",
    "            'attention_mask': self._bytes_feature(args[1]),\n",
    "            'labels': self._bytes_feature(args[2])}\n",
    "\n",
    "        example_proto = tf.train.Example(\n",
    "            features=tf.train.Features(feature=feature))\n",
    "        return example_proto.SerializeToString()\n",
    "\n",
    "    def get_shards(self):\n",
    "        skf = KFold(n_splits=self.args.n_shards, shuffle=False)\n",
    "        return [j for i,j in skf.split(self.data)]\n",
    "\n",
    "    def get_shard_data(self, samples):\n",
    "        pad_token_id = self.tokenizer.pad_token_id\n",
    "        for sample in samples:\n",
    "            input_ids = tf.convert_to_tensor(\n",
    "                self.data['ja_token'][sample], dtype=tf.int32)\n",
    "            attention_mask = tf.where(input_ids != 0, x=1, y=0)\n",
    "            labels = tf.convert_to_tensor(\n",
    "                self.data['en_token'][sample], dtype=tf.int32)\n",
    "            yield {\n",
    "                \"input_ids\": tf.io.serialize_tensor(input_ids),\n",
    "                \"attention_mask\": tf.io.serialize_tensor(attention_mask),\n",
    "                \"labels\": tf.io.serialize_tensor(labels)\n",
    "            }\n",
    "\n",
    "    def write(self):\n",
    "        for shard, samples in tqdm(enumerate(self.get_shards()), total=self.args.n_shards):\n",
    "            with tf.io.TFRecordWriter(f\"{self.args.main_dir}/bart_tfrec/shard_{shard+1}.tfrec\") as f:\n",
    "                for sample in self.get_shard_data(samples):\n",
    "                    example = self.serialize_example(\n",
    "                        sample['input_ids'], \n",
    "                        sample['attention_mask'],\n",
    "                        sample['labels'])\n",
    "                    f.write(example)\n",
    "\n",
    "# TFRWriter(args).write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, args):\n",
    "        self.files = glob.glob(args.main_dir + \"/bart_tfrec/*.tfrec\")\n",
    "        self.args = args\n",
    "        self.AUTOTUNE = tf.data.AUTOTUNE\n",
    "        self.train_files, self.val_files = train_test_split(\n",
    "            self.files, test_size=args.test_size, shuffle=True, \n",
    "            random_state=args.random_state)\n",
    "        self.train = self.get_train()\n",
    "        self.val = self.get_val()\n",
    "\n",
    "    def read_tfrecord(self, example):\n",
    "        feature_description = {\n",
    "            'input_ids': tf.io.FixedLenFeature([], tf.string),\n",
    "            'attention_mask': tf.io.FixedLenFeature([], tf.string),\n",
    "            'labels': tf.io.FixedLenFeature([], tf.string)}\n",
    "        \n",
    "        example = tf.io.parse_single_example(example, feature_description)\n",
    "        example['input_ids'] = tf.io.parse_tensor(\n",
    "            example['input_ids'], out_type=tf.int32)\n",
    "        example['attention_mask'] = tf.io.parse_tensor(\n",
    "            example['attention_mask'], out_type=tf.int32) \n",
    "        example['labels'] = tf.io.parse_tensor(\n",
    "            example['labels'], out_type=tf.int32)\n",
    "        return example\n",
    "\n",
    "    def load_dataset(self, files):\n",
    "        ignore_order = tf.data.Options()\n",
    "        ignore_order.experimental_deterministic = False\n",
    "        dataset = tf.data.TFRecordDataset(files)\n",
    "        dataset = dataset.with_options(ignore_order)\n",
    "        dataset = dataset.map(self.read_tfrecord, num_parallel_calls=self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_train(self):\n",
    "        dataset = self.load_dataset(self.train_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_ids': [None],\n",
    "                'attention_mask': [None],\n",
    "                'labels': [None]\n",
    "            },\n",
    "            padding_values={\n",
    "                'input_ids': tf.constant(0, dtype=tf.int32),\n",
    "                'attention_mask': tf.constant(0, dtype=tf.int32),\n",
    "                'labels': tf.constant(-100, dtype=tf.int32)\n",
    "            })        \n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_val(self):\n",
    "        dataset = self.load_dataset(self.val_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_ids': [None],\n",
    "                'attention_mask': [None],\n",
    "                'labels': [None]\n",
    "            },\n",
    "            padding_values={\n",
    "                'input_ids': tf.constant(0, dtype=tf.int32),\n",
    "                'attention_mask': tf.constant(0, dtype=tf.int32),\n",
    "                'labels': tf.constant(-100, dtype=tf.int32)\n",
    "            })\n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "train = DataLoader(args).train\n",
    "\n",
    "inputs = next(iter(train))\n",
    "input_values = inputs['input_ids']\n",
    "labels = inputs['labels']\n",
    "attention_mask = inputs['attention_mask']\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineDecayWithWarmup(LearningRateSchedule):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "    def __call__(self, epoch):  \n",
    "        if epoch < self.args.warmup_epochs:\n",
    "            lr = ((self.args.lr_max - self.args.lr_start) / self.args.warmup_epochs) * epoch + self.args.lr_start\n",
    "        elif epoch < (self.args.warmup_epochs + self.args.sustain_epochs):\n",
    "            lr = self.args.lr_max\n",
    "        else:\n",
    "            progress = ((epoch - self.args.warmup_epochs - self.args.sustain_epochs) / \n",
    "            (self.args.epochs - self.args.warmup_epochs - self.args.sustain_epochs))\n",
    "            lr = (self.args.lr_max-self.args.lr_min) * (0.5 * (1.0 + tf.math.cos((22/7) * \n",
    "                self.args.n_cycles * 2.0 * progress)))\n",
    "            if self.args.lr_min is not None:\n",
    "                lr = tf.math.maximum(self.args.lr_min, lr)\n",
    "        return lr\n",
    "\n",
    "    def plot(self):\n",
    "        epochs = range(self.args.epochs+1)\n",
    "        lr = [self(epoch) for epoch in epochs]\n",
    "        plt.plot(epochs, lr)\n",
    "        plt.show()\n",
    "\n",
    "CosineDecayWithWarmup(args).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.tokenizer = T5Tokenizer(vocab_file=\"t5.model\")\n",
    "        self.train_dataset = DataLoader(args).train\n",
    "        self.val_dataset = DataLoader(args).val\n",
    "        schedule = CosineDecayWithWarmup(args)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(schedule)\n",
    "        self.gradient_accumulator = GradientAccumulator()\n",
    "        self.gradient_accumulator.accum_steps = args.accum_steps\n",
    "        self.model = TFT5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "        \n",
    "        self.model_name = f\"model_{int(self.args.n_samples/1000)}k\"\n",
    "        self.log_path = f\"{self.args.main_dir}\\model_weights_bart\\{self.model_name}.csv\"\n",
    "        if not os.path.exists(self.log_path):\n",
    "            print(\"Log file created.\")\n",
    "            columns = \"epoch,loss,per,wer,val_loss,val_per,val_wer\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(columns)\n",
    "\n",
    "        # Checkpointing\n",
    "        self.ckpt_dir = f\"{self.args.main_dir}/checkpoints_bart_{int(self.args.n_samples/1000)}k\"\n",
    "        self.ckpt = tf.train.Checkpoint(self.model)\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(\n",
    "            checkpoint=self.ckpt, directory=self.ckpt_dir, max_to_keep=5)\n",
    "\n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            self.start_epoch = int(self.ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "            self.ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "            print(f\"Resuming from epoch {self.start_epoch + 1}...\")\n",
    "        else:\n",
    "            self.start_epoch = 0\n",
    "            print(\"Starting from epoch 1...\")\n",
    "\n",
    "    def decoder(self, labels, logits):\n",
    "        labels = tf.where(labels < 0, x=0, y=labels)\n",
    "        logits = tf.argmax(logits, axis=-1)\n",
    "        logits = self.config.processor.batch_decode(logits, group_tokens=True)\n",
    "        return labels, logits\n",
    "\n",
    "    def display(self, epoch, t_labels, t_logits, v_labels, v_logits):\n",
    "        print(\"-\" * 129)\n",
    "        print(\"Training\")\n",
    "        for y_true, y_pred in zip(t_labels, t_logits):\n",
    "            print(f\"Target:    {y_true}\")\n",
    "            print(f\"Predicted: {y_pred}\") \n",
    "\n",
    "        print(\"\\nValidation\")\n",
    "        for y_true, y_pred in zip(v_labels, v_logits):\n",
    "            print(f\"Target:    {y_true}\")\n",
    "            print(f\"Predicted: {y_pred}\")\n",
    "        print(\"-\" * 129)\n",
    "\n",
    "    def fit(self):\n",
    "        for epoch in range(self.start_epoch, self.args.epochs):\n",
    "            print(f\"Epoch {epoch+1}/{self.args.epochs}: Learning rate @ {self.optimizer.lr(epoch):.2e}\")\n",
    "            stateful_metrics = [\"loss\", \"per\", \"wer\", \"val_loss\", \"val_per\", \"val_wer\"]\n",
    "            progbar = tf.keras.utils.Progbar(\n",
    "                self.args.train_steps, interval=0.05,\n",
    "                stateful_metrics=stateful_metrics)\n",
    "\n",
    "            # Training loop\n",
    "            for step, t_batch in enumerate(self.train_dataset):\n",
    "                t_inputs = t_batch['input_values']\n",
    "                t_labels = t_batch['labels']\n",
    "                with tf.GradientTape() as tape:\n",
    "                    t_loss, t_logits = self.model(\n",
    "                        input_values=t_inputs, labels=t_labels, training=True)[:2]\n",
    "                self.gradient_accumulator(tape.gradient(\n",
    "                    t_loss, self.model.trainable_weights))\n",
    "                gradients = self.gradient_accumulator.gradients\n",
    "                self.optimizer.apply_gradients(\n",
    "                    zip(gradients, self.model.trainable_weights))\n",
    "                self.gradient_accumulator.reset()             \n",
    "                t_labels, t_logits = self.decoder(t_labels, t_logits)\n",
    "                self.per_metrics.update_state(t_labels, t_logits)\n",
    "                t_labels = self.config.processor.batch_decode(t_labels, group_tokens=False)      \n",
    "                self.wer_metrics.update_state(t_labels, t_logits)\n",
    "                t_per = self.per_metrics.result()\n",
    "                t_wer = self.wer_metrics.result()\n",
    "                t_values = [(\"loss\", t_loss), (\"per\", t_per), (\"wer\", t_wer)]\n",
    "                progbar.update(step, values=t_values, finalize=False)\n",
    "\n",
    "            self.per_metrics.reset_states()\n",
    "            self.wer_metrics.reset_states()\n",
    "            \n",
    "            # Validation loop\n",
    "            for v_batch in self.val_dataset:\n",
    "                v_inputs = v_batch['input_values']\n",
    "                v_labels = v_batch['labels']\n",
    "                v_loss, v_logits = self.model(\n",
    "                    input_values=v_inputs, labels=v_labels, training=False)[:2]       \n",
    "                v_labels, v_logits = self.decoder(v_labels, v_logits)               \n",
    "                self.per_metrics.update_state(v_labels, v_logits)\n",
    "                v_labels = self.config.processor.batch_decode(v_labels, group_tokens=False)\n",
    "                self.wer_metrics.update_state(v_labels, v_logits)\n",
    "\n",
    "            v_per = self.per_metrics.result()\n",
    "            v_wer = self.wer_metrics.result()\n",
    "            v_values = [\n",
    "                (\"loss\", t_loss), (\"per\", t_per), (\"wer\", t_wer), (\"val_loss\", v_loss),\n",
    "                (\"val_per\", v_per), (\"val_wer\", v_wer)]\n",
    "            progbar.update(self.args.train_steps, values=v_values, finalize=True)\n",
    "            self.per_metrics.reset_states()\n",
    "            self.wer_metrics.reset_states()\n",
    "\n",
    "            # Print sample transcriptions for both loops\n",
    "            self.display(epoch, t_labels, t_logits, v_labels, v_logits)\n",
    "\n",
    "            # Checkpointing\n",
    "            self.ckpt.save(file_prefix=f\"{self.ckpt_dir}/{self.model_name}\")\n",
    "\n",
    "            # Logging\n",
    "            log = f\"{epoch+1},{t_loss},{t_per},{t_wer},{v_loss},{v_per},{v_wer}\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(log)\n",
    "\n",
    "            save_path = f\"{self.args.main_dir}/model_weights\"\n",
    "            self.model.save_weights(f\"{save_path}/{self.model_name}_{epoch+1}.h5\")\n",
    "\n",
    "Trainer(args).fit()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b851d2923cfa3a2562599062e05fd9893d86a7c009c64d8ad3756552e4dd5f41"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('tf-gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
