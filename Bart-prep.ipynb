{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import glob\n",
    "import random\n",
    "import cutlet\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sacrebleu.metrics import BLEU\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    TFT5ForConditionalGeneration,\n",
    "    GradientAccumulator,\n",
    "    logging)\n",
    "\n",
    "def seed_everything(SEED):\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    print(\"Random seed set.\")\n",
    "\n",
    "seed_everything(42)\n",
    "tf.get_logger().setLevel('FATAL')\n",
    "logging.set_verbosity_error()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "katsu = cutlet.Cutlet()\n",
    "katsu.use_foreign_spelling = True\n",
    "\n",
    "JA_unicode = []\n",
    "with open(\"E:\\Datasets\\ASR-dataset\\JA_unicode.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.split()[1:]\n",
    "        for char in line:\n",
    "            JA_unicode.append(char)\n",
    "\n",
    "def clean_kanji(sentence):\n",
    "    symbols = f\"[^{JA_unicode}]\"\n",
    "    sentence = re.sub(symbols, \"\", sentence.strip())\n",
    "    symbols = r\"[（.*?）！-～.,;..._。、-〿・■（）：ㇰ-ㇿ㈠-㉃㊀-㋾㌀-㍿「」『』→ー -~‘–※π—ゐ’“”]\"\n",
    "    sentence = re.sub(symbols, \"\", sentence.strip())\n",
    "    return sentence\n",
    "\n",
    "def clean_romaji(sentence):\n",
    "    sentence = sentence.strip().lower()\n",
    "    sentence = re.sub(r\"[^a-zA-Z0-9\\ ]\", \"\", sentence)\n",
    "    sentence = sentence.split()\n",
    "    for i, mora in enumerate(sentence):\n",
    "        if (mora == \"n\") | (mora == \"u\") & (i < len(sentence) - 1):\n",
    "            prev_mora = sentence.pop(i-1)\n",
    "            sentence[i-1] = \"\".join([prev_mora, mora])\n",
    "    sentence = \" \".join(sentence)\n",
    "    return sentence\n",
    "\n",
    "def kanji2romaji(text):\n",
    "    try:\n",
    "        new_line = clean_kanji(text)\n",
    "        new_line = katsu.romaji(new_line)\n",
    "        new_line = clean_romaji(new_line)\n",
    "    except:\n",
    "        new_line = None\n",
    "    return new_line\n",
    "\n",
    "def clean_en(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r\"[^a-z0-9\\ \\?\\.\\!\\,\\'\\\"]\", \"\", sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd8c5bbb6f2492f9aa841c7c202ee29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1266032 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main_dir = \"D:\\School-stuff\\Sem-2\\PR-Project\\HoloASR\\Datasets\"\n",
    "opus_ja_paths = glob.glob(f\"{main_dir}\\OPUS100-dataset\\*.ja\")\n",
    "tatoeba_ja_paths = glob.glob(f\"{main_dir}\\Tatoeba-dataset\\*.ja\")\n",
    "coursera_ja_paths = glob.glob(f\"{main_dir}\\Coursera-dataset\\*.ja.txt\")\n",
    "\n",
    "ja_paths = opus_ja_paths + tatoeba_ja_paths + coursera_ja_paths\n",
    "\n",
    "ja_lines, en_lines = [], []\n",
    "for ja_path in ja_paths:\n",
    "    if ja_path.endswith(\".ja\"):\n",
    "        en_path = ja_path.rsplit(\".\", 1)[0] + \".en\"\n",
    "    else:\n",
    "        en_path = ja_path.replace(\"ja\", \"en\")\n",
    "    with open(ja_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [line.strip(\"\\n\") for line in f.readlines()]\n",
    "        ja_lines.extend(lines)\n",
    "    with open(en_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [line.strip(\"\\n\") for line in f.readlines()]\n",
    "        en_lines.extend(lines)\n",
    "\n",
    "tqdm.pandas()\n",
    "data = pd.DataFrame({'ja_raw': ja_lines, 'en': en_lines})\n",
    "data['ja_ro'] = data['ja_raw'].progress_apply(kanji2romaji)\n",
    "data = data[data['ja_ro'].notnull()].reset_index(drop=True)\n",
    "data['en'] = data['en'].progress_apply(clean_en)\n",
    "data.to_csv(\n",
    "    r\"E:\\Datasets\\ASR-dataset\\tokenizer_text\\tokenizer_text.csv\", \n",
    "    index=False, encoding=\"utf-8\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"E:\\Datasets\\ASR-dataset\\tokenizer_text\\tokenizer_text.csv\")\n",
    "data = data.dropna().reset_index(drop=True)\n",
    "\n",
    "with open(r\"E:\\Datasets\\ASR-dataset\\tokenizer_text\\tokenizer_text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for row in tqdm(data.iterrows(), total=len(data)):\n",
    "        idx, (_, en, ja) = row\n",
    "        row =  \" \".join([en, ja]) + \"\\n\"\n",
    "        f.write(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = r\"E:\\Datasets\\ASR-dataset\\tokenizer_text\\tokenizer_text.txt\"\n",
    "model_prefix = r\"E:\\Datasets\\ASR-dataset\\tokenizer_text\\t5\"\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    f\"--input={text_file} --model_prefix={model_prefix} --vocab_size={32128} --pad_id=0 --unk_id=1 --bos_id=-1 --eos_id=2 --pad_piece=<pad> --unk_piece=<unk> --eos_piece=</s>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFRWriter():\n",
    "    def __init__(self):\n",
    "        self.main_dir = \"E://Datasets/ASR-dataset\"\n",
    "        self.n_shards = 10\n",
    "        self.tokenizer = T5Tokenizer(\n",
    "            vocab_file=f\"{self.main_dir}/tokenizer_text/t5.model\",\n",
    "            eos_token=\"</s>\",\n",
    "            unk_token=\"<unk>\",\n",
    "            pad_token=\"<pad>\")\n",
    "        self.task_prefix = \"translate Japanese to English: \"\n",
    "        self.data = self.get_data()\n",
    "\n",
    "    def get_data(self):\n",
    "        tqdm.pandas()\n",
    "        data = pd.read_csv(\n",
    "            f\"{self.main_dir}/tokenizer_text/tokenizer_text.csv\", \n",
    "            encoding=\"utf-8\")\n",
    "        data = data.dropna().reset_index(drop=True)[['ja_ro', 'en']]\n",
    "        data['ja_token'] = data['ja_ro'].progress_apply(\n",
    "            lambda x: self.tokenizer(self.task_prefix + x.lower()).input_ids)\n",
    "        data['en_token'] = data['en'].progress_apply(\n",
    "            lambda x: self.tokenizer(x).input_ids)\n",
    "        data['ja_len'] = data['ja_token'].apply(len)\n",
    "        data = data.query(\"ja_len <= 21\")\n",
    "        data = data.sample(n=50000, random_state=42, ignore_index=True)\n",
    "        data = data.sort_values(by=\"ja_len\", ignore_index=True, ascending=True)\n",
    "        data.to_csv(\n",
    "            os.path.join(self.main_dir, r\"tokenizer_text\\tokenizer_text2.csv\"),\n",
    "            index=False)\n",
    "        return data\n",
    "\n",
    "    def _bytes_feature(self, value):\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    def serialize_example(self, *args):\n",
    "        feature = {\n",
    "            'input_ids': self._bytes_feature(args[0]),\n",
    "            'attention_mask': self._bytes_feature(args[1]),\n",
    "            'labels': self._bytes_feature(args[2])}\n",
    "\n",
    "        example_proto = tf.train.Example(\n",
    "            features=tf.train.Features(feature=feature))\n",
    "        return example_proto.SerializeToString()\n",
    "\n",
    "    def get_shards(self):\n",
    "        skf = KFold(n_splits=self.n_shards, shuffle=False)\n",
    "        return [j for i,j in skf.split(self.data)]\n",
    "\n",
    "    def get_shard_data(self, samples):\n",
    "        for sample in samples:\n",
    "            input_ids = tf.convert_to_tensor(\n",
    "                self.data['ja_token'][sample], dtype=tf.int32)\n",
    "            attention_mask = tf.where(input_ids != 0, x=1, y=0)\n",
    "            labels = tf.convert_to_tensor(\n",
    "                self.data['en_token'][sample], dtype=tf.int32)\n",
    "            yield {\n",
    "                \"input_ids\": tf.io.serialize_tensor(input_ids),\n",
    "                \"attention_mask\": tf.io.serialize_tensor(attention_mask),\n",
    "                \"labels\": tf.io.serialize_tensor(labels)\n",
    "            }\n",
    "\n",
    "    def write(self):\n",
    "        for shard, samples in tqdm(enumerate(self.get_shards()), total=self.n_shards):\n",
    "            with tf.io.TFRecordWriter(f\"{self.main_dir}/bart_tfrec/shard_{shard+1}.tfrec\") as f:\n",
    "                for sample in self.get_shard_data(samples):\n",
    "                    example = self.serialize_example(\n",
    "                        sample['input_ids'],\n",
    "                        sample['attention_mask'],\n",
    "                        sample['labels'],\n",
    "                        )\n",
    "                    f.write(example)\n",
    "\n",
    "TFRWriter().write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ArgParser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--random_state\", default=42)\n",
    "    parser.add_argument(\"--main_dir\", default=\"E://Datasets/ASR-dataset\")\n",
    "    parser.add_argument(\"--n_shards\", default=10)\n",
    "    parser.add_argument(\"--test_size\", default=0.1)\n",
    "    parser.add_argument(\"--batch_size\", default=16)\n",
    "    parser.add_argument(\"--buffer_size\", default=512)\n",
    "\n",
    "    # Trainer\n",
    "    parser.add_argument(\"--accum_steps\", default=2)\n",
    "\n",
    "    # Scheduler\n",
    "    parser.add_argument(\"--learning_rate\", default=5e-5)\n",
    "    parser.add_argument(\"--epochs\", default=5)\n",
    "\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    n_samples = len(pd.read_csv(\n",
    "        f\"{args.main_dir}/tokenizer_text/tokenizer_text2.csv\"))   \n",
    "\n",
    "    n_train = int(n_samples * (1 - args.test_size))\n",
    "    n_val = int(n_samples * args.test_size)\n",
    "    train_steps = int(np.ceil(n_train / args.batch_size))\n",
    "    val_steps = int(np.ceil(n_val / args.batch_size))\n",
    "        \n",
    "    parser.add_argument(\"--n_samples\", default=n_samples)\n",
    "    parser.add_argument(\"--n_train\", default=n_train)\n",
    "    parser.add_argument(\"--n_val\", default=n_val)\n",
    "    parser.add_argument(\"--train_steps\", default=train_steps)  \n",
    "    parser.add_argument(\"--val_steps\", default=val_steps)\n",
    "\n",
    "    return parser.parse_known_args()[0]\n",
    "\n",
    "args = ArgParser()\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, args):\n",
    "        self.files = glob.glob(args.main_dir + \"/bart_tfrec/*.tfrec\")\n",
    "        self.args = args\n",
    "        self.AUTOTUNE = tf.data.AUTOTUNE\n",
    "        self.train_files, self.val_files = train_test_split(\n",
    "            self.files, test_size=args.test_size, shuffle=True, \n",
    "            random_state=args.random_state)\n",
    "        self.train = self.get_train()\n",
    "        self.val = self.get_val()\n",
    "\n",
    "    def read_tfrecord(self, example):\n",
    "        feature_description = {\n",
    "            'input_ids': tf.io.FixedLenFeature([], tf.string),\n",
    "            'attention_mask': tf.io.FixedLenFeature([], tf.string),\n",
    "            'labels': tf.io.FixedLenFeature([], tf.string)\n",
    "            }\n",
    "        \n",
    "        example = tf.io.parse_single_example(example, feature_description)\n",
    "        example['input_ids'] = tf.io.parse_tensor(\n",
    "            example['input_ids'], out_type=tf.int32)\n",
    "        example['attention_mask'] = tf.io.parse_tensor(\n",
    "            example['attention_mask'], out_type=tf.int32) \n",
    "        example['labels'] = tf.io.parse_tensor(\n",
    "            example['labels'], out_type=tf.int32)\n",
    "        return example\n",
    "\n",
    "    def load_dataset(self, files):\n",
    "        ignore_order = tf.data.Options()\n",
    "        ignore_order.experimental_deterministic = False\n",
    "        dataset = tf.data.TFRecordDataset(files)\n",
    "        dataset = dataset.with_options(ignore_order)\n",
    "        dataset = dataset.map(self.read_tfrecord, num_parallel_calls=self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_train(self):\n",
    "        dataset = self.load_dataset(self.train_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_ids': [None],\n",
    "                'attention_mask': [None],\n",
    "                'labels': [None]\n",
    "            },\n",
    "            padding_values={\n",
    "                'input_ids': tf.constant(0, dtype=tf.int32),\n",
    "                'attention_mask': tf.constant(0, dtype=tf.int32),\n",
    "                'labels': tf.constant(-100, dtype=tf.int32)\n",
    "            })        \n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_val(self):\n",
    "        dataset = self.load_dataset(self.val_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_ids': [None],\n",
    "                'attention_mask': [None],\n",
    "                'labels': [None]\n",
    "            },\n",
    "            padding_values={\n",
    "                'input_ids': tf.constant(0, dtype=tf.int32),\n",
    "                'attention_mask': tf.constant(0, dtype=tf.int32),\n",
    "                'labels': tf.constant(-100, dtype=tf.int32)\n",
    "            })\n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "# train = DataLoader(args).train\n",
    "\n",
    "# inputs = next(iter(train))\n",
    "# input_values = inputs['input_ids']\n",
    "# labels = inputs['labels']\n",
    "# attention_mask = inputs['attention_mask']\n",
    "# print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLEUMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"BLEU\", **kwargs):\n",
    "        super(BLEUMetric, self).__init__(name=name, **kwargs)\n",
    "        self.bleu = BLEU()\n",
    "        self.accumulator = self.add_weight(name=\"total_bleu\", initializer=\"zeros\")\n",
    "        self.counter = self.add_weight(name=\"counter\", initializer=\"zeros\")\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        bleu_score = self.bleu.corpus_score(hypotheses=y_true, references=y_pred).score\n",
    "        self.accumulator.assign_add(bleu_score)\n",
    "        self.counter.assign_add(1)\n",
    "\n",
    "    def result(self):\n",
    "        return tf.math.divide_no_nan(self.accumulator, self.counter)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.accumulator.assign(0.0)\n",
    "        self.counter.assign(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.tokenizer = T5Tokenizer(\n",
    "            vocab_file=f\"{self.args.main_dir}/tokenizer_text/t5.model\",\n",
    "            eos_token=\"</s>\",\n",
    "            unk_token=\"<unk>\",\n",
    "            pad_token=\"<pad>\")\n",
    "        self.train_dataset = DataLoader(args).train\n",
    "        self.val_dataset = DataLoader(args).val\n",
    "        schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=args.learning_rate,\n",
    "            decay_steps=1,\n",
    "            decay_rate=0.5,\n",
    "            staircase=False\n",
    "        )\n",
    "        self.optimizer = tf.keras.optimizers.Adam(args.learning_rate)\n",
    "        self.bleu_metric = BLEUMetric()\n",
    "        self.gradient_accumulator = GradientAccumulator()\n",
    "        self.gradient_accumulator.accum_steps = args.accum_steps\n",
    "        self.model = TFT5ForConditionalGeneration.from_pretrained(\n",
    "            \"t5-base\",\n",
    "            output_hidden_states=False,\n",
    "            output_attentions=False,\n",
    "            use_cache=True)\n",
    "\n",
    "        self.model_name = f\"model_{int(self.args.n_samples/1000)}k\"\n",
    "        self.log_path = f\"{self.args.main_dir}/bart_model_weights\\{self.model_name}.csv\"\n",
    "        if not os.path.exists(self.log_path):\n",
    "            print(\"Log file created.\")\n",
    "            columns = \"epoch,loss,bleu,val_loss,val_bleu\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(columns)\n",
    "\n",
    "    def decoder(self, labels, logits):\n",
    "        labels = tf.where(labels < 0, x=0, y=labels)\n",
    "        labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)        \n",
    "        logits = tf.argmax(logits, axis=-1)\n",
    "        logits = ['.' if x == '' else x for x in self.tokenizer.batch_decode(logits, skip_special_tokens=True)]\n",
    "        return labels, logits\n",
    "\n",
    "    def display(self, epoch, t_labels, t_logits, v_labels, v_logits):\n",
    "        print(\"-\" * 129)\n",
    "        print(\"Training\")\n",
    "        for y_true, y_pred in zip(t_labels, t_logits):\n",
    "            print(f\"Target:    {y_true}\")\n",
    "            print(f\"Predicted: {y_pred}\") \n",
    "\n",
    "        print(\"\\nValidation\")\n",
    "        for y_true, y_pred in zip(v_labels, v_logits):\n",
    "            print(f\"Target:    {y_true}\")\n",
    "            print(f\"Predicted: {y_pred}\")\n",
    "        print(\"-\" * 129)\n",
    "        \n",
    "    def fit(self):\n",
    "        # Checkpointing\n",
    "        self.ckpt_dir = f\"{self.args.main_dir}/bart_checkpoints_{int(self.args.n_samples/1000)}k\"\n",
    "        self.ckpt = tf.train.Checkpoint(self.model)\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(\n",
    "            checkpoint=self.ckpt, directory=self.ckpt_dir, max_to_keep=5)\n",
    "\n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            self.start_epoch = int(self.ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "            self.ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "            print(f\"Resuming from epoch {self.start_epoch + 1}...\")\n",
    "        else:\n",
    "            self.start_epoch = 0\n",
    "            print(\"Starting from epoch 1...\")\n",
    "\n",
    "        for epoch in range(self.args.epochs):\n",
    "            print(f\"Epoch {epoch+1}/{self.args.epochs}\")\n",
    "            # print(f\"Epoch {epoch+1}/{self.args.epochs}: Learning rate @ {self.optimizer.lr(epoch):.2e}\")\n",
    "            stateful_metrics = [\"loss\", \"bleu\", \"val_loss\", \"val_bleu\"]\n",
    "            progbar = tf.keras.utils.Progbar(\n",
    "                self.args.train_steps, interval=0.05,\n",
    "                stateful_metrics=stateful_metrics)\n",
    "\n",
    "            # Training loop\n",
    "            for step, t_batch in enumerate(self.train_dataset):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    t_loss, t_logits = self.model(\n",
    "                        input_ids=t_batch['input_ids'],\n",
    "                        attention_mask=t_batch['attention_mask'],\n",
    "                        labels=t_batch['labels'],\n",
    "                        training=True)[:2]              \n",
    "                self.gradient_accumulator(tape.gradient(t_loss, self.model.trainable_weights))\n",
    "                self.optimizer.apply_gradients(zip(\n",
    "                    self.gradient_accumulator.gradients, \n",
    "                    self.model.trainable_weights))\n",
    "                t_labels, t_logits = self.decoder(t_batch['labels'], t_logits)\n",
    "                self.bleu_metric.update_state(t_labels, t_logits)\n",
    "                t_bleu = self.bleu_metric.result()\n",
    "                t_values = [\n",
    "                    (\"loss\", tf.reduce_mean(t_loss)),\n",
    "                    (\"bleu\", t_bleu)]\n",
    "                progbar.update(step, values=t_values, finalize=False)\n",
    "            self.bleu_metric.reset_states()\n",
    "\n",
    "            # Validation loop\n",
    "            for v_batch in self.val_dataset:\n",
    "                v_loss, v_logits = self.model(\n",
    "                    input_ids=v_batch['input_ids'],\n",
    "                    attention_mask=v_batch['attention_mask'],\n",
    "                    labels=v_batch['labels'],\n",
    "                    training=False)[:2]\n",
    "                v_labels, v_logits = self.decoder(v_batch['labels'], v_logits)\n",
    "                self.bleu_metric.update_state(v_labels, v_logits)\n",
    "            \n",
    "            v_bleu = self.bleu_metric.result()\n",
    "            v_values = [\n",
    "                (\"loss\", tf.reduce_mean(t_loss)), \n",
    "                (\"bleu\", t_bleu),\n",
    "                (\"val_loss\", tf.reduce_mean(v_loss)),\n",
    "                (\"val_bleu\", v_bleu)]\n",
    "            progbar.update(self.args.train_steps, values=v_values, finalize=True)\n",
    "            self.bleu_metric.reset_states()\n",
    "\n",
    "            # Print sample transcriptions for both loops\n",
    "            self.display(epoch, t_labels, t_logits, v_labels, v_logits)\n",
    "\n",
    "            # Checkpointing\n",
    "            self.ckpt.save(file_prefix=f\"{self.ckpt_dir}/{self.model_name}\")\n",
    "\n",
    "            # Logging\n",
    "            log = f\"{epoch+1},{t_loss},{t_bleu},{v_loss},{v_bleu}\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(log)\n",
    "\n",
    "            save_path = f\"{self.args.main_dir}/bart_model_weights\"\n",
    "            self.model.save_weights(f\"{save_path}/{self.model_name}_{epoch+1}.h5\")\n",
    "\n",
    "Trainer(args).fit()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b851d2923cfa3a2562599062e05fd9893d86a7c009c64d8ad3756552e4dd5f41"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('tf-gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
