{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "import tensorflow as tf\r\n",
    "import glob"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Input Layer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "class TokenEmbedding(tf.keras.layers.Layer):\r\n",
    "    def __init__(self, num_vocab=1000, maxlen=100, num_hidden=64):\r\n",
    "        super().__init__()\r\n",
    "        self.embedding = tf.keras.layers.Embedding(\r\n",
    "            input_dim=num_vocab, output_dim=num_hidden)\r\n",
    "        self.pos_embedding = tf.keras.layers.Embedding(\r\n",
    "            input_dim=maxlen, output_dim=num_hidden)\r\n",
    "        \r\n",
    "    def call(self, x):\r\n",
    "        x = self.embedding(x)\r\n",
    "        pos = self.pos_embedding(\r\n",
    "            tf.range(start=0, limit=tf.shape(x)[-1], delta=1))\r\n",
    "        return x + pos\r\n",
    "\r\n",
    "class SpeechFeatureEmbedding(tf.keras.layers.Layer):\r\n",
    "    def __init__(self, maxlen=100, num_hidden=64):\r\n",
    "        super().__init__()\r\n",
    "        self.conv1d = tf.keras.layers.Conv1D(\r\n",
    "            filters=num_hidden, kernel_size=11, strides=2, padding=\"same\",\r\n",
    "            activation=\"relu\")\r\n",
    "        self.pos_embedding = tf.keras.layers.Embedding(\r\n",
    "            input_dim=maxlen, output_dim=num_hidden)\r\n",
    "\r\n",
    "    def call(self, x):\r\n",
    "        print(\"before:\", x)\r\n",
    "        x = self.pos_embedding(x)\r\n",
    "        x = self.conv1d(x)\r\n",
    "        x = self.conv1d(x)\r\n",
    "        x = self.conv1d(x)\r\n",
    "        print(\"after:\", x)\r\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Encoder Layer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "class Encoder(tf.keras.layers.Layer):\r\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\r\n",
    "        super().__init__()\r\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(\r\n",
    "            num_heads=num_heads, key_dim=embed_dim)\r\n",
    "        self.ffn = tf.keras.Sequential([\r\n",
    "            tf.keras.layers.Dense(feed_forward_dim, activation=\"relu\"),\r\n",
    "            tf.keras.layers.Dense(embed_dim)])\r\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\r\n",
    "\r\n",
    "    def call(self, inputs, training):\r\n",
    "        # Attention\r\n",
    "        x1 = self.attention(query=inputs, value=inputs)\r\n",
    "        x1 = self.dropout(x1, training=training)\r\n",
    "        x1 = self.layer_norm(inputs + x1)\r\n",
    "        \r\n",
    "        # Feed-forward network\r\n",
    "        x2 = self.ffn(x1)\r\n",
    "        x2 = self.dropout(x2, training=training)\r\n",
    "        x2 = self.layer_norm(x1 + x2)  \r\n",
    "        return x2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decoder Layer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "class Decoder(tf.keras.layers.Layer):\r\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\r\n",
    "        super().__init__()\r\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(\r\n",
    "            num_heads=num_heads, key_dim=embed_dim)\r\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n",
    "        self.self_dropout = tf.keras.layers.Dropout(0.5)\r\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\r\n",
    "        self.fnn = tf.keras.Sequential([\r\n",
    "            tf.keras.layers.Dense(feed_forward_dim, activation=\"relu\"),\r\n",
    "            tf.keras.layers.Dense(embed_dim)])\r\n",
    "\r\n",
    "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\r\n",
    "        i = tf.range(n_dest)[:, None]\r\n",
    "        j = tf.range(n_src)\r\n",
    "        m = i >= j - n_src + n_dest\r\n",
    "        mask = tf.cast(m, dtype)\r\n",
    "        mask = tf.reshape(mask, [1, n_dest, n_src])\r\n",
    "        multiples = tf.concat(\r\n",
    "            values=[\r\n",
    "                tf.expand_dims(batch_size, -1), \r\n",
    "                tf.constant([1, 1], dtype=tf.int32)],\r\n",
    "            axis=0)\r\n",
    "        return tf.tile(mask, multiples)\r\n",
    "\r\n",
    "    def call(self, encoder_out, target):\r\n",
    "        batch_size = tf.shape(target)[0]\r\n",
    "        seq_len = tf.shape(target)[1]\r\n",
    "        causal_mask = self.causal_attention_mask(\r\n",
    "            batch_size, seq_len, seq_len, tf.bool)\r\n",
    "        \r\n",
    "        # Target flow\r\n",
    "        x1 = self.attention(\r\n",
    "            query=target, value=target, attention_mask=causal_mask)\r\n",
    "        x1 = self.dropout(x1)\r\n",
    "        x1 = self.layer_norm(target + x1)\r\n",
    "\r\n",
    "        # Input flow\r\n",
    "        x2 = self.attention(query=x1, value=encoder_out)\r\n",
    "        x2 = self.dropout(x2)\r\n",
    "        x2 = self.layer_norm(x1 + x2)\r\n",
    "\r\n",
    "        # Feed-forward network\r\n",
    "        x3 = self.ffn(x2)\r\n",
    "        x3 = self.layer_norm(x2 + x3)\r\n",
    "        return x3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "class Transformer(tf.keras.Model):\r\n",
    "    def __init__(\r\n",
    "        self, num_hidden=64, num_heads=2, num_feed_forward=128,\r\n",
    "        source_maxlen=100, target_maxlen=100, num_layers_enc=4,\r\n",
    "        num_layers_dec=1, num_classes=10):\r\n",
    "\r\n",
    "        super().__init__()\r\n",
    "        self.num_classes = num_classes\r\n",
    "        self.target_maxlen = target_maxlen\r\n",
    "        self.num_layers_enc = num_layers_enc\r\n",
    "        self.num_layers_dec = num_layers_dec\r\n",
    "        self.loss_metric = tf.keras.metrics.Mean(name=\"loss\")\r\n",
    "        self.encoder_input = SpeechFeatureEmbedding(\r\n",
    "            num_hidden=num_hidden, maxlen=source_maxlen)\r\n",
    "        \r\n",
    "        self.encoder_layer = Encoder(\r\n",
    "            embed_dim=num_hidden, num_heads=num_heads, \r\n",
    "            feed_forward_dim=num_feed_forward)\r\n",
    "        \r\n",
    "        self.decoder_input = TokenEmbedding(\r\n",
    "            num_vocab=num_classes, maxlen=target_maxlen, num_hidden=num_hidden)\r\n",
    "        \r\n",
    "        self.decoder_layer = Decoder(\r\n",
    "            embed_dim=num_hidden, num_heads=num_heads, \r\n",
    "            feed_forward_dim=num_feed_forward)\r\n",
    "        \r\n",
    "        self.classifier = tf.keras.layers.Dense(num_classes)\r\n",
    "   \r\n",
    "    def call(self, inputs):\r\n",
    "        x = self.encoder_input(inputs[0])\r\n",
    "        for i in range(self.num_layers_enc):\r\n",
    "            x = self.encoder_layer(x)\r\n",
    "        y = self.decoder_input(inputs[1])\r\n",
    "        for i in range(self.num_layers_dec):\r\n",
    "            x = self.decoder_layer(x, y)\r\n",
    "        x = self.classifier(x)\r\n",
    "        return x\r\n",
    "\r\n",
    "    @property\r\n",
    "    def metrics(self):\r\n",
    "        return [self.loss_metric]\r\n",
    "\r\n",
    "    def train_step(self, batch):\r\n",
    "        source = batch[\"source\"]\r\n",
    "        target = batch[\"target\"]\r\n",
    "        decoder_input = target[:, :-1]\r\n",
    "        decoder_target = target[:, 1:]\r\n",
    "        with tf.GradientTape() as tape:\r\n",
    "            preds = self([source, decoder_input])\r\n",
    "            one_hot = tf.one_hot(decoder_target, depth=self.num_classes)\r\n",
    "            mask = tf.math.logical_not(tf.math.equal(decoder_target, 0))\r\n",
    "            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\r\n",
    "            self.loss_metric.update_state(loss)\r\n",
    "            return {\"loss\": self.loss_metric.result()}\r\n",
    "\r\n",
    "    def test_step(self, batch):\r\n",
    "        source = batch[\"source\"]\r\n",
    "        target = batch[\"target\"]\r\n",
    "        decoder_input = target[:, :-1]\r\n",
    "        decoder_target = target[:, 1:]\r\n",
    "        preds = self([source, decoder_input])\r\n",
    "        one_hot = tf.one_hot(decoder_target, depth=self.num_classes)\r\n",
    "        mask = tf.math.logical_not(tf.math.equal(decoder_target, 0))\r\n",
    "        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\r\n",
    "        self.loss_metric.update_state(loss)\r\n",
    "        return {\"loss\": self.loss_metric.result()}\r\n",
    "\r\n",
    "    def infer(self, source, target_start_token_idx):\r\n",
    "        batch_size = tf.shape(source)[0]\r\n",
    "        encoder = self.encoder(source)\r\n",
    "        decoder_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\r\n",
    "        decoder_logits = []\r\n",
    "        for i in range(self.target_maxlen - 1):\r\n",
    "            decoder_out = self.decoder(encoder, decoder_input)\r\n",
    "            logits = self.classifier(decoder_out)\r\n",
    "            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\r\n",
    "            last_logit = tf.expand_dims(logits[:, -1], axis=-1)\r\n",
    "            decoder_logits.append(last_logit)\r\n",
    "            decoder_input = tf.concat([decoder_input, last_logit], axis=-1)\r\n",
    "        return decoder_input"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "def get_data(maxlen=50):\r\n",
    "    data = []\r\n",
    "    wavs = glob.glob(\"Datasets/LJSpeech-dataset/wavs/*.wav\")\r\n",
    "    with open(\"Datasets\\LJSpeech-dataset\\metadata.csv\", encoding=\"utf-8\") as f:\r\n",
    "        id_to_text = {\r\n",
    "            line.strip().split(\"|\")[0]: line.strip().split(\"|\")[2] for line in f}\r\n",
    "    for w in wavs:\r\n",
    "        id = w.split(\"\\\\\")[-1].split(\".\")[0]\r\n",
    "        if len(id_to_text[id]) < maxlen:\r\n",
    "            data.append({\"audio\": w, \"text\": id_to_text[id]})\r\n",
    "    return data\r\n",
    "\r\n",
    "class Vectorizer:\r\n",
    "    def __init__(self, maxlen=50):\r\n",
    "        self.vocab = (\r\n",
    "            [\"-\", \"#\", \"<\", \">\"]\r\n",
    "            + [chr(i + 96) for i in range(1, 27)]\r\n",
    "            + [\" \", \".\", \",\", \"?\"])\r\n",
    "        self.maxlen = maxlen\r\n",
    "        self.char_to_idx = {}\r\n",
    "        for i, ch in enumerate(self.vocab):\r\n",
    "            self.char_to_idx[ch] = i\r\n",
    "        \r\n",
    "    def __call__(self, text):\r\n",
    "        text = text.lower()\r\n",
    "        text = text[: self.maxlen - 2]\r\n",
    "        text = \"<\" + text + \">\"\r\n",
    "        pad_len = self.maxlen - len(text)\r\n",
    "        return [self.char_to_idx.get(ch, 1) for ch in text] + [0] * pad_len\r\n",
    "\r\n",
    "    def get_vocab(self):\r\n",
    "        return self.vocab\r\n",
    "\r\n",
    "max_target_len = 200\r\n",
    "data = get_data()\r\n",
    "vectorizer = Vectorizer(max_target_len)\r\n",
    "print(\"Vocab size:\", len(vectorizer.get_vocab()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocab size: 34\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "def create_text_ds(data):\r\n",
    "    texts = [i[\"text\"] for i in data]\r\n",
    "    text_ds = [vectorizer(t) for t in texts]\r\n",
    "    text_ds = tf.data.Dataset.from_tensor_slices(text_ds)\r\n",
    "    return text_ds\r\n",
    "\r\n",
    "def path_to_audio(path):\r\n",
    "    audio = tf.io.read_file(path)\r\n",
    "    audio = tf.audio.decode_wav(audio, 1)[0]\r\n",
    "    audio = tf.squeeze(audio, axis=-1)\r\n",
    "    stft = tf.signal.stft(audio, frame_length=200, frame_step=80, fft_length=256)\r\n",
    "    x = tf.math.pow(tf.abs(stft), 0.5)\r\n",
    "    mean = tf.math.reduce_mean(x, 1, keepdims=True)\r\n",
    "    std = tf.math.reduce_std(x, 1, keepdims=True)\r\n",
    "    x = (x - mean) / std\r\n",
    "    audio_len = tf.shape(x)[0]\r\n",
    "    pad_len = 2754\r\n",
    "    paddings = tf.constant([[0, pad_len], [0, 0]])\r\n",
    "    x = tf.pad(x, paddings, \"CONSTANT\")[:pad_len, :]\r\n",
    "    return x\r\n",
    "\r\n",
    "def create_audio_ds(data):\r\n",
    "    flist = [i[\"audio\"] for i in data]\r\n",
    "    audio_ds = tf.data.Dataset.from_tensor_slices(flist)\r\n",
    "    audio_ds = audio_ds.map(\r\n",
    "        path_to_audio, num_parallel_calls=tf.data.AUTOTUNE)\r\n",
    "    return audio_ds\r\n",
    "\r\n",
    "def create_tf_dataset(data, batch_size=4):\r\n",
    "    audio_ds = create_audio_ds(data)\r\n",
    "    text_ds = create_text_ds(data)\r\n",
    "    ds = tf.data.Dataset.zip((audio_ds, text_ds))\r\n",
    "    ds = ds.map(lambda x, y: {\"source\": x, \"target\": y})\r\n",
    "    ds = ds.batch(batch_size)\r\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\r\n",
    "    return ds\r\n",
    "\r\n",
    "split = int(len(data) * 0.8)\r\n",
    "train_data = data[:split]\r\n",
    "val_data = data[split:]\r\n",
    "ds = create_tf_dataset(train_data, batch_size=64)\r\n",
    "val_ds = create_tf_dataset(val_data, batch_size=64)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "class DisplayOutputs(tf.keras.callbacks.Callback):\r\n",
    "    def __init__(self, batch, idx_to_token, target_start_token_idx=27,\r\n",
    "        target_end_token_idx=28):\r\n",
    "        self.batch = batch\r\n",
    "        self.target_start_token_idx = target_start_token_idx\r\n",
    "        self.target_end_token_idx = target_end_token_idx\r\n",
    "        self.idx_to_char = idx_to_token\r\n",
    "    \r\n",
    "    def on_epoch_end(self, epoch, logs=None):\r\n",
    "        if epoch % 5 != 0:\r\n",
    "            return\r\n",
    "        source = self.batch[\"source\"]\r\n",
    "        target = self.batch[\"target\"].numpy()\r\n",
    "        batch_size = tf.shape(source)[0]\r\n",
    "        preds = self.model.generate(source, self.target_start_token_idx).numpy()\r\n",
    "        for i in range(batch_size):\r\n",
    "            target_text = \"\".join([self.idx_to_char[j] for j in target[i, :]])\r\n",
    "            prediction = \"\"\r\n",
    "            for idx in preds[i, :]:\r\n",
    "                prediction += self.idx_to_char[idx]\r\n",
    "                if idx == self.target_end_token_idx:\r\n",
    "                    break\r\n",
    "            print(f\"target:     {target_text.replace('-', '')}\")\r\n",
    "            print(f\"prediction: {prediction}\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "# class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "batch = next(iter(val_ds))\r\n",
    "\r\n",
    "idx_to_char = vectorizer.get_vocab()\r\n",
    "display_cb = DisplayOutputs(\r\n",
    "    batch, idx_to_char, target_start_token_idx=2, target_end_token_idx=3)\r\n",
    "\r\n",
    "model = Transformer(\r\n",
    "    num_hidden=200,\r\n",
    "    num_heads=2,\r\n",
    "    num_feed_forward=400,\r\n",
    "    target_maxlen=max_target_len,\r\n",
    "    num_layers_enc=4,\r\n",
    "    num_layers_dec=1,\r\n",
    "    num_classes=34)\r\n",
    "\r\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(\r\n",
    "    from_logits=True, label_smoothing=0.1)\r\n",
    "\r\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\r\n",
    "model.compile(optimizer=optimizer, loss=loss_fn)\r\n",
    "\r\n",
    "history = model.fit(ds, validation_data=val_ds, callbacks=[display_cb], epochs=1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "before: Tensor(\"IteratorGetNext:0\", shape=(None, None, 129), dtype=float32)\n",
      "after: Tensor(\"transformer_8/speech_feature_embedding_8/conv1d_8/Relu_2:0\", shape=(None, None, 17, 200), dtype=float32)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "in user code:\n\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py:853 train_function  *\n        return step_function(self, iterator)\n    <ipython-input-86-d73e3bbee55a>:35 call  *\n        x = self.decoder_layer(x, y)\n    <ipython-input-25-c3c03673c8e4>:39 call  *\n        x2 = self.attention(query=x1, value=encoder_out)\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\base_layer.py:1037 __call__  **\n        outputs = call_fn(inputs, *args, **kwargs)\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\layers\\multi_head_attention.py:498 call\n        key = self._key_dense(key)\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\base_layer.py:1037 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\layers\\einsum_dense.py:197 call\n        ret = tf.einsum(self.equation, inputs, self.kernel)\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\special_math_ops.py:751 einsum\n        return _einsum_v2(equation, *inputs, **kwargs)\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\special_math_ops.py:1180 _einsum_v2\n        return gen_linalg_ops.einsum(inputs, resolved_equation)\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_linalg_ops.py:1090 einsum\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:748 _apply_op_helper\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:599 _create_op_internal\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3561 _create_op_internal\n        ret = Operation(\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2041 __init__\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1883 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Shape must be rank 3 but is rank 4\n    \t for 0th input and equation: abc,cde->abde for '{{node transformer_8/decoder_7/multi_head_attention_15/key/einsum_1/Einsum}} = Einsum[N=2, T=DT_FLOAT, equation=\"abc,cde->abde\"](transformer_8/encoder_7/layer_normalization_14/batchnorm_7/add_1, transformer_8/decoder_7/multi_head_attention_15/key/einsum_1/Einsum/ReadVariableOp)' with input shapes: [?,?,17,200], [200,2,200].\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-cbdafa76956e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdisplay_cb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    757\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 759\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    760\u001b[0m             *args, **kwds))\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3065\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3066\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3067\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3463\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3296\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3297\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3298\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3299\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3300\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1007\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1008\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    669\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    992\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    993\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 994\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    995\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py:853 train_function  *\n        return step_function(self, iterator)\n    <ipython-input-86-d73e3bbee55a>:35 call  *\n        x = self.decoder_layer(x, y)\n    <ipython-input-25-c3c03673c8e4>:39 call  *\n        x2 = self.attention(query=x1, value=encoder_out)\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\base_layer.py:1037 __call__  **\n        outputs = call_fn(inputs, *args, **kwargs)\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\layers\\multi_head_attention.py:498 call\n        key = self._key_dense(key)\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\base_layer.py:1037 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\layers\\einsum_dense.py:197 call\n        ret = tf.einsum(self.equation, inputs, self.kernel)\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\special_math_ops.py:751 einsum\n        return _einsum_v2(equation, *inputs, **kwargs)\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\special_math_ops.py:1180 _einsum_v2\n        return gen_linalg_ops.einsum(inputs, resolved_equation)\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_linalg_ops.py:1090 einsum\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:748 _apply_op_helper\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:599 _create_op_internal\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3561 _create_op_internal\n        ret = Operation(\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2041 __init__\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\n    E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1883 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Shape must be rank 3 but is rank 4\n    \t for 0th input and equation: abc,cde->abde for '{{node transformer_8/decoder_7/multi_head_attention_15/key/einsum_1/Einsum}} = Einsum[N=2, T=DT_FLOAT, equation=\"abc,cde->abde\"](transformer_8/encoder_7/layer_normalization_14/batchnorm_7/add_1, transformer_8/decoder_7/multi_head_attention_15/key/einsum_1/Einsum/ReadVariableOp)' with input shapes: [?,?,17,200], [200,2,200].\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('tf-gpu': conda)"
  },
  "interpreter": {
   "hash": "b851d2923cfa3a2562599062e05fd9893d86a7c009c64d8ad3756552e4dd5f41"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}