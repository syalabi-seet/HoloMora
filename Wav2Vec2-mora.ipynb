{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from convert_romaji import Romaji2Kana\n",
    "\n",
    "import MeCab\n",
    "import cutlet\n",
    "import jiwer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    TFWav2Vec2ForCTC,\n",
    "    Wav2Vec2Config,\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    logging)\n",
    "\n",
    "from convert_romaji import Romaji2Kana\n",
    "\n",
    "def seed_everything(SEED):\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    print(\"Random seed set.\")\n",
    "\n",
    "seed_everything(42)\n",
    "tf.get_logger().setLevel('FATAL')\n",
    "logging.set_verbosity_error()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=4, buffer_size=1024, epochs=15, learning_rate=5e-05, lr_max=5e-05, lr_min=1e-08, lr_start=1e-08, main_dir='E://Datasets/Acoustic_model', model_name='facebook/wav2vec2-base', n_cycles=0.5, n_samples=40000, n_shards=20, n_train=36000, n_val=4000, random_state=42, sample_rate=16000, sustain_epochs=0, test_size=0.1, train_steps=9000, val_steps=1000, vocab_size=38, warmup_epochs=1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ArgParser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # DataLoader\n",
    "    parser.add_argument(\"--main_dir\", default=\"E://Datasets/Acoustic_model\")\n",
    "    parser.add_argument(\"--sample_rate\", default=16000)\n",
    "    parser.add_argument(\"--test_size\", default=0.1)\n",
    "    parser.add_argument(\"--random_state\", default=42)\n",
    "    parser.add_argument(\"--batch_size\", default=4)\n",
    "    parser.add_argument(\"--n_shards\", default=20)\n",
    "    parser.add_argument(\"--buffer_size\", default=1024)\n",
    "    parser.add_argument(\"--n_samples\", default=40000)\n",
    "\n",
    "    # Trainer\n",
    "    parser.add_argument(\"--model_name\", default=\"facebook/wav2vec2-base\")\n",
    "    parser.add_argument(\"--epochs\", default=15)\n",
    "\n",
    "    # Scheduler\n",
    "    parser.add_argument(\"--learning_rate\", default=5e-5)\n",
    "    parser.add_argument(\"--lr_start\", default=1e-8)\n",
    "    parser.add_argument(\"--lr_min\", default=1e-8)\n",
    "    parser.add_argument(\"--lr_max\", default=5e-5)\n",
    "    parser.add_argument(\"--n_cycles\", default=0.5)\n",
    "    parser.add_argument(\"--warmup_epochs\", default=1)\n",
    "    parser.add_argument(\"--sustain_epochs\", default=0)\n",
    "\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    with open(f\"{args.main_dir}/vocab.json\", \"r\") as f:\n",
    "        vocab_size = len(json.load(f))\n",
    "   \n",
    "    n_train = int(args.n_samples * (1 - args.test_size))\n",
    "    n_val = int(args.n_samples * args.test_size)\n",
    "    train_steps = int(np.ceil(n_train / args.batch_size))\n",
    "    val_steps = int(np.ceil(n_val / args.batch_size))\n",
    "\n",
    "    parser.add_argument(\"--vocab_size\", default=vocab_size)\n",
    "    parser.add_argument(\"--n_train\", default=n_train)\n",
    "    parser.add_argument(\"--n_val\", default=n_val)\n",
    "    parser.add_argument(\"--train_steps\", default=train_steps)  \n",
    "    parser.add_argument(\"--val_steps\", default=val_steps)\n",
    "    \n",
    "    return parser.parse_known_args()[0]\n",
    "\n",
    "args = ArgParser()\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.data = pd.concat([\n",
    "            self.get_kokoro(),\n",
    "            self.get_jsut(),\n",
    "            self.get_commonvoice()], \n",
    "            ignore_index=True)\n",
    "        self.kanji_unicode = self.get_kanji_unicode()\n",
    "        self.katsu = cutlet.Cutlet()\n",
    "        self.katsu.use_foreign_spelling = False\n",
    "    \n",
    "        tqdm.pandas()\n",
    "        # Remove rows that contains non-kanji characters\n",
    "        self.data = self.data[self.data['sentence'].progress_apply(self.check_kanji)] \n",
    "\n",
    "        # Remove words within parenthesis\n",
    "        parenthesis =  r\"\\（.*\\）|\\(.*\\)|\\「.*\\」|\\『.*\\』\"\n",
    "        self.data = self.data[~self.data['sentence'].str.contains(parenthesis)]\n",
    "\n",
    "        # Remove punctuations from sentences\n",
    "        self.data['sentence'] = self.data['sentence'].progress_apply(self.clean_kanji)\n",
    "\n",
    "        self.data['romaji'] = self.data['sentence'].progress_apply(self.kanji2romaji)\n",
    "        self.data['length'] = self.data['path'].progress_apply(self.get_length)\n",
    "        self.data = self.data.query(f\"length >= 50000 & length <= 90000\")\n",
    "        self.data = self.data.reset_index(drop=True)\n",
    "        self.data = self.data.sample(n=self.args.n_samples, random_state=42, ignore_index=True)\n",
    "        self.data.sort_values(by=\"length\", axis=0, ascending=True, inplace=True, ignore_index=True)\n",
    "        self.data.to_csv(\n",
    "            f\"{self.args.main_dir}/ASRDataset.csv\", \n",
    "            encoding=\"utf-8\", index=False)\n",
    "\n",
    "    def get_kokoro(self):\n",
    "        in_dir = \"Datasets\\KOKORO-dataset\"\n",
    "\n",
    "        data = []\n",
    "        transcript_path = f\"{in_dir}/transcripts/*.metadata.txt\"\n",
    "        for transcript in glob.glob(transcript_path):\n",
    "            with open(transcript, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f.readlines():\n",
    "                    data.append(line.split(\"|\"))\n",
    "\n",
    "        data = pd.DataFrame(\n",
    "            data, columns=[\n",
    "                'text_id', 'path', 'start_idx', \n",
    "                'end_idx', 'sentence', 'phonemes'])       \n",
    "\n",
    "        # paths = data['path'].unique()\n",
    "        # for path in tqdm(paths, total=len(paths)):\n",
    "        #     folder_name = path.split(\"_\", 1)[0]\n",
    "        #     in_path = os.path.join(in_dir, folder_name, path)\n",
    "        #     y, sr = librosa.load(in_path, sr=None)\n",
    "        #     for text_id in data.loc[data['path']==path, 'text_id']:\n",
    "        #         out_path = os.path.join(self.args.main_dir, 'wav_cleaned', text_id) + \".wav\"\n",
    "        #         if not os.path.exists(out_path):\n",
    "        #             start_idx = int(data.loc[data['text_id']==text_id, 'start_idx'].item())\n",
    "        #             end_idx = int(data.loc[data['text_id']==text_id, 'end_idx'].item())\n",
    "        #             y_slice = librosa.resample(\n",
    "        #                 y[start_idx:end_idx], orig_sr=sr, target_sr=self.sample_rate)\n",
    "        #             sf.write(out_path, y_slice, samplerate=self.sample_rate, subtype='PCM_16')\n",
    "\n",
    "        data = data[['text_id', 'sentence']]\n",
    "        data['text_id'] = data['text_id'].apply(lambda x: x + \".wav\")\n",
    "        data.columns = ['path', 'sentence']\n",
    "        data['corpus'] = ['kokoro'] * len(data)\n",
    "        return data\n",
    "\n",
    "    def get_jsut(self):\n",
    "        filenames, sentences = [], []\n",
    "        for transcript in glob.glob(r\"Datasets/JSUT-dataset/*/transcript_utf8.txt\"):\n",
    "            file_path = transcript.rsplit(\"\\\\\", 1)[0]\n",
    "            with open(transcript, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines: \n",
    "                    filename, sentence = line.split(\":\")\n",
    "                    filenames.append(os.path.join(file_path, \"wav\", filename) + \".wav\")\n",
    "                    sentences.append(sentence.strip(\"\\n\"))\n",
    "        data = pd.DataFrame({'path': filenames, 'sentence': sentences}) \n",
    "        data['corpus'] = ['jsut'] * len(data)\n",
    "        for i, in_path in tqdm(enumerate(data['path']), total=len(data['path'])):\n",
    "            in_path = in_path.replace(\"\\\\\", \"/\")\n",
    "            out_path = f\"{self.args.main_dir}\\wav_cleaned\"\n",
    "            filename = in_path.rsplit(\"/\", 1)[-1]\n",
    "            out_path = os.path.join(out_path, filename)\n",
    "            if not os.path.exists(out_path):\n",
    "                subprocess.call([\n",
    "                    \"ffmpeg\", \"-i\", in_path,\"-acodec\", \"pcm_s16le\", \n",
    "                    \"-ar\", str(self.sample_rate), out_path])\n",
    "            data['path'][i] = filename\n",
    "        return data\n",
    "\n",
    "    def get_commonvoice(self):\n",
    "        data = pd.read_csv(r\"Datasets/CommonVoice-dataset/validated.tsv\", sep=\"\\t\")\n",
    "        data = data[['path', 'sentence']]    \n",
    "        data['path'] = data['path'].apply(\n",
    "            lambda x: r\"Datasets/CommonVoice-dataset/mp3/\" + x)\n",
    "        data['corpus'] = ['common_voice'] * len(data)\n",
    "        for i, in_path in tqdm(enumerate(data['path']), total=len(data['path'])):\n",
    "            in_path = in_path.replace(\"\\\\\", \"/\")\n",
    "            out_path = f\"{self.args.main_dir}\\wav_cleaned\"\n",
    "            filename = in_path.rsplit(\"/\", 1)[-1]\n",
    "            filename = filename.replace(\"mp3\", \"wav\")\n",
    "            out_path = os.path.join(out_path, filename)\n",
    "            if not os.path.exists(out_path):\n",
    "                subprocess.call([\n",
    "                    \"ffmpeg\", \"-i\", in_path,\"-acodec\", \"pcm_s16le\", \n",
    "                    \"-ar\", str(self.sample_rate), out_path])\n",
    "            data['path'][i] = filename\n",
    "        return data\n",
    "\n",
    "    def get_kanji_unicode(self):\n",
    "        vocab = set()\n",
    "        with open(\n",
    "            r\"D:\\School-stuff\\Sem-2\\PR-Project\\HoloASR\\Datasets\\kanji_unicode.txt\", \n",
    "            encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                for char in line.split()[1:]:\n",
    "                    vocab.add(char)\n",
    "        return \"|\".join(sorted(vocab))\n",
    "    \n",
    "    def check_kanji(self, sentence):\n",
    "        pattern = f\"[^{self.kanji_unicode}]\"\n",
    "        if re.match(pattern, sentence) == None:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def clean_kanji(self, sentence):\n",
    "        sentence = \"\".join(sentence.split())\n",
    "        pattern = r\"[・\\。\\！\\.\\？\\、]\"\n",
    "        sentence = re.sub(pattern, \"\", sentence)\n",
    "        return sentence\n",
    "\n",
    "    def kanji2romaji(self, sentence):\n",
    "        try:\n",
    "            sentence = self.katsu.romaji(sentence)\n",
    "            sentence = sentence.replace(\" \", \"|\").lower()\n",
    "        except:\n",
    "            sentence = None\n",
    "        return sentence\n",
    "\n",
    "    def get_length(self, path):\n",
    "        path = os.path.join(self.args.main_dir, 'wav_cleaned', path)\n",
    "        y, sr = librosa.load(path, sr=None)\n",
    "        return len(y)\n",
    "\n",
    "# data = Dataset(args).data\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 4))\n",
    "# sns.histplot(x=data['length'], hue=data['corpus'], palette=\"bright\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, args):\n",
    "        tokenizer = Wav2Vec2CTCTokenizer(\n",
    "            vocab_file=f\"{args.main_dir}/vocab.json\",\n",
    "            do_lower_case=False)\n",
    "\n",
    "        feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "            feature_size=1,\n",
    "            sampling_rate=args.sample_rate,\n",
    "            padding_value=0.0,\n",
    "            do_normalize=False,\n",
    "            return_attention_mask=False)\n",
    "\n",
    "        self.processor = Wav2Vec2Processor(\n",
    "            feature_extractor=feature_extractor,\n",
    "            tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFRWriter():\n",
    "    def __init__(self, args):\n",
    "        self.data = pd.read_csv(os.path.join(args.main_dir, \"ASRDataset.csv\"), encoding=\"utf-8\")\n",
    "        self.args = args\n",
    "        self.processor = Config(args).processor\n",
    "\n",
    "    def _bytes_feature(self, value):\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    def _int64_feature(self, value):\n",
    "        \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "    def _float_feature(self, value):\n",
    "        \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "    def serialize_example(self, *args):\n",
    "        feature = {\n",
    "            'input_values': self._bytes_feature(args[0]),\n",
    "            'labels': self._bytes_feature(args[1])}\n",
    "\n",
    "        example_proto = tf.train.Example(\n",
    "            features=tf.train.Features(feature=feature))\n",
    "        return example_proto.SerializeToString()\n",
    "\n",
    "    def get_labels(self, sample):\n",
    "        labels = self.data.loc[self.data['path']==sample, \"romaji\"].item()\n",
    "        labels = (self.processor.tokenizer.bos_token + labels + \n",
    "            self.processor.tokenizer.eos_token)\n",
    "        with self.processor.as_target_processor():\n",
    "            labels = self.processor(labels, is_split_into_words=True).input_ids\n",
    "        return tf.convert_to_tensor(labels, dtype=tf.int32)\n",
    "\n",
    "    def get_audio(self, sample):\n",
    "        path = os.path.join(self.args.main_dir, \"wav_cleaned\", sample)\n",
    "        audio = librosa.load(path, sr=None)[0]\n",
    "        audio /= audio.max()\n",
    "        return tf.convert_to_tensor(audio, dtype=tf.float32)\n",
    "\n",
    "    def get_shards(self):\n",
    "        skf = KFold(n_splits=self.args.n_shards, shuffle=False)\n",
    "        return [\n",
    "            list(map(lambda x: self.data['path'][x], j))\n",
    "            for i, j in skf.split(self.data['path'])]\n",
    "\n",
    "    def get_shard_data(self, samples):\n",
    "        for sample in samples:\n",
    "            audio = self.get_audio(sample)\n",
    "            labels = self.get_labels(sample)\n",
    "            yield {\n",
    "                'input_values': tf.io.serialize_tensor(audio),\n",
    "                'labels': tf.io.serialize_tensor(labels)}\n",
    "\n",
    "    def write(self):\n",
    "        for shard, samples in tqdm(enumerate(self.get_shards()), total=self.args.n_shards):\n",
    "            with tf.io.TFRecordWriter(f\"{self.args.main_dir}/wav2vec2_tfrec/shard_{shard+1}.tfrec\") as f:\n",
    "                for sample in self.get_shard_data(samples):\n",
    "                    example = self.serialize_example(\n",
    "                        sample['input_values'],\n",
    "                        sample['labels'])\n",
    "                    f.write(example)\n",
    "\n",
    "# TFRWriter(args).write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, args):\n",
    "        self.files = glob.glob(args.main_dir + \"/wav2vec2_tfrec/*.tfrec\")\n",
    "        self.args = args\n",
    "        self.AUTOTUNE = tf.data.AUTOTUNE\n",
    "        self.train_files, self.val_files = train_test_split(\n",
    "            self.files, test_size=args.test_size, shuffle=True, \n",
    "            random_state=args.random_state)\n",
    "        self.train = self.get_train()\n",
    "        self.val = self.get_val()\n",
    "\n",
    "    def read_tfrecord(self, example):\n",
    "        feature_description = {\n",
    "            'input_values': tf.io.FixedLenFeature([], tf.string),\n",
    "            'labels': tf.io.FixedLenFeature([], tf.string)}\n",
    "        \n",
    "        example = tf.io.parse_single_example(example, feature_description)\n",
    "        example['input_values'] = tf.io.parse_tensor(\n",
    "            example['input_values'], out_type=tf.float32)\n",
    "        example['labels'] = tf.io.parse_tensor(\n",
    "            example['labels'], out_type=tf.int32)\n",
    "        return example\n",
    "\n",
    "    def load_dataset(self, files):\n",
    "        ignore_order = tf.data.Options()\n",
    "        ignore_order.experimental_deterministic = False\n",
    "        dataset = tf.data.TFRecordDataset(files)\n",
    "        dataset = dataset.with_options(ignore_order)\n",
    "        dataset = dataset.map(self.read_tfrecord, num_parallel_calls=self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_train(self):\n",
    "        dataset = self.load_dataset(self.train_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_values': [None],\n",
    "                'labels': [None]\n",
    "            },\n",
    "            padding_values={\n",
    "                'input_values': tf.constant(0, dtype=tf.float32), \n",
    "                'labels': tf.constant(-100, dtype=tf.int32)\n",
    "            })        \n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_val(self):\n",
    "        dataset = self.load_dataset(self.val_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_values': [None],\n",
    "                'labels': [None]\n",
    "            },\n",
    "            padding_values={\n",
    "                'input_values': tf.constant(0, dtype=tf.float32),\n",
    "                'labels': tf.constant(-100, dtype=tf.int32)\n",
    "            })\n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "# train = DataLoader(args).train\n",
    "# output = next(iter(train))\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16,10))\n",
    "# for i, array in enumerate(train.take(16)):\n",
    "#     plt.subplot(4, 4, i+1)\n",
    "#     y = array['input_values'].numpy()\n",
    "#     librosa.display.waveplot(y=y, sr=16000)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class PERMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, tokenizer, name=\"PER\", **kwargs):\n",
    "        super(PERMetric, self).__init__(name=name,  **kwargs)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.accumulator = self.add_weight(name=\"total_per\", initializer=\"zeros\")\n",
    "        self.counter = self.add_weight(name=\"per_count\", initializer=\"zeros\")    \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Re-encode logits\n",
    "        try:\n",
    "            y_pred = self.tokenizer(y_pred, is_split_into_words=False).input_ids\n",
    "        except:\n",
    "            y_pred = [[self.tokenizer.pad_token_id] for _ in range(len(y_pred))]\n",
    "\n",
    "        hypothesis = tf.ragged.constant(y_pred).to_sparse()\n",
    "\n",
    "        # Convert dense to sparse tensor for edit_distance function\n",
    "        truth = tf.RaggedTensor.from_tensor(y_true, padding=0).to_sparse()\n",
    "\n",
    "        # Calculate Levenshtein distance\n",
    "        distance = tf.edit_distance(hypothesis, truth, normalize=True)\n",
    "\n",
    "        # Add distance and number of samples to variables\n",
    "        self.accumulator.assign_add(tf.reduce_mean(distance))\n",
    "        self.counter.assign_add(len(y_true))\n",
    "\n",
    "    def result(self):\n",
    "        # Divides accumulated distance scores against number of samples passed,\n",
    "        # mimics mean reduction over batch\n",
    "        return tf.math.divide_no_nan(self.accumulator, self.counter)   \n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.accumulator.assign(0.0)\n",
    "        self.counter.assign(0.0)\n",
    "\n",
    "class WERMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"WER\", **kwargs):\n",
    "        super(WERMetric, self).__init__(name=name,  **kwargs)\n",
    "        self.tagger = MeCab.Tagger(\"-Owakati\")\n",
    "        self.accumulator = self.add_weight(name=\"total_wer\", initializer=\"zeros\")\n",
    "        self.counter = self.add_weight(name=\"wer_count\", initializer=\"zeros\")    \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Calculate wer score\n",
    "        wer = jiwer.wer(y_true, y_pred)\n",
    "\n",
    "        # Add distance and number of batches to variables\n",
    "        self.accumulator.assign_add(wer)\n",
    "        self.counter.assign_add(1)\n",
    "\n",
    "    def result(self):\n",
    "        # Divides accumulated distance scores against number of batches passed,\n",
    "        # mimics mean reduction over batch\n",
    "        return tf.math.divide_no_nan(self.accumulator, self.counter)   \n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.accumulator.assign(0.0)\n",
    "        self.counter.assign(0.0)\n",
    "\n",
    "class CosineDecayWithWarmup(LearningRateSchedule):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "    def __call__(self, epoch):  \n",
    "        if epoch < self.args.warmup_epochs:\n",
    "            lr = ((self.args.lr_max - self.args.lr_start) / self.args.warmup_epochs) * epoch + self.args.lr_start\n",
    "        elif epoch < (self.args.warmup_epochs + self.args.sustain_epochs):\n",
    "            lr = self.args.lr_max\n",
    "        else:\n",
    "            progress = ((epoch - self.args.warmup_epochs - self.args.sustain_epochs) / \n",
    "            (self.args.epochs - self.args.warmup_epochs - self.args.sustain_epochs))\n",
    "            lr = (self.args.lr_max-self.args.lr_min) * (0.5 * (1.0 + tf.math.cos((22/7) * \n",
    "                self.args.n_cycles * 2.0 * progress)))\n",
    "            if self.args.lr_min is not None:\n",
    "                lr = tf.math.maximum(self.args.lr_min, lr)\n",
    "        return lr\n",
    "\n",
    "    def plot(self):\n",
    "        epochs = range(self.args.epochs+1)\n",
    "        lr = [self(epoch) for epoch in epochs]\n",
    "        plt.plot(epochs, lr)\n",
    "        plt.xlabel(\"learning_rate\")\n",
    "        plt.ylabel(\"epochs\")\n",
    "        plt.show()\n",
    "\n",
    "CosineDecayWithWarmup(args).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\transformers\\configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from epoch 1...\n",
      "Epoch 1/15: Learning rate @ 1.00e-08\n",
      "9000/9000 [==============================] - 15773s 2s/step - loss: 23.4724 - per: 0.0548 - wer: 0.5600 - val_loss: 21.1262 - val_per: 0.0340 - val_wer: 0.3468\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Training\n",
      "Target:    maiban ni jikan eigo no benkyou wo shimasu\n",
      "Predicted: raiban nijikan eigo no denkyou shimasu\n",
      "Target:    tokai de wa deau hito no hotondo ga mishiranu hito de aru\n",
      "Predicted: tokai de wa deau hito no hotondo ga mishira no hito de aru\n",
      "Target:    kare wa soko ni tatte yorikakari teishutsu shite mattaku shitsumon shimasendeshita\n",
      "Predicted: kareba sapo ni tate yorigagari teitsu shite mataku shtsu mon shnzendeshita\n",
      "Target:    futotta dake na no ni kinniku wo fuyashita to iiharu\n",
      "Predicted: futota dakenana ni kinigu wo fuyashita to iharu\n",
      "\n",
      "Validation\n",
      "Target:    ya hinauta wo kuchiguse ni kyoujou de utatte\n",
      "Predicted: yahi na uta wo kuchiguse ni kyoujou de utate\n",
      "Target:    juu sanya no tsuki wa zutto hikuu nattaga\n",
      "Predicted: jusan ya no tsuki wa zuto hiku nataga\n",
      "Target:    seinen wa ichi shuukan hodo tatte mata kita\n",
      "Predicted: seinen wa isshu kan hodo date mata kita\n",
      "Target:    tsukuri wa futatabi nyoubou no te wo furihanashita\n",
      "Predicted: suezoui wa futatabi nyoubou no te wo friganashita\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 2/15: Learning rate @ 5.00e-05\n",
      "9000/9000 [==============================] - 17327s 2s/step - loss: 6.0137 - per: 0.0377 - wer: 0.4054 - val_loss: 16.4501 - val_per: 0.0314 - val_wer: 0.2953\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Training\n",
      "Target:    ookami wa emono ga houfu na chiiki ni sunde iru\n",
      "Predicted: oukami wa emono ga houfu na chiki ni sunde iru\n",
      "Target:    denshin wo hatsumei shita no wa dare da ka shitte imasu ka\n",
      "Predicted: denshin wo hatsumei shita no wa dare daka shite imasu ka\n",
      "Target:    hataraku kyokashou wo dono you ni shite nyuushu shimasu ka\n",
      "Predicted: hataikukyokashou wo dono you ni shite nyushu shimasu ka\n",
      "Target:    kenia wa katsute igirisu no shokumin chi de atta\n",
      "Predicted: kenia wa katsute igirisu no shokuminchi de ata\n",
      "\n",
      "Validation\n",
      "Target:    ya hinauta wo kuchiguse ni kyoujou de utatte\n",
      "Predicted: yahi na uta wo kuchiguse ni kyoujou de utate\n",
      "Target:    juu sanya no tsuki wa zutto hikuu nattaga\n",
      "Predicted: jusan ya no suki wa zuto hiku nataga\n",
      "Target:    seinen wa ichi shuukan hodo tatte mata kita\n",
      "Predicted: seinen wa ichshukan hodo tate matakita\n",
      "Target:    tsukuri wa futatabi nyoubou no te wo furihanashita\n",
      "Predicted: suetsui wa futatabi nyoubou no te wo furiganashita\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 3/15: Learning rate @ 4.94e-05\n",
      "9000/9000 [==============================] - 19297s 2s/step - loss: 15.9151 - per: 0.0348 - wer: 0.3602 - val_loss: 16.7726 - val_per: 0.0300 - val_wer: 0.2732\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Training\n",
      "Target:    taema naku shiranai hito ga sa tari ki tari shita\n",
      "Predicted: taema naku shiranai hito ga i tari ki tari shita\n",
      "Target:    iya ni natte mata sugu kaette konai to mo kagiranaiga\n",
      "Predicted: iya ni nate mata sugu kaete kanarito mo kakakanaiga\n",
      "Target:    sore wa imada ni kanojo no mune no ue ni atta\n",
      "Predicted: sore wa imada ni kanojo no mune no oi ni ata\n",
      "Target:    hai n nasai to kotaeru no wa kitto okusan deshita\n",
      "Predicted: ohain nasai to taeru no wa kito okusan deshita\n",
      "\n",
      "Validation\n",
      "Target:    ya hinauta wo kuchiguse ni kyoujou de utatte\n",
      "Predicted: yahi na uta wo kuchiguse ni kyoujou de utate\n",
      "Target:    juu sanya no tsuki wa zutto hikuu nattaga\n",
      "Predicted: jusan ya no tsuki wa zu to hiku nataga\n",
      "Target:    seinen wa ichi shuukan hodo tatte mata kita\n",
      "Predicted: seinen wa ichi shukan hodo da te mata kita\n",
      "Target:    tsukuri wa futatabi nyoubou no te wo furihanashita\n",
      "Predicted: suetsukuri wa futatabi nyoubou no te wo furiganashita\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 4/15: Learning rate @ 4.75e-05\n",
      "2919/9000 [========>.....................] - ETA: 3:26:15 - loss: 9.0652 - per: 0.0342 - wer: 0.3459 "
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.config = Config(args)\n",
    "        self.train_dataset = DataLoader(args).train\n",
    "        self.val_dataset = DataLoader(args).val\n",
    "        schedule = CosineDecayWithWarmup(args)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(schedule)\n",
    "        self.per_metrics = PERMetric(tokenizer=self.config.processor.tokenizer)\n",
    "        self.wer_metrics = WERMetric()\n",
    "        self.model = TFWav2Vec2ForCTC.from_pretrained(\n",
    "            args.model_name,\n",
    "            from_pt=True,\n",
    "            ctc_loss_reduction=\"mean\",\n",
    "            gradient_checkpointing=True,\n",
    "            pad_token_id=self.config.processor.tokenizer.pad_token_id,\n",
    "            vocab_size=len(self.config.processor.tokenizer),\n",
    "            use_bfloat16=True)\n",
    "        self.model.freeze_feature_extractor()\n",
    "        \n",
    "        self.model_name = f\"model_{int(self.args.n_samples/1000)}k\"\n",
    "        self.log_path = f\"{self.args.main_dir}/model_weights/{self.model_name}.csv\"\n",
    "        if not os.path.exists(self.log_path):\n",
    "            print(\"Log file created.\")\n",
    "            columns = \"epoch,loss,per,wer,val_loss,val_per,val_wer\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(columns)\n",
    "\n",
    "    def decoder(self, labels, logits):\n",
    "        labels = tf.where(labels == -100, x=0, y=labels)\n",
    "        logits = tf.argmax(logits, axis=-1)\n",
    "        logits = self.config.processor.batch_decode(\n",
    "            logits, group_tokens=True, skip_special_tokens=True)\n",
    "        return labels, logits\n",
    "\n",
    "    def display(self, epoch, t_labels, t_logits, v_labels, v_logits):\n",
    "        print(\"-\" * 129)\n",
    "        print(\"Training\")\n",
    "        for y_true, y_pred in zip(t_labels, t_logits):\n",
    "            print(f\"Target:    {y_true}\")\n",
    "            print(f\"Predicted: {y_pred}\") \n",
    "\n",
    "        print(\"\\nValidation\")\n",
    "        for y_true, y_pred in zip(v_labels, v_logits):\n",
    "            print(f\"Target:    {y_true}\")\n",
    "            print(f\"Predicted: {y_pred}\")\n",
    "        print(\"-\" * 129)\n",
    "\n",
    "    def fit(self):\n",
    "        # Checkpoint manager\n",
    "        self.ckpt_dir = f\"{self.args.main_dir}/checkpoints\"\n",
    "        self.ckpt = tf.train.Checkpoint(self.model)\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(\n",
    "            checkpoint=self.ckpt, directory=self.ckpt_dir, max_to_keep=5)\n",
    "\n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            self.start_epoch = int(self.ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "            self.ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "            print(f\"Resuming from epoch {self.start_epoch + 1}...\")\n",
    "        else:\n",
    "            self.start_epoch = 0\n",
    "            print(\"Starting from epoch 1...\")\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.args.epochs+1):\n",
    "            print(f\"Epoch {epoch+1}/{self.args.epochs}: Learning rate @ {self.optimizer.lr(epoch):.2e}\")\n",
    "            stateful_metrics = [\"loss\", \"per\", \"wer\", \"val_loss\", \"val_per\", \"val_wer\"]\n",
    "            progbar = tf.keras.utils.Progbar(\n",
    "                self.args.train_steps, interval=0.05,\n",
    "                stateful_metrics=stateful_metrics)\n",
    "\n",
    "            # Training loop\n",
    "            for step, t_batch in enumerate(self.train_dataset):\n",
    "                t_inputs = t_batch['input_values']\n",
    "                t_labels = t_batch['labels']\n",
    "                with tf.GradientTape() as tape:\n",
    "                    t_loss, t_logits = self.model(\n",
    "                        input_values=t_inputs, labels=t_labels, training=True)[:2]\n",
    "                gradients = tape.gradient(t_loss, self.model.trainable_weights)  \n",
    "                self.optimizer.apply_gradients(zip(gradients, self.model.trainable_weights))    \n",
    "                t_labels, t_logits = self.decoder(t_labels, t_logits)\n",
    "                self.per_metrics.update_state(t_labels, t_logits)\n",
    "                t_labels = self.config.processor.batch_decode(\n",
    "                    t_labels, group_tokens=False, skip_special_tokens=True)   \n",
    "                self.wer_metrics.update_state(t_labels, t_logits)\n",
    "                t_per = self.per_metrics.result()\n",
    "                t_wer = self.wer_metrics.result()\n",
    "                t_values = [(\"loss\", t_loss), (\"per\", t_per), (\"wer\", t_wer)]\n",
    "                progbar.update(step, values=t_values, finalize=False)\n",
    "\n",
    "            self.per_metrics.reset_states()\n",
    "            self.wer_metrics.reset_states()\n",
    "            \n",
    "            # Validation loop\n",
    "            for v_batch in self.val_dataset:\n",
    "                v_inputs = v_batch['input_values']\n",
    "                v_labels = v_batch['labels']\n",
    "                v_loss, v_logits = self.model(\n",
    "                    input_values=v_inputs, labels=v_labels, training=False)[:2]       \n",
    "                v_labels, v_logits = self.decoder(v_labels, v_logits)               \n",
    "                self.per_metrics.update_state(v_labels, v_logits)\n",
    "                v_labels = self.config.processor.batch_decode(\n",
    "                    v_labels, group_tokens=False, skip_special_tokens=True)\n",
    "                self.wer_metrics.update_state(v_labels, v_logits)\n",
    "\n",
    "            v_per = self.per_metrics.result()\n",
    "            v_wer = self.wer_metrics.result()\n",
    "            v_values = [\n",
    "                (\"loss\", t_loss), (\"per\", t_per), (\"wer\", t_wer), (\"val_loss\", v_loss),\n",
    "                (\"val_per\", v_per), (\"val_wer\", v_wer)]\n",
    "            progbar.update(self.args.train_steps, values=v_values, finalize=True)\n",
    "            self.per_metrics.reset_states()\n",
    "            self.wer_metrics.reset_states()\n",
    "\n",
    "            # Print sample transcriptions for both loops\n",
    "            self.display(epoch, t_labels, t_logits, v_labels, v_logits)\n",
    "\n",
    "            # Checkpointing\n",
    "            self.ckpt.save(file_prefix=f\"{self.ckpt_dir}/{self.model_name}\")\n",
    "\n",
    "            # Logging\n",
    "            log = f\"{epoch+1},{t_loss},{t_per},{t_wer},{v_loss},{v_per},{v_wer}\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(log)\n",
    "\n",
    "            save_path = f\"{self.args.main_dir}/model_weights\"\n",
    "            self.model.save_weights(f\"{save_path}/{self.model_name}_{epoch+1}.h5\")\n",
    "\n",
    "Trainer(args).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pd.read_csv(\"E:\\Datasets\\ASR-dataset\\model_weights\\model_40k.csv\", index_col=\"epoch\")\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "sns.lineplot(x=history.index, y=history['per'], label=\"per\", ax=ax1)\n",
    "sns.lineplot(x=history.index, y=history['wer'], label=\"wer\", ax=ax2)\n",
    "sns.lineplot(x=history.index, y=history['val_per'], label=\"val_per\", ax=ax1)\n",
    "sns.lineplot(x=history.index, y=history['val_wer'], label=\"val_wer\", ax=ax2)\n",
    "plt.suptitle(\"Acoustic model\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"acoustic_history.png\", transparent=False, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b851d2923cfa3a2562599062e05fd9893d86a7c009c64d8ad3756552e4dd5f41"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('tf-gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
