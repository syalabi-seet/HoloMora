{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from convert_romaji import Romaji2Kana\n",
    "\n",
    "import MeCab\n",
    "import cutlet\n",
    "import jiwer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    TFWav2Vec2ForCTC,\n",
    "    Wav2Vec2Config,\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    logging)\n",
    "\n",
    "from convert_romaji import Romaji2Kana\n",
    "\n",
    "def seed_everything(SEED):\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    print(\"Random seed set.\")\n",
    "\n",
    "seed_everything(42)\n",
    "tf.get_logger().setLevel('FATAL')\n",
    "logging.set_verbosity_error()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=4, buffer_size=1024, epochs=20, learning_rate=5e-05, lr_max=5e-05, lr_min=1e-08, lr_start=1e-08, main_dir='E://Datasets/Acoustic_model', model_name='facebook/wav2vec2-base', n_cycles=0.5, n_samples=40000, n_shards=20, n_train=36000, n_val=4000, random_state=42, sample_rate=16000, sustain_epochs=0, test_size=0.1, train_steps=9000, val_steps=1000, vocab_size=37, warmup_epochs=2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ArgParser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # DataLoader\n",
    "    parser.add_argument(\"--main_dir\", default=\"E://Datasets/Acoustic_model\")\n",
    "    parser.add_argument(\"--sample_rate\", default=16000)\n",
    "    parser.add_argument(\"--test_size\", default=0.1)\n",
    "    parser.add_argument(\"--random_state\", default=42)\n",
    "    parser.add_argument(\"--batch_size\", default=4)\n",
    "    parser.add_argument(\"--n_shards\", default=20)\n",
    "    parser.add_argument(\"--buffer_size\", default=1024)\n",
    "    parser.add_argument(\"--n_samples\", default=40000)\n",
    "\n",
    "    # Trainer\n",
    "    parser.add_argument(\"--model_name\", default=\"facebook/wav2vec2-base\")\n",
    "    parser.add_argument(\"--epochs\", default=20)\n",
    "\n",
    "    # Scheduler\n",
    "    parser.add_argument(\"--learning_rate\", default=5e-5)\n",
    "    parser.add_argument(\"--lr_start\", default=1e-8)\n",
    "    parser.add_argument(\"--lr_min\", default=1e-8)\n",
    "    parser.add_argument(\"--lr_max\", default=5e-5)\n",
    "    parser.add_argument(\"--n_cycles\", default=0.5)\n",
    "    parser.add_argument(\"--warmup_epochs\", default=2)\n",
    "    parser.add_argument(\"--sustain_epochs\", default=0)\n",
    "\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    with open(f\"{args.main_dir}/vocab.json\", \"r\") as f:\n",
    "        vocab_size = len(json.load(f))\n",
    "   \n",
    "    n_train = int(args.n_samples * (1 - args.test_size))\n",
    "    n_val = int(args.n_samples * args.test_size)\n",
    "    train_steps = int(np.ceil(n_train / args.batch_size))\n",
    "    val_steps = int(np.ceil(n_val / args.batch_size))\n",
    "\n",
    "    parser.add_argument(\"--vocab_size\", default=vocab_size)\n",
    "    parser.add_argument(\"--n_train\", default=n_train)\n",
    "    parser.add_argument(\"--n_val\", default=n_val)\n",
    "    parser.add_argument(\"--train_steps\", default=train_steps)  \n",
    "    parser.add_argument(\"--val_steps\", default=val_steps)\n",
    "    \n",
    "    return parser.parse_known_args()[0]\n",
    "\n",
    "args = ArgParser()\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.data = pd.concat([\n",
    "            self.get_kokoro(),\n",
    "            self.get_jsut(),\n",
    "            self.get_commonvoice()], \n",
    "            ignore_index=True)\n",
    "        self.kanji_unicode = self.get_kanji_unicode()\n",
    "        self.katsu = cutlet.Cutlet()\n",
    "        self.katsu.use_foreign_spelling = False\n",
    "    \n",
    "        tqdm.pandas()\n",
    "        # Remove rows that contains non-kanji characters\n",
    "        self.data = self.data[self.data['sentence'].progress_apply(self.check_kanji)] \n",
    "\n",
    "        # Remove words within parenthesis\n",
    "        parenthesis =  r\"\\（.*\\）|\\(.*\\)|\\「.*\\」|\\『.*\\』\"\n",
    "        self.data = self.data[~self.data['sentence'].str.contains(parenthesis)]\n",
    "\n",
    "        # Remove punctuations from sentences\n",
    "        self.data['sentence'] = self.data['sentence'].progress_apply(self.clean_kanji)\n",
    "\n",
    "        self.data['romaji'] = self.data['sentence'].progress_apply(self.kanji2romaji)\n",
    "        self.data['length'] = self.data['path'].progress_apply(self.get_length)\n",
    "        self.data = self.data.query(f\"length >= 50000 & length <= 90000\")\n",
    "        self.data = self.data.reset_index(drop=True)\n",
    "        self.data = self.data.sample(n=self.args.n_samples, random_state=42, ignore_index=True)\n",
    "        self.data.sort_values(by=\"length\", axis=0, ascending=True, inplace=True, ignore_index=True)\n",
    "        self.data.to_csv(\n",
    "            f\"{self.args.main_dir}/ASRDataset.csv\", \n",
    "            encoding=\"utf-8\", index=False)\n",
    "\n",
    "    def get_kokoro(self):\n",
    "        in_dir = \"Datasets\\KOKORO-dataset\"\n",
    "\n",
    "        data = []\n",
    "        transcript_path = f\"{in_dir}/transcripts/*.metadata.txt\"\n",
    "        for transcript in glob.glob(transcript_path):\n",
    "            with open(transcript, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f.readlines():\n",
    "                    data.append(line.split(\"|\"))\n",
    "\n",
    "        data = pd.DataFrame(\n",
    "            data, columns=[\n",
    "                'text_id', 'path', 'start_idx', \n",
    "                'end_idx', 'sentence', 'phonemes'])       \n",
    "\n",
    "        # paths = data['path'].unique()\n",
    "        # for path in tqdm(paths, total=len(paths)):\n",
    "        #     folder_name = path.split(\"_\", 1)[0]\n",
    "        #     in_path = os.path.join(in_dir, folder_name, path)\n",
    "        #     y, sr = librosa.load(in_path, sr=None)\n",
    "        #     for text_id in data.loc[data['path']==path, 'text_id']:\n",
    "        #         out_path = os.path.join(self.args.main_dir, 'wav_cleaned', text_id) + \".wav\"\n",
    "        #         if not os.path.exists(out_path):\n",
    "        #             start_idx = int(data.loc[data['text_id']==text_id, 'start_idx'].item())\n",
    "        #             end_idx = int(data.loc[data['text_id']==text_id, 'end_idx'].item())\n",
    "        #             y_slice = librosa.resample(\n",
    "        #                 y[start_idx:end_idx], orig_sr=sr, target_sr=self.sample_rate)\n",
    "        #             sf.write(out_path, y_slice, samplerate=self.sample_rate, subtype='PCM_16')\n",
    "\n",
    "        data = data[['text_id', 'sentence']]\n",
    "        data['text_id'] = data['text_id'].apply(lambda x: x + \".wav\")\n",
    "        data.columns = ['path', 'sentence']\n",
    "        data['corpus'] = ['kokoro'] * len(data)\n",
    "        return data\n",
    "\n",
    "    def get_jsut(self):\n",
    "        filenames, sentences = [], []\n",
    "        for transcript in glob.glob(r\"Datasets/JSUT-dataset/*/transcript_utf8.txt\"):\n",
    "            file_path = transcript.rsplit(\"\\\\\", 1)[0]\n",
    "            with open(transcript, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines: \n",
    "                    filename, sentence = line.split(\":\")\n",
    "                    filenames.append(os.path.join(file_path, \"wav\", filename) + \".wav\")\n",
    "                    sentences.append(sentence.strip(\"\\n\"))\n",
    "        data = pd.DataFrame({'path': filenames, 'sentence': sentences}) \n",
    "        data['corpus'] = ['jsut'] * len(data)\n",
    "        for i, in_path in tqdm(enumerate(data['path']), total=len(data['path'])):\n",
    "            in_path = in_path.replace(\"\\\\\", \"/\")\n",
    "            out_path = f\"{self.args.main_dir}\\wav_cleaned\"\n",
    "            filename = in_path.rsplit(\"/\", 1)[-1]\n",
    "            out_path = os.path.join(out_path, filename)\n",
    "            if not os.path.exists(out_path):\n",
    "                subprocess.call([\n",
    "                    \"ffmpeg\", \"-i\", in_path,\"-acodec\", \"pcm_s16le\", \n",
    "                    \"-ar\", str(self.sample_rate), out_path])\n",
    "            data['path'][i] = filename\n",
    "        return data\n",
    "\n",
    "    def get_commonvoice(self):\n",
    "        data = pd.read_csv(r\"Datasets/CommonVoice-dataset/validated.tsv\", sep=\"\\t\")\n",
    "        data = data[['path', 'sentence']]    \n",
    "        data['path'] = data['path'].apply(\n",
    "            lambda x: r\"Datasets/CommonVoice-dataset/mp3/\" + x)\n",
    "        data['corpus'] = ['common_voice'] * len(data)\n",
    "        for i, in_path in tqdm(enumerate(data['path']), total=len(data['path'])):\n",
    "            in_path = in_path.replace(\"\\\\\", \"/\")\n",
    "            out_path = f\"{self.args.main_dir}\\wav_cleaned\"\n",
    "            filename = in_path.rsplit(\"/\", 1)[-1]\n",
    "            filename = filename.replace(\"mp3\", \"wav\")\n",
    "            out_path = os.path.join(out_path, filename)\n",
    "            if not os.path.exists(out_path):\n",
    "                subprocess.call([\n",
    "                    \"ffmpeg\", \"-i\", in_path,\"-acodec\", \"pcm_s16le\", \n",
    "                    \"-ar\", str(self.sample_rate), out_path])\n",
    "            data['path'][i] = filename\n",
    "        return data\n",
    "\n",
    "    def get_kanji_unicode(self):\n",
    "        vocab = set()\n",
    "        with open(\n",
    "            r\"D:\\School-stuff\\Sem-2\\PR-Project\\HoloASR\\Datasets\\kanji_unicode.txt\", \n",
    "            encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                for char in line.split()[1:]:\n",
    "                    vocab.add(char)\n",
    "        return \"|\".join(sorted(vocab))\n",
    "    \n",
    "    def check_kanji(self, sentence):\n",
    "        pattern = f\"[^{self.kanji_unicode}]\"\n",
    "        if re.match(pattern, sentence) == None:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def clean_kanji(self, sentence):\n",
    "        sentence = \"\".join(sentence.split())\n",
    "        pattern = r\"[・\\。\\！\\.\\？\\、]\"\n",
    "        sentence = re.sub(pattern, \"\", sentence)\n",
    "        return sentence\n",
    "\n",
    "    def kanji2romaji(self, sentence):\n",
    "        try:\n",
    "            sentence = self.katsu.romaji(sentence)\n",
    "            sentence = sentence.replace(\" \", \"\").lower()\n",
    "        except:\n",
    "            sentence = None\n",
    "        return sentence\n",
    "\n",
    "    def get_length(self, path):\n",
    "        path = os.path.join(self.args.main_dir, 'wav_cleaned', path)\n",
    "        y, sr = librosa.load(path, sr=None)\n",
    "        return len(y)\n",
    "\n",
    "# data = Dataset(args).data\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 4))\n",
    "# sns.histplot(x=data['length'], hue=data['corpus'], palette=\"bright\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, args):\n",
    "        tokenizer = Wav2Vec2CTCTokenizer(\n",
    "            vocab_file=f\"{args.main_dir}/vocab.json\",\n",
    "            do_lower_case=False)\n",
    "\n",
    "        feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "            feature_size=1,\n",
    "            sampling_rate=args.sample_rate,\n",
    "            padding_value=0.0,\n",
    "            do_normalize=True,\n",
    "            return_attention_mask=False)\n",
    "\n",
    "        self.processor = Wav2Vec2Processor(\n",
    "            feature_extractor=feature_extractor,\n",
    "            tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFRWriter():\n",
    "    def __init__(self, args):\n",
    "        self.data = pd.read_csv(os.path.join(args.main_dir, \"ASRDataset.csv\"), encoding=\"utf-8\")\n",
    "        self.args = args\n",
    "        self.processor = Config(args).processor\n",
    "\n",
    "    def _bytes_feature(self, value):\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    def _int64_feature(self, value):\n",
    "        \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "    def _float_feature(self, value):\n",
    "        \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "    def serialize_example(self, *args):\n",
    "        feature = {\n",
    "            'input_values': self._bytes_feature(args[0]),\n",
    "            'labels': self._bytes_feature(args[1])}\n",
    "\n",
    "        example_proto = tf.train.Example(\n",
    "            features=tf.train.Features(feature=feature))\n",
    "        return example_proto.SerializeToString()\n",
    "\n",
    "    def get_labels(self, sample):\n",
    "        labels = self.data.loc[self.data['path']==sample, \"romaji\"].item()\n",
    "        labels = (self.processor.tokenizer.bos_token + labels + \n",
    "            self.processor.tokenizer.eos_token)\n",
    "        with self.processor.as_target_processor():\n",
    "            labels = self.processor(labels, is_split_into_words=True).input_ids\n",
    "        return tf.convert_to_tensor(labels, dtype=tf.int32)\n",
    "\n",
    "    def get_audio(self, sample):\n",
    "        path = os.path.join(self.args.main_dir, \"wav_cleaned\", sample)\n",
    "        audio = librosa.load(path, sr=None)[0]\n",
    "        audio /= audio.max()\n",
    "        return tf.convert_to_tensor(audio, dtype=tf.float32)\n",
    "\n",
    "    def get_shards(self):\n",
    "        skf = KFold(n_splits=self.args.n_shards, shuffle=False)\n",
    "        return [\n",
    "            list(map(lambda x: self.data['path'][x], j))\n",
    "            for i, j in skf.split(self.data['path'])]\n",
    "\n",
    "    def get_shard_data(self, samples):\n",
    "        for sample in samples:\n",
    "            audio = self.get_audio(sample)\n",
    "            labels = self.get_labels(sample)\n",
    "            yield {\n",
    "                'input_values': tf.io.serialize_tensor(audio),\n",
    "                'labels': tf.io.serialize_tensor(labels)}\n",
    "\n",
    "    def write(self):\n",
    "        for shard, samples in tqdm(enumerate(self.get_shards()), total=self.args.n_shards):\n",
    "            with tf.io.TFRecordWriter(f\"{self.args.main_dir}/wav2vec2_tfrec/shard_{shard+1}.tfrec\") as f:\n",
    "                for sample in self.get_shard_data(samples):\n",
    "                    example = self.serialize_example(\n",
    "                        sample['input_values'],\n",
    "                        sample['labels'])\n",
    "                    f.write(example)\n",
    "\n",
    "# TFRWriter(args).write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, args):\n",
    "        self.files = glob.glob(args.main_dir + \"/wav2vec2_tfrec/*.tfrec\")\n",
    "        self.args = args\n",
    "        self.AUTOTUNE = tf.data.AUTOTUNE\n",
    "        self.train_files, self.val_files = train_test_split(\n",
    "            self.files, test_size=args.test_size, shuffle=True, \n",
    "            random_state=args.random_state)\n",
    "        self.train = self.get_train()\n",
    "        self.val = self.get_val()\n",
    "\n",
    "    def read_tfrecord(self, example):\n",
    "        feature_description = {\n",
    "            'input_values': tf.io.FixedLenFeature([], tf.string),\n",
    "            'labels': tf.io.FixedLenFeature([], tf.string)}\n",
    "        \n",
    "        example = tf.io.parse_single_example(example, feature_description)\n",
    "        example['input_values'] = tf.io.parse_tensor(\n",
    "            example['input_values'], out_type=tf.float32)\n",
    "        example['labels'] = tf.io.parse_tensor(\n",
    "            example['labels'], out_type=tf.int32)\n",
    "        return example\n",
    "\n",
    "    def load_dataset(self, files):\n",
    "        ignore_order = tf.data.Options()\n",
    "        ignore_order.experimental_deterministic = False\n",
    "        dataset = tf.data.TFRecordDataset(files)\n",
    "        dataset = dataset.with_options(ignore_order)\n",
    "        dataset = dataset.map(self.read_tfrecord, num_parallel_calls=self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_train(self):\n",
    "        dataset = self.load_dataset(self.train_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_values': [None],\n",
    "                'labels': [None]\n",
    "            },\n",
    "            padding_values={\n",
    "                'input_values': tf.constant(0, dtype=tf.float32), \n",
    "                'labels': tf.constant(-100, dtype=tf.int32)\n",
    "            })        \n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_val(self):\n",
    "        dataset = self.load_dataset(self.val_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_values': [None],\n",
    "                'labels': [None]\n",
    "            },\n",
    "            padding_values={\n",
    "                'input_values': tf.constant(0, dtype=tf.float32),\n",
    "                'labels': tf.constant(-100, dtype=tf.int32)\n",
    "            })\n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "# train = DataLoader(args).train\n",
    "# next(iter(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16,10))\n",
    "# for i, array in enumerate(train.take(16)):\n",
    "#     plt.subplot(4, 4, i+1)\n",
    "#     y = array['input_values'].numpy()\n",
    "#     librosa.display.waveplot(y=y, sr=16000)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class PERMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, tokenizer, name=\"PER\", **kwargs):\n",
    "        super(PERMetric, self).__init__(name=name,  **kwargs)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.accumulator = self.add_weight(name=\"total_per\", initializer=\"zeros\")\n",
    "        self.counter = self.add_weight(name=\"per_count\", initializer=\"zeros\")    \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Re-encode logits\n",
    "        try:\n",
    "            y_pred = self.tokenizer(y_pred, is_split_into_words=False).input_ids\n",
    "        except:\n",
    "            y_pred = [[self.tokenizer.pad_token_id] for _ in range(len(y_pred))]\n",
    "\n",
    "        hypothesis = tf.ragged.constant(y_pred).to_sparse()\n",
    "\n",
    "        # Convert dense to sparse tensor for edit_distance function\n",
    "        truth = tf.RaggedTensor.from_tensor(y_true, padding=0).to_sparse()\n",
    "\n",
    "        # Calculate Levenshtein distance\n",
    "        distance = tf.edit_distance(hypothesis, truth, normalize=True)\n",
    "\n",
    "        # Add distance and number of samples to variables\n",
    "        self.accumulator.assign_add(tf.reduce_mean(distance))\n",
    "        self.counter.assign_add(len(y_true))\n",
    "\n",
    "    def result(self):\n",
    "        # Divides accumulated distance scores against number of samples passed,\n",
    "        # mimics mean reduction over batch\n",
    "        return tf.math.divide_no_nan(self.accumulator, self.counter)   \n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.accumulator.assign(0.0)\n",
    "        self.counter.assign(0.0)\n",
    "\n",
    "class WERMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"WER\", **kwargs):\n",
    "        super(WERMetric, self).__init__(name=name,  **kwargs)\n",
    "        self.tagger = MeCab.Tagger(\"-Owakati\")\n",
    "        self.accumulator = self.add_weight(name=\"total_wer\", initializer=\"zeros\")\n",
    "        self.counter = self.add_weight(name=\"wer_count\", initializer=\"zeros\")   \n",
    "\n",
    "    def processor(self, texts):\n",
    "        new_texts = []\n",
    "        for text in texts:\n",
    "            text = Romaji2Kana(text)\n",
    "            text = self.tagger.parse(text).split()\n",
    "            new_texts.append(\" \".join(text))\n",
    "        return new_texts      \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Convert romaji into hiragana with word tokenization\n",
    "        y_true = self.processor(y_true)\n",
    "        y_pred = self.processor(y_pred)\n",
    "\n",
    "        # Calculate wer score\n",
    "        wer = jiwer.wer(y_true, y_pred)\n",
    "\n",
    "        # Add distance and number of batches to variables\n",
    "        self.accumulator.assign_add(wer)\n",
    "        self.counter.assign_add(1)\n",
    "\n",
    "    def result(self):\n",
    "        # Divides accumulated distance scores against number of batches passed,\n",
    "        # mimics mean reduction over batch\n",
    "        return tf.math.divide_no_nan(self.accumulator, self.counter)   \n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.accumulator.assign(0.0)\n",
    "        self.counter.assign(0.0)\n",
    "\n",
    "class CosineDecayWithWarmup(LearningRateSchedule):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "    def __call__(self, epoch):  \n",
    "        if epoch < self.args.warmup_epochs:\n",
    "            lr = ((self.args.lr_max - self.args.lr_start) / self.args.warmup_epochs) * epoch + self.args.lr_start\n",
    "        elif epoch < (self.args.warmup_epochs + self.args.sustain_epochs):\n",
    "            lr = self.args.lr_max\n",
    "        else:\n",
    "            progress = ((epoch - self.args.warmup_epochs - self.args.sustain_epochs) / \n",
    "            (self.args.epochs - self.args.warmup_epochs - self.args.sustain_epochs))\n",
    "            lr = (self.args.lr_max-self.args.lr_min) * (0.5 * (1.0 + tf.math.cos((22/7) * \n",
    "                self.args.n_cycles * 2.0 * progress)))\n",
    "            if self.args.lr_min is not None:\n",
    "                lr = tf.math.maximum(self.args.lr_min, lr)\n",
    "        return lr\n",
    "\n",
    "    def plot(self):\n",
    "        epochs = range(self.args.epochs+1)\n",
    "        lr = [self(epoch) for epoch in epochs]\n",
    "        plt.plot(epochs, lr)\n",
    "        plt.xlabel(\"learning_rate\")\n",
    "        plt.ylabel(\"epochs\")\n",
    "        plt.show()\n",
    "\n",
    "CosineDecayWithWarmup(args).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\transformers\\configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from epoch 3...\n",
      "Epoch 3/20: Learning rate @ 5.00e-05\n",
      "9000/9000 [==============================] - 20693s 2s/step - loss: 8.8508 - per: 0.0167 - wer: 0.1866 - val_loss: 10.8782 - val_per: 0.0109 - val_wer: 0.1157\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Training\n",
      "Target:    <s>maibannijikaneigonobenkyouwoshimasu</s>\n",
      "Predicted: <s>maibannijikaneigonobenkyoushimasu</s>\n",
      "Target:    <s>tokaidewadeauhitonohotondogamishiranuhitodearu</s>\n",
      "Predicted: <s>tokaidewadeauhitonohotondogamishiranuhitodearu</s>\n",
      "Target:    <s>karewasokonitatteyorikakariteishutsushitemattakushitsumonshimasendeshita</s>\n",
      "Predicted: <s>karewasokonitatteyorikagariteistsushitemattakushutsumonshimnsendeshita</s>\n",
      "Target:    <s>futottadakenanonikinnikuwofuyashitatoiiharu</s>\n",
      "Predicted: <s>futottadakenanonikinniguwofuyashitatoiiharu</s>\n",
      "\n",
      "Validation\n",
      "Target:    <s>yahinautawokuchigusenikyoujoudeutatte</s>\n",
      "Predicted: <s>iyahinautawokuchigusenikyoujoudeutatte</s>\n",
      "Target:    <s>juusanyanotsukiwazuttohikuunattaga</s>\n",
      "Predicted: <s>juusanyanotsukiwazuutohikuunattaga</s>\n",
      "Target:    <s>seinenwaichishuukanhodotattematakita</s>\n",
      "Predicted: <s>seinenwaichishuukahododattematakita</s>\n",
      "Target:    <s>tsukuriwafutatabinyoubounotewofurihanashita</s>\n",
      "Predicted: <s>suetsukuriwafutatabinyoubounotewofuriganashita</s>\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 4/20: Learning rate @ 4.96e-05\n",
      "7947/9000 [=========================>....] - ETA: 37:57 - loss: 6.9891 - per: 0.0144 - wer: 0.1630"
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.config = Config(args)\n",
    "        self.train_dataset = DataLoader(args).train\n",
    "        self.val_dataset = DataLoader(args).val\n",
    "        schedule = CosineDecayWithWarmup(args)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(schedule)\n",
    "        self.per_metrics = PERMetric(tokenizer=self.config.processor.tokenizer)\n",
    "        self.wer_metrics = WERMetric()\n",
    "        self.model = TFWav2Vec2ForCTC.from_pretrained(\n",
    "            args.model_name,\n",
    "            from_pt=True,\n",
    "            ctc_loss_reduction=\"mean\",\n",
    "            gradient_checkpointing=True,\n",
    "            pad_token_id=self.config.processor.tokenizer.pad_token_id,\n",
    "            vocab_size=len(self.config.processor.tokenizer),\n",
    "            use_bfloat16=True)\n",
    "        self.model.freeze_feature_extractor()\n",
    "        \n",
    "        self.model_name = f\"model_{int(self.args.n_samples/1000)}k\"\n",
    "        self.log_path = f\"{self.args.main_dir}/model_weights/{self.model_name}.csv\"\n",
    "        if not os.path.exists(self.log_path):\n",
    "            print(\"Log file created.\")\n",
    "            columns = \"epoch,loss,per,wer,val_loss,val_per,val_wer\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(columns)\n",
    "\n",
    "    def decoder(self, labels, logits):\n",
    "        labels = tf.where(labels < 0, x=0, y=labels)\n",
    "        logits = tf.argmax(logits, axis=-1)\n",
    "        logits = self.config.processor.batch_decode(\n",
    "            logits, group_tokens=True, skip_special_tokens=True)\n",
    "        return labels, logits\n",
    "\n",
    "    def display(self, epoch, t_labels, t_logits, v_labels, v_logits):\n",
    "        print(\"-\" * 129)\n",
    "        print(\"Training\")\n",
    "        for y_true, y_pred in zip(t_labels, t_logits):\n",
    "            print(f\"Target:    {y_true}\")\n",
    "            print(f\"Predicted: {y_pred}\") \n",
    "\n",
    "        print(\"\\nValidation\")\n",
    "        for y_true, y_pred in zip(v_labels, v_logits):\n",
    "            print(f\"Target:    {y_true}\")\n",
    "            print(f\"Predicted: {y_pred}\")\n",
    "        print(\"-\" * 129)\n",
    "\n",
    "    def fit(self):\n",
    "        # Checkpoint manager\n",
    "        self.ckpt_dir = f\"{self.args.main_dir}/checkpoints\"\n",
    "        self.ckpt = tf.train.Checkpoint(self.model)\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(\n",
    "            checkpoint=self.ckpt, directory=self.ckpt_dir, max_to_keep=5)\n",
    "\n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            self.start_epoch = int(self.ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "            self.ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "            print(f\"Resuming from epoch {self.start_epoch + 1}...\")\n",
    "        else:\n",
    "            self.start_epoch = 0\n",
    "            print(\"Starting from epoch 1...\")\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.args.epochs+1):\n",
    "            print(f\"Epoch {epoch+1}/{self.args.epochs}: Learning rate @ {self.optimizer.lr(epoch):.2e}\")\n",
    "            stateful_metrics = [\"loss\", \"per\", \"wer\", \"val_loss\", \"val_per\", \"val_wer\"]\n",
    "            progbar = tf.keras.utils.Progbar(\n",
    "                self.args.train_steps, interval=0.05,\n",
    "                stateful_metrics=stateful_metrics)\n",
    "\n",
    "            # Training loop\n",
    "            for step, t_batch in enumerate(self.train_dataset):\n",
    "                t_inputs = t_batch['input_values']\n",
    "                t_labels = t_batch['labels']\n",
    "                with tf.GradientTape() as tape:\n",
    "                    t_loss, t_logits = self.model(\n",
    "                        input_values=t_inputs, labels=t_labels, training=True)[:2]\n",
    "                gradients = tape.gradient(t_loss, self.model.trainable_weights)  \n",
    "                self.optimizer.apply_gradients(zip(gradients, self.model.trainable_weights))    \n",
    "                t_labels, t_logits = self.decoder(t_labels, t_logits)\n",
    "                self.per_metrics.update_state(t_labels, t_logits)\n",
    "                t_labels = self.config.processor.batch_decode(\n",
    "                    t_labels, group_tokens=False, skip_special_tokens=True)   \n",
    "                self.wer_metrics.update_state(t_labels, t_logits)\n",
    "                t_per = self.per_metrics.result()\n",
    "                t_wer = self.wer_metrics.result()\n",
    "                t_values = [(\"loss\", t_loss), (\"per\", t_per), (\"wer\", t_wer)]\n",
    "                progbar.update(step, values=t_values, finalize=False)\n",
    "\n",
    "            self.per_metrics.reset_states()\n",
    "            self.wer_metrics.reset_states()\n",
    "            \n",
    "            # Validation loop\n",
    "            for v_batch in self.val_dataset:\n",
    "                v_inputs = v_batch['input_values']\n",
    "                v_labels = v_batch['labels']\n",
    "                v_loss, v_logits = self.model(\n",
    "                    input_values=v_inputs, labels=v_labels, training=False)[:2]       \n",
    "                v_labels, v_logits = self.decoder(v_labels, v_logits)               \n",
    "                self.per_metrics.update_state(v_labels, v_logits)\n",
    "                v_labels = self.config.processor.batch_decode(\n",
    "                    v_labels, group_tokens=False, skip_special_tokens=True)\n",
    "                self.wer_metrics.update_state(v_labels, v_logits)\n",
    "\n",
    "            v_per = self.per_metrics.result()\n",
    "            v_wer = self.wer_metrics.result()\n",
    "            v_values = [\n",
    "                (\"loss\", t_loss), (\"per\", t_per), (\"wer\", t_wer), (\"val_loss\", v_loss),\n",
    "                (\"val_per\", v_per), (\"val_wer\", v_wer)]\n",
    "            progbar.update(self.args.train_steps, values=v_values, finalize=True)\n",
    "            self.per_metrics.reset_states()\n",
    "            self.wer_metrics.reset_states()\n",
    "\n",
    "            # Print sample transcriptions for both loops\n",
    "            self.display(epoch, t_labels, t_logits, v_labels, v_logits)\n",
    "\n",
    "            # Checkpointing\n",
    "            self.ckpt.save(file_prefix=f\"{self.ckpt_dir}/{self.model_name}\")\n",
    "\n",
    "            # Logging\n",
    "            log = f\"{epoch+1},{t_loss},{t_per},{t_wer},{v_loss},{v_per},{v_wer}\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(log)\n",
    "\n",
    "            save_path = f\"{self.args.main_dir}/model_weights\"\n",
    "            self.model.save_weights(f\"{save_path}/{self.model_name}_{epoch+1}.h5\")\n",
    "\n",
    "Trainer(args).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pd.read_csv(\"E:\\Datasets\\ASR-dataset\\model_weights\\model_40k.csv\", index_col=\"epoch\")\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "sns.lineplot(x=history.index, y=history['per'], label=\"per\", ax=ax1)\n",
    "sns.lineplot(x=history.index, y=history['wer'], label=\"wer\", ax=ax2)\n",
    "sns.lineplot(x=history.index, y=history['val_per'], label=\"val_per\", ax=ax1)\n",
    "sns.lineplot(x=history.index, y=history['val_wer'], label=\"val_wer\", ax=ax2)\n",
    "plt.suptitle(\"Acoustic model\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"acoustic_history.png\", transparent=False, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b851d2923cfa3a2562599062e05fd9893d86a7c009c64d8ad3756552e4dd5f41"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('tf-gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
