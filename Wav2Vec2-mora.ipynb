{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import regex as re\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import groupby\n",
    "from convert_romaji import Romaji2Kana\n",
    "\n",
    "import cutlet\n",
    "import jiwer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "\n",
    "from transformers import (\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2Processor,\n",
    "    TFWav2Vec2ForCTC,\n",
    "    logging)\n",
    "\n",
    "from convert_romaji import Romaji2Kana\n",
    "\n",
    "def seed_everything(SEED):\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    print(\"Random seed set.\")\n",
    "\n",
    "seed_everything(42)\n",
    "tf.get_logger().setLevel('FATAL')\n",
    "logging.set_verbosity_error()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=4, buffer_size=512, epochs=15, learning_rate=5e-05, lr_max=5e-05, lr_min=1e-10, lr_start=1e-08, main_dir='E://Datasets/Acoustic_model', model_name='facebook/wav2vec2-base', n_cycles=0.5, n_samples=50000, n_shards=40, n_train=45000, n_val=5000, random_state=42, sample_rate=16000, sustain_epochs=0, test_size=0.1, train_steps=11250, val_steps=1250, vocab_size=37, warmup_epochs=3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ArgParser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # DataLoader\n",
    "    parser.add_argument(\"--main_dir\", default=\"E://Datasets/Acoustic_model\")\n",
    "    parser.add_argument(\"--sample_rate\", default=16000)\n",
    "    parser.add_argument(\"--test_size\", default=0.1)\n",
    "    parser.add_argument(\"--random_state\", default=42)\n",
    "    parser.add_argument(\"--batch_size\", default=4)\n",
    "    parser.add_argument(\"--n_shards\", default=40)\n",
    "    parser.add_argument(\"--buffer_size\", default=512)\n",
    "    parser.add_argument(\"--n_samples\", default=50000)\n",
    "\n",
    "    # Trainer\n",
    "    parser.add_argument(\"--model_name\", default=\"facebook/wav2vec2-base\")\n",
    "    parser.add_argument(\"--epochs\", default=15)\n",
    "\n",
    "    # Scheduler\n",
    "    parser.add_argument(\"--learning_rate\", default=5e-5)\n",
    "    parser.add_argument(\"--lr_start\", default=1e-8)\n",
    "    parser.add_argument(\"--lr_min\", default=1e-10)\n",
    "    parser.add_argument(\"--lr_max\", default=5e-5)\n",
    "    parser.add_argument(\"--n_cycles\", default=0.5)\n",
    "    parser.add_argument(\"--warmup_epochs\", default=3)\n",
    "    parser.add_argument(\"--sustain_epochs\", default=0)\n",
    "\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    with open(f\"{args.main_dir}/vocab.json\", \"r\") as f:\n",
    "        vocab_size = len(json.load(f))\n",
    "   \n",
    "    n_train = int(args.n_samples * (1 - args.test_size))\n",
    "    n_val = int(args.n_samples * args.test_size)\n",
    "    train_steps = int(np.ceil(n_train / args.batch_size))\n",
    "    val_steps = int(np.ceil(n_val / args.batch_size))\n",
    "\n",
    "    parser.add_argument(\"--vocab_size\", default=vocab_size)\n",
    "    parser.add_argument(\"--n_train\", default=n_train)\n",
    "    parser.add_argument(\"--n_val\", default=n_val)\n",
    "    parser.add_argument(\"--train_steps\", default=train_steps)  \n",
    "    parser.add_argument(\"--val_steps\", default=val_steps)\n",
    "    \n",
    "    return parser.parse_known_args()[0]\n",
    "\n",
    "args = ArgParser()\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.data_dir = \"E:/Datasets/Acoustic_model/raw_data\"\n",
    "        self.data = pd.concat([\n",
    "            self.get_kokoro(),\n",
    "            self.get_jsut(),\n",
    "            self.get_commonvoice()], \n",
    "            ignore_index=True)\n",
    "        self.kanji_unicode = self.get_kanji_unicode()\n",
    "        self.katsu = cutlet.Cutlet()\n",
    "        self.katsu.use_foreign_spelling = False\n",
    "        self.vocab = self.get_vocab()\n",
    "    \n",
    "        tqdm.pandas()\n",
    "        # Remove words within parenthesis\n",
    "        parenthesis =  r\"\\（.*\\）|\\(.*\\)|\\「.*\\」|\\『.*\\』\"\n",
    "        self.data = self.data[~self.data['sentence'].str.contains(parenthesis)]\n",
    "\n",
    "        # Remove punctuations from sentences\n",
    "        self.data['sentence'] = self.data['sentence'].progress_apply(self.clean_kanji)\n",
    "        self.data['romaji'] = self.data['sentence'].progress_apply(self.kanji2romaji)\n",
    "        self.data['length'] = self.data['path'].progress_apply(self.get_length)\n",
    "        self.data = self.data[self.data['length'].between(48000, 95000)]\n",
    "        self.data = self.data.reset_index(drop=True)\n",
    "        self.data = self.data.sample(n=self.args.n_samples, random_state=42, ignore_index=True)\n",
    "        self.data.sort_values(by=\"length\", axis=0, ascending=True, inplace=True, ignore_index=True)\n",
    "        self.data.to_csv(\n",
    "            f\"{self.args.main_dir}/ASRDataset.csv\", \n",
    "            encoding=\"utf-8\", index=False)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        with open(r\"E:\\Datasets\\Acoustic_model\\vocab.json\") as f:\n",
    "            vocab = \"|\".join(list(json.load(f).keys())[4:])\n",
    "        return vocab\n",
    "\n",
    "    def get_kokoro(self):\n",
    "        data = []\n",
    "        transcript_path = f\"{self.data_dir}/KOKORO-dataset/transcripts/*.metadata.txt\"\n",
    "        for transcript in glob.glob(transcript_path):\n",
    "            with open(transcript, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f.readlines():\n",
    "                    data.append(line.split(\"|\"))\n",
    "\n",
    "        data = pd.DataFrame(\n",
    "            data, columns=[\n",
    "                'text_id', 'path', 'start_idx', \n",
    "                'end_idx', 'sentence', 'phonemes'])       \n",
    "\n",
    "        # paths = data['path'].unique()\n",
    "        # for path in tqdm(paths, total=len(paths)):\n",
    "        #     folder_name = path.split(\"_\", 1)[0]\n",
    "        #     in_path = os.path.join(f\"{self.data_dir}/KOKORO-dataset\", folder_name, path)\n",
    "        #     y, sr = librosa.load(in_path, sr=None)\n",
    "        #     for text_id in data.loc[data['path']==path, 'text_id']:\n",
    "        #         out_path = os.path.join(self.args.main_dir, 'wav_cleaned', text_id) + \".wav\"\n",
    "        #         if not os.path.exists(out_path):\n",
    "        #             start_idx = int(data.loc[data['text_id']==text_id, 'start_idx'].item())\n",
    "        #             end_idx = int(data.loc[data['text_id']==text_id, 'end_idx'].item())\n",
    "        #             y_slice = librosa.resample(\n",
    "        #                 y[start_idx:end_idx], orig_sr=sr, target_sr=self.args.sample_rate)\n",
    "        #             sf.write(out_path, y_slice, samplerate=self.args.sample_rate, subtype='PCM_16')\n",
    "\n",
    "        data = data[['text_id', 'sentence']]\n",
    "        data['text_id'] = data['text_id'].apply(lambda x: x + \".wav\")\n",
    "        data.columns = ['path', 'sentence']\n",
    "        data['corpus'] = ['kokoro'] * len(data)\n",
    "        return data\n",
    "\n",
    "    def get_jsut(self):\n",
    "        filenames, sentences = [], []\n",
    "        for transcript in glob.glob(f\"{self.data_dir}/JSUT-dataset/*/transcript_utf8.txt\"):\n",
    "            file_path = transcript.rsplit(\"\\\\\", 1)[0]\n",
    "            with open(transcript, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines: \n",
    "                    filename, sentence = line.split(\":\")\n",
    "                    filenames.append(os.path.join(file_path, \"wav\", filename) + \".wav\")\n",
    "                    sentences.append(sentence.strip(\"\\n\"))\n",
    "        data = pd.DataFrame({'path': filenames, 'sentence': sentences}) \n",
    "        data['corpus'] = ['jsut'] * len(data)\n",
    "        for i, in_path in tqdm(enumerate(data['path']), total=len(data['path'])):\n",
    "            in_path = in_path.replace(\"\\\\\", \"/\")\n",
    "            out_path = f\"{self.args.main_dir}\\wav_cleaned\"\n",
    "            filename = in_path.rsplit(\"/\", 1)[-1]\n",
    "            out_path = os.path.join(out_path, filename)\n",
    "            if not os.path.exists(out_path):\n",
    "                subprocess.call([\n",
    "                    \"ffmpeg\", \"-i\", in_path,\"-acodec\", \"pcm_s16le\", \n",
    "                    \"-ar\", str(self.args.sample_rate), out_path])\n",
    "            data['path'][i] = filename\n",
    "        return data\n",
    "\n",
    "    def get_commonvoice(self):\n",
    "        data = pd.read_csv(f\"{self.data_dir}/CommonVoice-dataset/validated.tsv\", sep=\"\\t\")\n",
    "        data = data[['path', 'sentence']]    \n",
    "        data['path'] = data['path'].apply(\n",
    "            lambda x: f\"{self.data_dir}/CommonVoice-dataset/clips/\" + x)\n",
    "        data['corpus'] = ['common_voice'] * len(data)\n",
    "        for i, in_path in tqdm(enumerate(data['path']), total=len(data['path'])):\n",
    "            in_path = in_path.replace(\"\\\\\", \"/\")\n",
    "            out_path = f\"{self.args.main_dir}\\wav_cleaned\"\n",
    "            filename = in_path.rsplit(\"/\", 1)[-1]\n",
    "            filename = filename.replace(\"mp3\", \"wav\")\n",
    "            out_path = os.path.join(out_path, filename)\n",
    "            if not os.path.exists(out_path):\n",
    "                subprocess.call([\n",
    "                    \"ffmpeg\", \"-i\", in_path,\"-acodec\", \"pcm_s16le\", \n",
    "                    \"-ar\", str(self.args.sample_rate), out_path])\n",
    "            data['path'][i] = filename\n",
    "        return data\n",
    "\n",
    "    def get_kanji_unicode(self):\n",
    "        vocab = set()\n",
    "        with open(\n",
    "            f\"{self.data_dir}/kanji_unicode.txt\", \n",
    "            encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                for char in line.split()[1:]:\n",
    "                    vocab.add(char)\n",
    "        return \"\".join(sorted(vocab))\n",
    "        \n",
    "    def clean_kanji(self, sentence):\n",
    "        sentence = \"\".join(sentence.split())\n",
    "        pattern = f\"[^{self.kanji_unicode}]\"\n",
    "        sentence = re.sub(pattern, \"\", sentence)\n",
    "        return sentence\n",
    "\n",
    "    def kanji2romaji(self, sentence):\n",
    "        try:\n",
    "            sentence = self.katsu.romaji(sentence)\n",
    "            sentence = \" \".join(sentence.split())\n",
    "            sentence = re.sub(r\"'|-| \", \"\", sentence).lower()\n",
    "        except:\n",
    "            sentence = None\n",
    "        return sentence\n",
    "\n",
    "    def get_length(self, path):\n",
    "        path = os.path.join(self.args.main_dir, 'wav_cleaned', path)\n",
    "        y = librosa.load(path, sr=None)[0]\n",
    "        return len(y)\n",
    "\n",
    "# data = Dataset(args).data\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 4))\n",
    "# sns.histplot(x=data['length'], hue=data['corpus'], palette=\"bright\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(r\"E:\\Datasets\\Acoustic_model\\vocab.json\") as f:\n",
    "#     vocab = list(json.load(f).keys())[5:]\n",
    "\n",
    "# double_consonants = set()\n",
    "# for romaji in data['romaji']:\n",
    "#     for i in range(len(romaji)):\n",
    "#         if i < len(romaji) - 1:\n",
    "#             if romaji[i] == romaji[i+1]:\n",
    "#                 double_consonants.add(romaji[i])\n",
    "\n",
    "# double_consonants = list(double_consonants)\n",
    "# print(double_consonants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, args):\n",
    "        tokenizer = Wav2Vec2CTCTokenizer(\n",
    "            vocab_file=f\"{args.main_dir}/vocab.json\",\n",
    "            do_lower_case=False)\n",
    "\n",
    "        feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "            feature_size=1,\n",
    "            sampling_rate=args.sample_rate,\n",
    "            padding_value=0.0,\n",
    "            do_normalize=False,\n",
    "            return_attention_mask=False)\n",
    "\n",
    "        self.processor = Wav2Vec2Processor(\n",
    "            feature_extractor=feature_extractor,\n",
    "            tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFRWriter():\n",
    "    def __init__(self, args):\n",
    "        self.data = pd.read_csv(os.path.join(args.main_dir, \"ASRDataset.csv\"), encoding=\"utf-8\")\n",
    "        self.args = args\n",
    "        self.config = Config(args)\n",
    "\n",
    "    def _bytes_feature(self, value):\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    def _int64_feature(self, value):\n",
    "        \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "    def _float_feature(self, value):\n",
    "        \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "    def serialize_example(self, *args):\n",
    "        feature = {\n",
    "            'input_values': self._bytes_feature(args[0]),\n",
    "            'labels': self._bytes_feature(args[1])}\n",
    "\n",
    "        example_proto = tf.train.Example(\n",
    "            features=tf.train.Features(feature=feature))\n",
    "        return example_proto.SerializeToString()\n",
    "\n",
    "    def get_labels(self, sample):\n",
    "        labels = self.data.loc[self.data['path']==sample, \"romaji\"].item()\n",
    "        labels = (self.config.processor.tokenizer.bos_token + labels + \n",
    "            self.config.processor.tokenizer.eos_token)\n",
    "        labels = self.config.processor.tokenizer(labels, is_split_into_words=False).input_ids\n",
    "        return tf.convert_to_tensor(labels, dtype=tf.int32)\n",
    "\n",
    "    def get_audio(self, sample):\n",
    "        path = os.path.join(self.args.main_dir, \"wav_cleaned\", sample)\n",
    "        audio = librosa.load(path, sr=None)[0]\n",
    "        return tf.convert_to_tensor(audio, dtype=tf.float32)\n",
    "\n",
    "    def get_shards(self):\n",
    "        skf = KFold(n_splits=self.args.n_shards, shuffle=False)\n",
    "        return [\n",
    "            list(map(lambda x: self.data['path'][x], j))\n",
    "            for _, j in skf.split(self.data['path'])]\n",
    "\n",
    "    def get_shard_data(self, samples):\n",
    "        for sample in samples:\n",
    "            audio = self.get_audio(sample)\n",
    "            labels = self.get_labels(sample)\n",
    "            yield {\n",
    "                'input_values': tf.io.serialize_tensor(audio),\n",
    "                'labels': tf.io.serialize_tensor(labels)}\n",
    "\n",
    "    def write(self):\n",
    "        for shard, samples in tqdm(enumerate(self.get_shards()), total=self.args.n_shards):\n",
    "            with tf.io.TFRecordWriter(f\"{self.args.main_dir}/wav2vec2_tfrec/shard_{shard+1}.tfrec\") as f:\n",
    "                for sample in self.get_shard_data(samples):\n",
    "                    example = self.serialize_example(\n",
    "                        sample['input_values'],\n",
    "                        sample['labels'])\n",
    "                    f.write(example)\n",
    "\n",
    "# TFRWriter(args).write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, args):\n",
    "        self.files = glob.glob(args.main_dir + \"/wav2vec2_tfrec/*.tfrec\")\n",
    "        self.args = args\n",
    "        self.AUTOTUNE = tf.data.AUTOTUNE\n",
    "        self.train_files, self.val_files = train_test_split(\n",
    "            self.files, test_size=args.test_size, shuffle=True, \n",
    "            random_state=args.random_state)\n",
    "        self.train = self.get_train()\n",
    "        self.val = self.get_val()\n",
    "\n",
    "    def read_tfrecord(self, example):\n",
    "        feature_description = {\n",
    "            'input_values': tf.io.FixedLenFeature([], tf.string),\n",
    "            'labels': tf.io.FixedLenFeature([], tf.string)}\n",
    "        \n",
    "        example = tf.io.parse_single_example(example, feature_description)\n",
    "        example['input_values'] = tf.io.parse_tensor(\n",
    "            example['input_values'], out_type=tf.float32)\n",
    "        example['labels'] = tf.io.parse_tensor(\n",
    "            example['labels'], out_type=tf.int32)\n",
    "        return example\n",
    "\n",
    "    def load_dataset(self, files):\n",
    "        ignore_order = tf.data.Options()\n",
    "        ignore_order.experimental_deterministic = False\n",
    "        dataset = tf.data.TFRecordDataset(files)\n",
    "        dataset = dataset.with_options(ignore_order)\n",
    "        dataset = dataset.map(self.read_tfrecord, num_parallel_calls=self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_train(self):\n",
    "        dataset = self.load_dataset(self.train_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_values': [None],\n",
    "                'labels': [None]},\n",
    "            padding_values={\n",
    "                'input_values': tf.constant(0, dtype=tf.float32), \n",
    "                'labels': tf.constant(-100, dtype=tf.int32)})        \n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_val(self):\n",
    "        dataset = self.load_dataset(self.val_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_values': [None],\n",
    "                'labels': [None]},\n",
    "            padding_values={\n",
    "                'input_values': tf.constant(0, dtype=tf.float32),\n",
    "                'labels': tf.constant(-100, dtype=tf.int32)})\n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "# train = DataLoader(args).train\n",
    "# output = next(iter(train))\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16,10))\n",
    "# for i, array in enumerate(train.take(16)):\n",
    "#     plt.subplot(4, 4, i+1)\n",
    "#     y = array['input_values'].numpy()\n",
    "#     librosa.display.waveplot(y=y, sr=16000)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class PERMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"PER\", **kwargs):\n",
    "        super(PERMetric, self).__init__(name=name,  **kwargs)\n",
    "        self.accumulator = self.add_weight(name=\"total_per\", initializer=\"zeros\")\n",
    "        self.counter = self.add_weight(name=\"per_count\", initializer=\"zeros\")    \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        hypothesis = tf.cast(tf.sparse.from_dense(y_pred), dtype=tf.int32)\n",
    "\n",
    "        # Convert dense to sparse tensor for edit_distance function\n",
    "        truth = tf.RaggedTensor.from_tensor(y_true, padding=0).to_sparse()\n",
    "\n",
    "        # Calculate Levenshtein distance\n",
    "        distance = tf.edit_distance(hypothesis, truth, normalize=True)\n",
    "\n",
    "        # Add distance and number of samples to variables\n",
    "        self.accumulator.assign_add(tf.reduce_sum(distance))\n",
    "        self.counter.assign_add(len(y_true))\n",
    "\n",
    "    def result(self):\n",
    "        # Divides accumulated distance scores against number of samples passed,\n",
    "        # mimics mean reduction over batch\n",
    "        return tf.math.divide_no_nan(self.accumulator, self.counter)   \n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.accumulator.assign(0.0)\n",
    "        self.counter.assign(0.0)\n",
    "\n",
    "class SentencesToListOfCharacters(jiwer.AbstractTransform):\n",
    "    def process_string(self, s):\n",
    "        return list(s)\n",
    "\n",
    "    def process_list(self, inp):\n",
    "        chars = []\n",
    "        for sentence in inp:\n",
    "            chars.extend(self.process_string(sentence))\n",
    "        return chars\n",
    "\n",
    "class CERMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"CER\", **kwargs):\n",
    "        super(CERMetric, self).__init__(name=name,  **kwargs)\n",
    "        self.transform = jiwer.Compose([\n",
    "            jiwer.RemoveMultipleSpaces(),\n",
    "            jiwer.Strip(),\n",
    "            SentencesToListOfCharacters(),\n",
    "            jiwer.RemoveEmptyStrings(),\n",
    "        ])\n",
    "        self.accumulator = self.add_weight(name=\"total_cer\", initializer=\"zeros\")\n",
    "        self.counter = self.add_weight(name=\"cer_count\", initializer=\"zeros\")    \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        cer = jiwer.wer(\n",
    "            truth=y_true, hypothesis=y_pred, \n",
    "            truth_transform=self.transform, hypothesis_transform=self.transform)\n",
    "\n",
    "        # Add distance and number of samples to variables\n",
    "        self.accumulator.assign_add(cer)\n",
    "        self.counter.assign_add(1)\n",
    "\n",
    "    def result(self):\n",
    "        # Divides accumulated distance scores against number of samples passed,\n",
    "        # mimics mean reduction over batch\n",
    "        return tf.math.divide_no_nan(self.accumulator, self.counter)   \n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.accumulator.assign(0.0)\n",
    "        self.counter.assign(0.0)\n",
    "\n",
    "class CosineDecayWithWarmup(LearningRateSchedule):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "    def __call__(self, epoch):  \n",
    "        if epoch < self.args.warmup_epochs:\n",
    "            lr = ((self.args.lr_max - self.args.lr_start) / self.args.warmup_epochs) * epoch + self.args.lr_start\n",
    "        elif epoch < (self.args.warmup_epochs + self.args.sustain_epochs):\n",
    "            lr = self.args.lr_max\n",
    "        else:\n",
    "            progress = ((epoch - self.args.warmup_epochs - self.args.sustain_epochs) / \n",
    "            (self.args.epochs - self.args.warmup_epochs - self.args.sustain_epochs))\n",
    "            lr = (self.args.lr_max-self.args.lr_min) * (0.5 * (1.0 + tf.math.cos((22/7) * \n",
    "                self.args.n_cycles * 2.0 * progress)))\n",
    "            if self.args.lr_min is not None:\n",
    "                lr = tf.math.maximum(self.args.lr_min, lr)\n",
    "        return lr\n",
    "\n",
    "    def plot(self):\n",
    "        epochs = range(self.args.epochs+1)\n",
    "        lr = [self(epoch) for epoch in epochs]\n",
    "        plt.plot(epochs, lr)\n",
    "        plt.xlabel(\"learning_rate\")\n",
    "        plt.ylabel(\"epochs\")\n",
    "        plt.show()\n",
    "\n",
    "CosineDecayWithWarmup(args).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\transformers\\configuration_utils.py:341: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from epoch 1...\n",
      "Epoch 1/15: Learning rate @ 1.00e-08\n",
      "  166/11250 [..............................] - ETA: 3:49:19 - loss: 119.9328 - per: 1.1220 - cer: 1.0032"
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.config = Config(args)\n",
    "        self.train_dataset = DataLoader(args).train\n",
    "        self.val_dataset = DataLoader(args).val\n",
    "        schedule = CosineDecayWithWarmup(args)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(schedule)\n",
    "        self.per_metrics = PERMetric()\n",
    "        self.cer_metrics = CERMetric()\n",
    "        self.model = TFWav2Vec2ForCTC.from_pretrained(\n",
    "            args.model_name,\n",
    "            from_pt=True,\n",
    "            ctc_loss_reduction=\"mean\",\n",
    "            gradient_checkpointing=True,\n",
    "            pad_token_id=self.config.processor.tokenizer.pad_token_id,\n",
    "            vocab_size=len(self.config.processor.tokenizer))\n",
    "        self.model.freeze_feature_extractor()\n",
    "        \n",
    "        self.model_name = f\"model_{int(self.args.n_samples/1000)}k_v1\"\n",
    "        self.log_path = f\"{self.args.main_dir}/model_weights/{self.model_name}.csv\"\n",
    "        if not os.path.exists(self.log_path):\n",
    "            print(\"Log file created.\")\n",
    "            columns = \"epoch,loss,per,cer,val_loss,val_per,val_cer\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(columns)\n",
    "\n",
    "    def group_tokens(self, text):\n",
    "        new_text = []\n",
    "        doubles = ['t', 'k', 'p', 's', 'a', 'e', 'i', 'o', 'u']\n",
    "        for token, group in groupby(text):\n",
    "            group = list(group)\n",
    "            if group[0] in doubles:\n",
    "                token = \"\".join(group[:2])\n",
    "            new_text.append(token)\n",
    "        return Romaji2Kana(\"\".join(new_text))\n",
    "    \n",
    "    def decoder(self, labels, logits):\n",
    "        labels = self.config.processor.batch_decode(\n",
    "            labels, skip_special_tokens=True, group_tokens=False)\n",
    "        labels = list(map(Romaji2Kana, labels))\n",
    "        logits = self.config.processor.batch_decode(\n",
    "            logits, skip_special_tokens=True, group_tokens=False)\n",
    "        logits = list(map(self.group_tokens, logits))\n",
    "        return labels, logits\n",
    "\n",
    "    def display(self, t_labels, t_logits, v_labels, v_logits):       \n",
    "        print(\"-\" * 129)\n",
    "        print(\"Training\")\n",
    "        for y_true, y_pred in zip(t_labels, t_logits):\n",
    "            print(f\"Target:    {y_true}\")\n",
    "            print(f\"Predicted: {y_pred}\") \n",
    "\n",
    "        print(\"\\nValidation\")\n",
    "        for y_true, y_pred in zip(v_labels, v_logits):\n",
    "            print(f\"Target:    {y_true}\")\n",
    "            print(f\"Predicted: {y_pred}\")\n",
    "        print(\"-\" * 129)\n",
    "\n",
    "    def fit(self):\n",
    "        # Checkpoint manager\n",
    "        self.ckpt_dir = f\"{self.args.main_dir}/checkpoints\"\n",
    "        self.ckpt = tf.train.Checkpoint(self.model)\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(\n",
    "            checkpoint=self.ckpt, directory=self.ckpt_dir, max_to_keep=5)\n",
    "\n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            self.start_epoch = int(self.ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "            self.ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "            print(f\"Resuming from epoch {self.start_epoch + 1}...\")\n",
    "        else:\n",
    "            self.start_epoch = 0\n",
    "            print(\"Starting from epoch 1...\")\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.args.epochs+1):\n",
    "            print(f\"Epoch {epoch+1}/{self.args.epochs}: Learning rate @ {self.optimizer.lr(epoch):.2e}\")\n",
    "            stateful_metrics = [\"loss\", \"per\", \"cer\", \"val_loss\", \"val_per\", \"val_cer\"]\n",
    "            progbar = tf.keras.utils.Progbar(\n",
    "                self.args.train_steps, interval=0.05,\n",
    "                stateful_metrics=stateful_metrics)\n",
    "\n",
    "            # Training loop\n",
    "            for step, t_batch in enumerate(self.train_dataset):\n",
    "                t_inputs = t_batch['input_values']\n",
    "                t_labels = t_batch['labels']\n",
    "                with tf.GradientTape() as tape:\n",
    "                    t_loss, t_logits = self.model(\n",
    "                        input_values=t_inputs, labels=t_labels, training=True)[:2]\n",
    "                gradients = tape.gradient(t_loss, self.model.trainable_weights)  \n",
    "                self.optimizer.apply_gradients(zip(gradients, self.model.trainable_weights))  \n",
    "                t_labels = tf.where(t_labels == -100, x=0, y=t_labels)\n",
    "                t_logits = tf.argmax(t_logits, axis=-1)\n",
    "                self.per_metrics.update_state(t_labels, t_logits)\n",
    "                t_labels, t_logits = self.decoder(t_labels, t_logits)\n",
    "                self.cer_metrics.update_state(t_labels, t_logits) \n",
    "                t_per = self.per_metrics.result()\n",
    "                t_cer = self.cer_metrics.result()\n",
    "                t_values = [(\"loss\", t_loss), (\"per\", t_per), (\"cer\", t_cer)]\n",
    "                progbar.update(step, values=t_values, finalize=False)\n",
    "            self.per_metrics.reset_states()\n",
    "            self.cer_metrics.reset_states()\n",
    "            \n",
    "            # Validation loop\n",
    "            for v_batch in self.val_dataset:\n",
    "                v_inputs = v_batch['input_values']\n",
    "                v_labels = v_batch['labels']\n",
    "                v_loss, v_logits = self.model(\n",
    "                    input_values=v_inputs, labels=v_labels, training=False)[:2]  \n",
    "                v_labels = tf.where(v_labels == -100, x=0, y=v_labels)\n",
    "                v_logits = tf.argmax(v_logits, axis=-1)     \n",
    "                self.per_metrics.update_state(v_labels, v_logits)\n",
    "                v_labels, v_logits = self.decoder(v_labels, v_logits)         \n",
    "                self.cer_metrics.update_state(v_labels, v_logits)\n",
    "\n",
    "            v_per = self.per_metrics.result()\n",
    "            v_cer = self.cer_metrics.result()\n",
    "            v_values = [\n",
    "                (\"loss\", t_loss), (\"per\", t_per), (\"cer\", t_cer), (\"val_loss\", v_loss),\n",
    "                (\"val_per\", v_per), (\"val_cer\", v_cer)]\n",
    "            progbar.update(self.args.train_steps, values=v_values, finalize=True)\n",
    "            self.per_metrics.reset_states()\n",
    "            self.cer_metrics.reset_states()\n",
    "\n",
    "            # Print sample transcriptions for both loops\n",
    "            self.display(t_labels, t_logits, v_labels, v_logits)\n",
    "\n",
    "            # Checkpointing\n",
    "            self.ckpt.save(file_prefix=f\"{self.ckpt_dir}/{self.model_name}\")\n",
    "\n",
    "            # Logging\n",
    "            log = f\"{epoch+1},{t_loss},{t_per},{t_cer},{v_loss},{v_per},{v_cer}\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(log)\n",
    "\n",
    "            save_path = f\"{self.args.main_dir}/model_weights\"\n",
    "            self.model.save_weights(f\"{save_path}/{self.model_name}_{epoch+1}.h5\")\n",
    "\n",
    "Trainer(args).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = pd.read_csv(\"E:\\Datasets\\ASR-dataset\\model_weights\\model_40k.csv\", index_col=\"epoch\")\n",
    "# history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "# sns.lineplot(x=history.index, y=history['per'], label=\"per\", ax=ax1)\n",
    "# sns.lineplot(x=history.index, y=history['wer'], label=\"wer\", ax=ax2)\n",
    "# sns.lineplot(x=history.index, y=history['val_per'], label=\"val_per\", ax=ax1)\n",
    "# sns.lineplot(x=history.index, y=history['val_wer'], label=\"val_wer\", ax=ax2)\n",
    "# plt.suptitle(\"Acoustic model\")\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"acoustic_history.png\", transparent=False, bbox_inches='tight')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b851d2923cfa3a2562599062e05fd9893d86a7c009c64d8ad3756552e4dd5f41"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('tf-gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
