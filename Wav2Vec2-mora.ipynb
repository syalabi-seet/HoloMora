{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import MeCab\n",
    "import cutlet\n",
    "import jiwer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    TFWav2Vec2ForCTC,\n",
    "    Wav2Vec2Config,\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    GradientAccumulator,\n",
    "    logging)\n",
    "\n",
    "def seed_everything(SEED):\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    print(\"Random seed set.\")\n",
    "\n",
    "seed_everything(42)\n",
    "tf.get_logger().setLevel('FATAL')\n",
    "logging.set_verbosity_error()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(accum_steps=4, batch_size=4, buffer_size=1024, epochs=15, learning_rate=5e-05, lr_max=5e-05, lr_min=1e-08, lr_start=1e-08, main_dir='E://Datasets/ASR-dataset', model_name='facebook/wav2vec2-base', n_cycles=0.5, n_samples=40000, n_shards=20, n_train=36000, n_val=4000, random_state=42, sample_rate=16000, sustain_epochs=0, test_size=0.1, train_steps=9000, val_steps=1000, vocab_size=38, warmup_epochs=2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ArgParser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # DataLoader\n",
    "    parser.add_argument(\"--main_dir\", default=\"E://Datasets/ASR-dataset\")\n",
    "    parser.add_argument(\"--sample_rate\", default=16000)\n",
    "    parser.add_argument(\"--test_size\", default=0.1)\n",
    "    parser.add_argument(\"--random_state\", default=42)\n",
    "    parser.add_argument(\"--batch_size\", default=4)\n",
    "    parser.add_argument(\"--n_shards\", default=20)\n",
    "    parser.add_argument(\"--buffer_size\", default=1024)\n",
    "    parser.add_argument(\"--n_samples\", default=40000)\n",
    "\n",
    "    # Trainer\n",
    "    parser.add_argument(\"--model_name\", default=\"facebook/wav2vec2-base\")\n",
    "    parser.add_argument(\"--epochs\", default=15)\n",
    "    parser.add_argument(\"--accum_steps\", default=4)\n",
    "\n",
    "    # Scheduler\n",
    "    parser.add_argument(\"--learning_rate\", default=5e-5)\n",
    "    parser.add_argument(\"--lr_start\", default=1e-8)\n",
    "    parser.add_argument(\"--lr_min\", default=1e-8)\n",
    "    parser.add_argument(\"--lr_max\", default=5e-5)\n",
    "    parser.add_argument(\"--n_cycles\", default=0.5)\n",
    "    parser.add_argument(\"--warmup_epochs\", default=2)\n",
    "    parser.add_argument(\"--sustain_epochs\", default=0)\n",
    "\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    with open(f\"{args.main_dir}/vocab.json\", \"r\") as f:\n",
    "        vocab_size = len(json.load(f))\n",
    "   \n",
    "    n_train = int(args.n_samples * (1 - args.test_size))\n",
    "    n_val = int(args.n_samples * args.test_size)\n",
    "    train_steps = int(np.ceil(n_train / args.batch_size))\n",
    "    val_steps = int(np.ceil(n_val / args.batch_size))\n",
    "\n",
    "    parser.add_argument(\"--vocab_size\", default=vocab_size)\n",
    "    parser.add_argument(\"--n_train\", default=n_train)\n",
    "    parser.add_argument(\"--n_val\", default=n_val)\n",
    "    parser.add_argument(\"--train_steps\", default=train_steps)  \n",
    "    parser.add_argument(\"--val_steps\", default=val_steps)\n",
    "    \n",
    "    return parser.parse_known_args()[0]\n",
    "\n",
    "args = ArgParser()\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.data = pd.concat([\n",
    "            self.get_kokoro(),\n",
    "            self.get_jsut(),\n",
    "            self.get_commonvoice()], \n",
    "            ignore_index=True)\n",
    "        self.katsu = cutlet.Cutlet()\n",
    "        self.katsu.use_foreign_spelling = False\n",
    "    \n",
    "        tqdm.pandas()\n",
    "        self.data['sentence'] = self.data['sentence'].progress_apply(self.clean_kanji)\n",
    "        self.data['romaji'] = self.data['sentence'].progress_apply(self.kanji2romaji)\n",
    "        self.data['length'] = self.data['path'].progress_apply(self.get_length)\n",
    "        self.data = self.data[self.data['sentence'].apply(list).apply(len)>=5]\n",
    "        self.data.query(\"(length >= 48000) & (length <= 80000)\", inplace=True)\n",
    "        self.data = self.data.dropna().reset_index(drop=True)\n",
    "        self.data = self.data.sample(n=self.args.n_samples, random_state=42, ignore_index=True)\n",
    "        self.data.sort_values(by=\"length\", axis=0, ascending=True, inplace=True, ignore_index=True)\n",
    "        self.data.to_csv(f\"{self.args.main_dir}/ASRDataset.csv\", encoding=\"utf-8\", index=False)\n",
    "\n",
    "    def get_kokoro(self):\n",
    "        in_dir = \"Datasets\\KOKORO-dataset\"\n",
    "\n",
    "        data = []\n",
    "        transcript_path = f\"{in_dir}/transcripts/*.metadata.txt\"\n",
    "        for transcript in glob.glob(transcript_path):\n",
    "            with open(transcript, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f.readlines():\n",
    "                    data.append(line.split(\"|\"))\n",
    "\n",
    "        data = pd.DataFrame(\n",
    "            data, columns=[\n",
    "                'text_id', 'path', 'start_idx', \n",
    "                'end_idx', 'sentence', 'phonemes'])       \n",
    "\n",
    "        # paths = data['path'].unique()\n",
    "        # for path in tqdm(paths, total=len(paths)):\n",
    "        #     folder_name = path.split(\"_\", 1)[0]\n",
    "        #     in_path = os.path.join(in_dir, folder_name, path)\n",
    "        #     y, sr = librosa.load(in_path, sr=None)\n",
    "        #     for text_id in data.loc[data['path']==path, 'text_id']:\n",
    "        #         out_path = os.path.join(self.args.main_dir, 'wav_cleaned', text_id) + \".wav\"\n",
    "        #         if not os.path.exists(out_path):\n",
    "        #             start_idx = int(data.loc[data['text_id']==text_id, 'start_idx'].item())\n",
    "        #             end_idx = int(data.loc[data['text_id']==text_id, 'end_idx'].item())\n",
    "        #             y_slice = librosa.resample(\n",
    "        #                 y[start_idx:end_idx], orig_sr=sr, target_sr=self.sample_rate)\n",
    "        #             sf.write(out_path, y_slice, samplerate=self.sample_rate, subtype='PCM_16')\n",
    "\n",
    "        data = data[['text_id', 'sentence']]\n",
    "        data['text_id'] = data['text_id'].apply(lambda x: x + \".wav\")\n",
    "        data.columns = ['path', 'sentence']\n",
    "        data['corpus'] = ['kokoro'] * len(data)\n",
    "        return data\n",
    "\n",
    "    def get_jsut(self):\n",
    "        filenames, sentences = [], []\n",
    "        for transcript in glob.glob(r\"Datasets/JSUT-dataset/*/transcript_utf8.txt\"):\n",
    "            file_path = transcript.rsplit(\"\\\\\", 1)[0]\n",
    "            with open(transcript, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines: \n",
    "                    filename, sentence = line.split(\":\")\n",
    "                    filenames.append(os.path.join(file_path, \"wav\", filename) + \".wav\")\n",
    "                    sentences.append(sentence.strip(\"\\n\"))\n",
    "        data = pd.DataFrame({'path': filenames, 'sentence': sentences}) \n",
    "        data['corpus'] = ['jsut'] * len(data)\n",
    "        for i, in_path in tqdm(enumerate(data['path']), total=len(data['path'])):\n",
    "            in_path = in_path.replace(\"\\\\\", \"/\")\n",
    "            out_path = f\"{self.args.main_dir}\\wav_cleaned\"\n",
    "            filename = in_path.rsplit(\"/\", 1)[-1]\n",
    "            out_path = os.path.join(out_path, filename)\n",
    "            if not os.path.exists(out_path):\n",
    "                subprocess.call([\n",
    "                    \"ffmpeg\", \"-i\", in_path,\"-acodec\", \"pcm_s16le\", \n",
    "                    \"-ar\", str(self.sample_rate), out_path])\n",
    "            data['path'][i] = filename\n",
    "        return data\n",
    "\n",
    "    def get_commonvoice(self):\n",
    "        data = pd.read_csv(r\"Datasets/CommonVoice-dataset/validated.tsv\", sep=\"\\t\")\n",
    "        data = data[['path', 'sentence']]    \n",
    "        data['path'] = data['path'].apply(\n",
    "            lambda x: r\"Datasets/CommonVoice-dataset/mp3/\" + x)\n",
    "        data['corpus'] = ['common_voice'] * len(data)\n",
    "        for i, in_path in tqdm(enumerate(data['path']), total=len(data['path'])):\n",
    "            in_path = in_path.replace(\"\\\\\", \"/\")\n",
    "            out_path = f\"{self.args.main_dir}\\wav_cleaned\"\n",
    "            filename = in_path.rsplit(\"/\", 1)[-1]\n",
    "            filename = filename.replace(\"mp3\", \"wav\")\n",
    "            out_path = os.path.join(out_path, filename)\n",
    "            if not os.path.exists(out_path):\n",
    "                subprocess.call([\n",
    "                    \"ffmpeg\", \"-i\", in_path,\"-acodec\", \"pcm_s16le\", \n",
    "                    \"-ar\", str(self.sample_rate), out_path])\n",
    "            data['path'][i] = filename\n",
    "        return data\n",
    "    \n",
    "    def clean_kanji(self, sentence):\n",
    "        symbols = r\"\\（.*\\）|\\(.*\\)|\\「.*\\」|\\『.*\\』\"\n",
    "        sentence = re.sub(symbols, \"\", sentence.strip())\n",
    "        return sentence\n",
    "\n",
    "    def kanji2romaji(self, sentence):\n",
    "        try:\n",
    "            new_line = self.katsu.romaji(sentence)\n",
    "            new_line = self.clean_romaji(new_line)\n",
    "        except:\n",
    "            new_line = None\n",
    "        return new_line\n",
    "\n",
    "    def clean_romaji(self, sentence):\n",
    "        sentence = sentence.strip().lower()\n",
    "        sentence = re.sub(r\"[^a-z0-9\\ ]\", \"\", sentence)\n",
    "        sentence = sentence.split()\n",
    "        for i, mora in enumerate(sentence):\n",
    "            if (mora == \"n\") | (mora == \"u\") & (i < len(sentence) - 1):\n",
    "                prev_mora = sentence.pop(i-1)\n",
    "                sentence[i-1] = \"\".join([prev_mora, mora])\n",
    "        sentence = \"|\".join(sentence)\n",
    "        return sentence\n",
    "\n",
    "    def get_length(self, path):\n",
    "        path = os.path.join(self.args.main_dir, 'wav_cleaned', path)\n",
    "        y, sr = librosa.load(path, sr=None)\n",
    "        return len(y)\n",
    "\n",
    "# data = Dataset(args).data\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 4))\n",
    "# sns.histplot(x=data['length'], hue=data['corpus'], palette=\"bright\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, args):\n",
    "        tokenizer = Wav2Vec2CTCTokenizer(\n",
    "            vocab_file=f\"{args.main_dir}/vocab.json\",\n",
    "            do_lower_case=False\n",
    "        )\n",
    "\n",
    "        feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "            feature_size=1,\n",
    "            sampling_rate=args.sample_rate,\n",
    "            padding_value=0.0,\n",
    "            do_normalize=True,\n",
    "            return_attention_mask=False\n",
    "        )\n",
    "\n",
    "        self.processor = Wav2Vec2Processor(\n",
    "            feature_extractor=feature_extractor,\n",
    "            tokenizer=tokenizer\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFRWriter():\n",
    "    def __init__(self, args):\n",
    "        self.data = pd.read_csv(os.path.join(args.main_dir, \"ASRDataset.csv\"), encoding=\"utf-8\")\n",
    "        self.args = args\n",
    "        self.processor = Config(args).processor\n",
    "\n",
    "    def _bytes_feature(self, value):\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    def _int64_feature(self, value):\n",
    "        \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "    def _float_feature(self, value):\n",
    "        \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "    def serialize_example(self, *args):\n",
    "        feature = {\n",
    "            'input_values': self._bytes_feature(args[0]),\n",
    "            'labels': self._bytes_feature(args[1])}\n",
    "\n",
    "        example_proto = tf.train.Example(\n",
    "            features=tf.train.Features(feature=feature))\n",
    "        return example_proto.SerializeToString()\n",
    "\n",
    "    def get_labels(self, sample):\n",
    "        labels = self.data.loc[self.data['path']==sample, \"romaji\"].item()\n",
    "        labels = (self.processor.tokenizer.bos_token + labels + \n",
    "            self.processor.tokenizer.eos_token)\n",
    "        with self.processor.as_target_processor():\n",
    "            labels = self.processor(labels, is_split_into_words=True).input_ids\n",
    "        return tf.convert_to_tensor(labels, dtype=tf.int32)\n",
    "\n",
    "    def get_audio(self, sample):\n",
    "        path = os.path.join(self.args.main_dir, \"wav_cleaned\", sample)\n",
    "        audio = librosa.load(path, sr=None)[0]\n",
    "        audio /= audio.max()\n",
    "        return tf.convert_to_tensor(audio, dtype=tf.float32)\n",
    "\n",
    "    def get_shards(self):\n",
    "        skf = KFold(n_splits=self.args.n_shards, shuffle=False)\n",
    "        return [\n",
    "            list(map(lambda x: self.data['path'][x], j))\n",
    "            for i, j in skf.split(self.data['path'])]\n",
    "\n",
    "    def get_shard_data(self, samples):\n",
    "        for sample in samples:\n",
    "            audio = self.get_audio(sample)\n",
    "            labels = self.get_labels(sample)\n",
    "            yield {\n",
    "                'input_values': tf.io.serialize_tensor(audio),\n",
    "                'labels': tf.io.serialize_tensor(labels)}\n",
    "\n",
    "    def write(self):\n",
    "        for shard, samples in tqdm(enumerate(self.get_shards()), total=self.args.n_shards):\n",
    "            with tf.io.TFRecordWriter(f\"{self.args.main_dir}/wav2vec2_tfrec/shard_{shard+1}.tfrec\") as f:\n",
    "                for sample in self.get_shard_data(samples):\n",
    "                    example = self.serialize_example(\n",
    "                        sample['input_values'],\n",
    "                        sample['labels'])\n",
    "                    f.write(example)\n",
    "\n",
    "# TFRWriter(args).write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, args):\n",
    "        self.files = glob.glob(args.main_dir + \"/wav2vec2_tfrec/*.tfrec\")\n",
    "        self.args = args\n",
    "        self.AUTOTUNE = tf.data.AUTOTUNE\n",
    "        self.train_files, self.val_files = train_test_split(\n",
    "            self.files, test_size=args.test_size, shuffle=True, \n",
    "            random_state=args.random_state)\n",
    "        self.train = self.get_train()\n",
    "        self.val = self.get_val()\n",
    "\n",
    "    def read_tfrecord(self, example):\n",
    "        feature_description = {\n",
    "            'input_values': tf.io.FixedLenFeature([], tf.string),\n",
    "            'labels': tf.io.FixedLenFeature([], tf.string)}\n",
    "        \n",
    "        example = tf.io.parse_single_example(example, feature_description)\n",
    "        example['input_values'] = tf.io.parse_tensor(\n",
    "            example['input_values'], out_type=tf.float32)\n",
    "        example['labels'] = tf.io.parse_tensor(\n",
    "            example['labels'], out_type=tf.int32)\n",
    "        return example\n",
    "\n",
    "    def load_dataset(self, files):\n",
    "        ignore_order = tf.data.Options()\n",
    "        ignore_order.experimental_deterministic = False\n",
    "        dataset = tf.data.TFRecordDataset(files)\n",
    "        dataset = dataset.with_options(ignore_order)\n",
    "        dataset = dataset.map(self.read_tfrecord, num_parallel_calls=self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_train(self):\n",
    "        dataset = self.load_dataset(self.train_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_values': [None],\n",
    "                'labels': [None]\n",
    "            },\n",
    "            padding_values={\n",
    "                'input_values': tf.constant(0, dtype=tf.float32), \n",
    "                'labels': tf.constant(-100, dtype=tf.int32)\n",
    "            })        \n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_val(self):\n",
    "        dataset = self.load_dataset(self.val_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_values': [None],\n",
    "                'labels': [None]\n",
    "            },\n",
    "            padding_values={\n",
    "                'input_values': tf.constant(0, dtype=tf.float32),\n",
    "                'labels': tf.constant(-100, dtype=tf.int32)\n",
    "            })\n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "# train = DataLoader(args).train\n",
    "# next(iter(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16,10))\n",
    "# for i, array in enumerate(train.take(16)):\n",
    "#     plt.subplot(4, 4, i+1)\n",
    "#     y = array['input_values'].numpy()\n",
    "#     librosa.display.waveplot(y=y, sr=16000)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class PERMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, tokenizer, name=\"PER\", **kwargs):\n",
    "        super(PERMetric, self).__init__(name=name,  **kwargs)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.accumulator = self.add_weight(name=\"total_per\", initializer=\"zeros\")\n",
    "        self.counter = self.add_weight(name=\"per_count\", initializer=\"zeros\")    \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Re-encode logits\n",
    "        try:\n",
    "            y_pred = self.tokenizer(y_pred, is_split_into_words=False).input_ids\n",
    "        except:\n",
    "            y_pred = [[self.tokenizer.pad_token_id] for _ in range(len(y_pred))]\n",
    "\n",
    "        hypothesis = tf.ragged.constant(y_pred).to_sparse()\n",
    "\n",
    "        # Convert dense to sparse tensor for edit_distance function\n",
    "        truth = tf.RaggedTensor.from_tensor(y_true, padding=0).to_sparse()\n",
    "\n",
    "        # Calculate Levenshtein distance\n",
    "        distance = tf.edit_distance(hypothesis, truth, normalize=True)\n",
    "\n",
    "        # Add distance and number of samples to variables\n",
    "        self.accumulator.assign_add(tf.reduce_mean(distance))\n",
    "        self.counter.assign_add(len(y_true))\n",
    "\n",
    "    def result(self):\n",
    "        # Divides accumulated distance scores against number of samples passed,\n",
    "        # mimics mean reduction over batch\n",
    "        return tf.math.divide_no_nan(self.accumulator, self.counter)   \n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.accumulator.assign(0.0)\n",
    "        self.counter.assign(0.0)\n",
    "\n",
    "class WERMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"WER\", **kwargs):\n",
    "        super(WERMetric, self).__init__(name=name,  **kwargs)\n",
    "        self.accumulator = self.add_weight(name=\"total_wer\", initializer=\"zeros\")\n",
    "        self.counter = self.add_weight(name=\"wer_count\", initializer=\"zeros\")    \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "\n",
    "        wer = jiwer.wer(y_true, y_pred)\n",
    "\n",
    "        # Add distance and number of batches to variables\n",
    "        self.accumulator.assign_add(wer)\n",
    "        self.counter.assign_add(1)\n",
    "\n",
    "    def result(self):\n",
    "        # Divides accumulated distance scores against number of batches passed,\n",
    "        # mimics mean reduction over batch\n",
    "        return tf.math.divide_no_nan(self.accumulator, self.counter)   \n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.accumulator.assign(0.0)\n",
    "        self.counter.assign(0.0)\n",
    "\n",
    "class CosineDecayWithWarmup(LearningRateSchedule):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "    def __call__(self, epoch):  \n",
    "        if epoch < self.args.warmup_epochs:\n",
    "            lr = ((self.args.lr_max - self.args.lr_start) / self.args.warmup_epochs) * epoch + self.args.lr_start\n",
    "        elif epoch < (self.args.warmup_epochs + self.args.sustain_epochs):\n",
    "            lr = self.args.lr_max\n",
    "        else:\n",
    "            progress = ((epoch - self.args.warmup_epochs - self.args.sustain_epochs) / \n",
    "            (self.args.epochs - self.args.warmup_epochs - self.args.sustain_epochs))\n",
    "            lr = (self.args.lr_max-self.args.lr_min) * (0.5 * (1.0 + tf.math.cos((22/7) * \n",
    "                self.args.n_cycles * 2.0 * progress)))\n",
    "            if self.args.lr_min is not None:\n",
    "                lr = tf.math.maximum(self.args.lr_min, lr)\n",
    "        return lr\n",
    "\n",
    "    def plot(self):\n",
    "        epochs = range(self.args.epochs+1)\n",
    "        lr = [self(epoch) for epoch in epochs]\n",
    "        plt.plot(epochs, lr)\n",
    "        plt.xlabel(\"learning_rate\")\n",
    "        plt.ylabel(\"epochs\")\n",
    "        plt.show()\n",
    "\n",
    "CosineDecayWithWarmup(args).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\transformers\\configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from epoch 6...\n",
      "Epoch 6/15: Learning rate @ 4.37e-05\n",
      "5906/9000 [==================>...........] - ETA: 1:33:51 - loss: 22.0199 - per: 0.0229 - wer: 0.2619"
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.config = Config(args)\n",
    "        self.train_dataset = DataLoader(args).train\n",
    "        self.val_dataset = DataLoader(args).val\n",
    "        schedule = CosineDecayWithWarmup(args)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(schedule)\n",
    "        self.gradient_accumulator = GradientAccumulator()\n",
    "        self.gradient_accumulator.accum_steps = args.accum_steps\n",
    "        self.per_metrics = PERMetric(tokenizer=self.config.processor.tokenizer)\n",
    "        self.wer_metrics = WERMetric()\n",
    "        self.model = TFWav2Vec2ForCTC.from_pretrained(\n",
    "            args.model_name,\n",
    "            from_pt=True,\n",
    "            ctc_loss_reduction=\"mean\",\n",
    "            gradient_checkpointing=True,\n",
    "            pad_token_id=self.config.processor.tokenizer.pad_token_id,\n",
    "            vocab_size=len(self.config.processor.tokenizer),\n",
    "            use_bfloat16=True)\n",
    "        self.model.freeze_feature_extractor()\n",
    "        \n",
    "        self.model_name = f\"model_{int(self.args.n_samples/1000)}k\"\n",
    "        self.log_path = f\"{self.args.main_dir}/model_weights/{self.model_name}.csv\"\n",
    "        if not os.path.exists(self.log_path):\n",
    "            print(\"Log file created.\")\n",
    "            columns = \"epoch,loss,per,wer,val_loss,val_per,val_wer\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(columns)\n",
    "\n",
    "    def decoder(self, labels, logits):\n",
    "        labels = tf.where(labels < 0, x=0, y=labels)\n",
    "        logits = tf.argmax(logits, axis=-1)\n",
    "        logits = self.config.processor.batch_decode(logits, group_tokens=True)\n",
    "        return labels, logits\n",
    "\n",
    "    def display(self, epoch, t_labels, t_logits, v_labels, v_logits):\n",
    "        print(\"-\" * 129)\n",
    "        print(\"Training\")\n",
    "        for y_true, y_pred in zip(t_labels, t_logits):\n",
    "            print(f\"Target:    {y_true}\")\n",
    "            print(f\"Predicted: {y_pred}\") \n",
    "\n",
    "        print(\"\\nValidation\")\n",
    "        for y_true, y_pred in zip(v_labels, v_logits):\n",
    "            print(f\"Target:    {y_true}\")\n",
    "            print(f\"Predicted: {y_pred}\")\n",
    "        print(\"-\" * 129)\n",
    "\n",
    "    def fit(self):\n",
    "        # Checkpoint manager\n",
    "        self.ckpt_dir = f\"{self.args.main_dir}/checkpoints\"\n",
    "        self.ckpt = tf.train.Checkpoint(self.model)\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(\n",
    "            checkpoint=self.ckpt, directory=self.ckpt_dir, max_to_keep=5)\n",
    "\n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            self.start_epoch = int(self.ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "            self.ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "            print(f\"Resuming from epoch {self.start_epoch + 1}...\")\n",
    "        else:\n",
    "            self.start_epoch = 0\n",
    "            print(\"Starting from epoch 1...\")\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.args.epochs):\n",
    "            print(f\"Epoch {epoch+1}/{self.args.epochs}: Learning rate @ {self.optimizer.lr(epoch):.2e}\")\n",
    "            stateful_metrics = [\"loss\", \"per\", \"wer\", \"val_loss\", \"val_per\", \"val_wer\"]\n",
    "            progbar = tf.keras.utils.Progbar(\n",
    "                self.args.train_steps, interval=0.05,\n",
    "                stateful_metrics=stateful_metrics)\n",
    "\n",
    "            # Training loop\n",
    "            for step, t_batch in enumerate(self.train_dataset):\n",
    "                t_inputs = t_batch['input_values']\n",
    "                t_labels = t_batch['labels']\n",
    "                with tf.GradientTape() as tape:\n",
    "                    t_loss, t_logits = self.model(\n",
    "                        input_values=t_inputs, labels=t_labels, training=True)[:2]\n",
    "                self.gradient_accumulator(tape.gradient(\n",
    "                    t_loss, self.model.trainable_weights))\n",
    "                self.optimizer.apply_gradients(zip(\n",
    "                    self.gradient_accumulator.gradients, \n",
    "                    self.model.trainable_weights))\n",
    "                self.gradient_accumulator.reset()             \n",
    "                t_labels, t_logits = self.decoder(t_labels, t_logits)\n",
    "                self.per_metrics.update_state(t_labels, t_logits)\n",
    "                t_labels = self.config.processor.batch_decode(t_labels, group_tokens=False)      \n",
    "                self.wer_metrics.update_state(t_labels, t_logits)\n",
    "                t_per = self.per_metrics.result()\n",
    "                t_wer = self.wer_metrics.result()\n",
    "                t_values = [(\"loss\", t_loss), (\"per\", t_per), (\"wer\", t_wer)]\n",
    "                progbar.update(step, values=t_values, finalize=False)\n",
    "\n",
    "            self.per_metrics.reset_states()\n",
    "            self.wer_metrics.reset_states()\n",
    "            \n",
    "            # Validation loop\n",
    "            for v_batch in self.val_dataset:\n",
    "                v_inputs = v_batch['input_values']\n",
    "                v_labels = v_batch['labels']\n",
    "                v_loss, v_logits = self.model(\n",
    "                    input_values=v_inputs, labels=v_labels, training=False)[:2]       \n",
    "                v_labels, v_logits = self.decoder(v_labels, v_logits)               \n",
    "                self.per_metrics.update_state(v_labels, v_logits)\n",
    "                v_labels = self.config.processor.batch_decode(v_labels, group_tokens=False)\n",
    "                self.wer_metrics.update_state(v_labels, v_logits)\n",
    "\n",
    "            v_per = self.per_metrics.result()\n",
    "            v_wer = self.wer_metrics.result()\n",
    "            v_values = [\n",
    "                (\"loss\", t_loss), (\"per\", t_per), (\"wer\", t_wer), (\"val_loss\", v_loss),\n",
    "                (\"val_per\", v_per), (\"val_wer\", v_wer)]\n",
    "            progbar.update(self.args.train_steps, values=v_values, finalize=True)\n",
    "            self.per_metrics.reset_states()\n",
    "            self.wer_metrics.reset_states()\n",
    "\n",
    "            # Print sample transcriptions for both loops\n",
    "            self.display(epoch, t_labels, t_logits, v_labels, v_logits)\n",
    "\n",
    "            # Checkpointing\n",
    "            self.ckpt.save(file_prefix=f\"{self.ckpt_dir}/{self.model_name}\")\n",
    "\n",
    "            # Logging\n",
    "            log = f\"{epoch+1},{t_loss},{t_per},{t_wer},{v_loss},{v_per},{v_wer}\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(log)\n",
    "\n",
    "            save_path = f\"{self.args.main_dir}/model_weights\"\n",
    "            self.model.save_weights(f\"{save_path}/{self.model_name}_{epoch+1}.h5\")\n",
    "\n",
    "Trainer(args).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pd.read_csv(\"E:\\Datasets\\ASR-dataset\\model_weights\\model_40k.csv\", index_col=\"epoch\")\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "sns.lineplot(x=history.index, y=history['per'], label=\"per\", ax=ax1)\n",
    "sns.lineplot(x=history.index, y=history['wer'], label=\"wer\", ax=ax2)\n",
    "sns.lineplot(x=history.index, y=history['val_per'], label=\"val_per\", ax=ax1)\n",
    "sns.lineplot(x=history.index, y=history['val_wer'], label=\"val_wer\", ax=ax2)\n",
    "plt.suptitle(\"Acoustic model\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"acoustic_history.png\", transparent=False, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b851d2923cfa3a2562599062e05fd9893d86a7c009c64d8ad3756552e4dd5f41"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('tf-gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
