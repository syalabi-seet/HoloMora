{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "from convert_romaji import Romaji2Kana\n",
    "\n",
    "import cutlet\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "\n",
    "from transformers import (\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2Processor,\n",
    "    TFWav2Vec2ForCTC,\n",
    "    logging)\n",
    "\n",
    "from convert_romaji import Romaji2Kana\n",
    "\n",
    "def seed_everything(SEED):\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    print(\"Random seed set.\")\n",
    "\n",
    "seed_everything(42)\n",
    "tf.get_logger().setLevel('FATAL')\n",
    "logging.set_verbosity_error()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=4, buffer_size=512, epochs=30, learning_rate=1e-05, lr_max=1e-05, lr_min=1e-10, lr_start=1e-08, main_dir='E://Datasets/Acoustic_model', model_name='facebook/wav2vec2-base', n_cycles=0.5, n_samples=50000, n_shards=40, n_train=45000, n_val=5000, random_state=42, sample_rate=16000, sustain_epochs=0, test_size=0.1, train_steps=11250, val_steps=1250, vocab_size=37, warmup_epochs=5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ArgParser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # DataLoader\n",
    "    parser.add_argument(\"--main_dir\", default=\"E://Datasets/Acoustic_model\")\n",
    "    parser.add_argument(\"--sample_rate\", default=16000)\n",
    "    parser.add_argument(\"--test_size\", default=0.1)\n",
    "    parser.add_argument(\"--random_state\", default=42)\n",
    "    parser.add_argument(\"--batch_size\", default=4)\n",
    "    parser.add_argument(\"--n_shards\", default=40)\n",
    "    parser.add_argument(\"--buffer_size\", default=512)\n",
    "    parser.add_argument(\"--n_samples\", default=50000)\n",
    "\n",
    "    # Trainer\n",
    "    parser.add_argument(\"--model_name\", default=\"facebook/wav2vec2-base\")\n",
    "    parser.add_argument(\"--epochs\", default=30)\n",
    "\n",
    "    # Scheduler\n",
    "    parser.add_argument(\"--learning_rate\", default=1e-5)\n",
    "    parser.add_argument(\"--lr_start\", default=1e-8)\n",
    "    parser.add_argument(\"--lr_min\", default=1e-10)\n",
    "    parser.add_argument(\"--lr_max\", default=1e-5)\n",
    "    parser.add_argument(\"--n_cycles\", default=0.5)\n",
    "    parser.add_argument(\"--warmup_epochs\", default=5)\n",
    "    parser.add_argument(\"--sustain_epochs\", default=0)\n",
    "\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    with open(f\"{args.main_dir}/vocab.json\", \"r\") as f:\n",
    "        vocab_size = len(json.load(f))\n",
    "   \n",
    "    n_train = int(args.n_samples * (1 - args.test_size))\n",
    "    n_val = int(args.n_samples * args.test_size)\n",
    "    train_steps = int(np.ceil(n_train / args.batch_size))\n",
    "    val_steps = int(np.ceil(n_val / args.batch_size))\n",
    "\n",
    "    parser.add_argument(\"--vocab_size\", default=vocab_size)\n",
    "    parser.add_argument(\"--n_train\", default=n_train)\n",
    "    parser.add_argument(\"--n_val\", default=n_val)\n",
    "    parser.add_argument(\"--train_steps\", default=train_steps)  \n",
    "    parser.add_argument(\"--val_steps\", default=val_steps)\n",
    "    \n",
    "    return parser.parse_known_args()[0]\n",
    "\n",
    "args = ArgParser()\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.data_dir = \"E:/Datasets/Acoustic_model/raw_data\"\n",
    "        self.data = pd.concat([\n",
    "            self.get_kokoro(),\n",
    "            self.get_jsut(),\n",
    "            self.get_commonvoice()], \n",
    "            ignore_index=True)\n",
    "        self.kanji_unicode = self.get_kanji_unicode()\n",
    "        self.katsu = cutlet.Cutlet()\n",
    "        self.katsu.use_foreign_spelling = False\n",
    "    \n",
    "        tqdm.pandas()\n",
    "        # Remove rows that contains non-kanji characters\n",
    "        self.data = self.data[self.data['sentence'].progress_apply(self.check_kanji)] \n",
    "\n",
    "        # Remove words within parenthesis\n",
    "        parenthesis =  r\"\\（.*\\）|\\(.*\\)|\\「.*\\」|\\『.*\\』\"\n",
    "        self.data = self.data[~self.data['sentence'].str.contains(parenthesis)]\n",
    "\n",
    "        # Remove punctuations from sentences\n",
    "        self.data['sentence'] = self.data['sentence'].progress_apply(self.clean_kanji)\n",
    "        self.data['romaji'] = self.data['sentence'].progress_apply(self.kanji2romaji)\n",
    "        self.data['length'] = self.data['path'].progress_apply(self.get_length)\n",
    "        self.data = self.data[self.data['length'].between(48000, 100000)]\n",
    "        self.data = self.data.reset_index(drop=True)\n",
    "        self.data = self.data.sample(n=self.args.n_samples, random_state=42, ignore_index=True)\n",
    "        self.data.sort_values(by=\"length\", axis=0, ascending=True, inplace=True, ignore_index=True)\n",
    "        self.data.to_csv(\n",
    "            f\"{self.args.main_dir}/ASRDataset.csv\", \n",
    "            encoding=\"utf-8\", index=False)\n",
    "\n",
    "    def get_kokoro(self):\n",
    "        data = []\n",
    "        transcript_path = f\"{self.data_dir}/KOKORO-dataset/transcripts/*.metadata.txt\"\n",
    "        for transcript in glob.glob(transcript_path):\n",
    "            with open(transcript, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f.readlines():\n",
    "                    data.append(line.split(\"|\"))\n",
    "\n",
    "        data = pd.DataFrame(\n",
    "            data, columns=[\n",
    "                'text_id', 'path', 'start_idx', \n",
    "                'end_idx', 'sentence', 'phonemes'])       \n",
    "\n",
    "        # paths = data['path'].unique()\n",
    "        # for path in tqdm(paths, total=len(paths)):\n",
    "        #     folder_name = path.split(\"_\", 1)[0]\n",
    "        #     in_path = os.path.join(f\"{self.data_dir}/KOKORO-dataset\", folder_name, path)\n",
    "        #     y, sr = librosa.load(in_path, sr=None)\n",
    "        #     for text_id in data.loc[data['path']==path, 'text_id']:\n",
    "        #         out_path = os.path.join(self.args.main_dir, 'wav_cleaned', text_id) + \".wav\"\n",
    "        #         if not os.path.exists(out_path):\n",
    "        #             start_idx = int(data.loc[data['text_id']==text_id, 'start_idx'].item())\n",
    "        #             end_idx = int(data.loc[data['text_id']==text_id, 'end_idx'].item())\n",
    "        #             y_slice = librosa.resample(\n",
    "        #                 y[start_idx:end_idx], orig_sr=sr, target_sr=self.args.sample_rate)\n",
    "        #             sf.write(out_path, y_slice, samplerate=self.args.sample_rate, subtype='PCM_16')\n",
    "\n",
    "        data = data[['text_id', 'sentence']]\n",
    "        data['text_id'] = data['text_id'].apply(lambda x: x + \".wav\")\n",
    "        data.columns = ['path', 'sentence']\n",
    "        data['corpus'] = ['kokoro'] * len(data)\n",
    "        return data\n",
    "\n",
    "    def get_jsut(self):\n",
    "        filenames, sentences = [], []\n",
    "        for transcript in glob.glob(f\"{self.data_dir}/JSUT-dataset/*/transcript_utf8.txt\"):\n",
    "            file_path = transcript.rsplit(\"\\\\\", 1)[0]\n",
    "            with open(transcript, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines: \n",
    "                    filename, sentence = line.split(\":\")\n",
    "                    filenames.append(os.path.join(file_path, \"wav\", filename) + \".wav\")\n",
    "                    sentences.append(sentence.strip(\"\\n\"))\n",
    "        data = pd.DataFrame({'path': filenames, 'sentence': sentences}) \n",
    "        data['corpus'] = ['jsut'] * len(data)\n",
    "        for i, in_path in tqdm(enumerate(data['path']), total=len(data['path'])):\n",
    "            in_path = in_path.replace(\"\\\\\", \"/\")\n",
    "            out_path = f\"{self.args.main_dir}\\wav_cleaned\"\n",
    "            filename = in_path.rsplit(\"/\", 1)[-1]\n",
    "            out_path = os.path.join(out_path, filename)\n",
    "            if not os.path.exists(out_path):\n",
    "                subprocess.call([\n",
    "                    \"ffmpeg\", \"-i\", in_path,\"-acodec\", \"pcm_s16le\", \n",
    "                    \"-ar\", str(self.args.sample_rate), out_path])\n",
    "            data['path'][i] = filename\n",
    "        return data\n",
    "\n",
    "    def get_commonvoice(self):\n",
    "        data = pd.read_csv(f\"{self.data_dir}/CommonVoice-dataset/validated.tsv\", sep=\"\\t\")\n",
    "        data = data[['path', 'sentence']]    \n",
    "        data['path'] = data['path'].apply(\n",
    "            lambda x: f\"{self.data_dir}/CommonVoice-dataset/clips/\" + x)\n",
    "        data['corpus'] = ['common_voice'] * len(data)\n",
    "        for i, in_path in tqdm(enumerate(data['path']), total=len(data['path'])):\n",
    "            in_path = in_path.replace(\"\\\\\", \"/\")\n",
    "            out_path = f\"{self.args.main_dir}\\wav_cleaned\"\n",
    "            filename = in_path.rsplit(\"/\", 1)[-1]\n",
    "            filename = filename.replace(\"mp3\", \"wav\")\n",
    "            out_path = os.path.join(out_path, filename)\n",
    "            if not os.path.exists(out_path):\n",
    "                subprocess.call([\n",
    "                    \"ffmpeg\", \"-i\", in_path,\"-acodec\", \"pcm_s16le\", \n",
    "                    \"-ar\", str(self.args.sample_rate), out_path])\n",
    "            data['path'][i] = filename\n",
    "        return data\n",
    "\n",
    "    def get_kanji_unicode(self):\n",
    "        vocab = set()\n",
    "        with open(\n",
    "            f\"{self.data_dir}/kanji_unicode.txt\", \n",
    "            encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                for char in line.split()[1:]:\n",
    "                    vocab.add(char)\n",
    "        return \"\".join(sorted(vocab))\n",
    "    \n",
    "    def check_kanji(self, sentence):\n",
    "        pattern = f\"[^{self.kanji_unicode}]\"\n",
    "        if re.match(pattern, sentence) == None:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def clean_kanji(self, sentence):\n",
    "        sentence = \"\".join(sentence.split())\n",
    "        pattern = r\"[・\\。\\！\\.\\？\\、]\"\n",
    "        sentence = re.sub(pattern, \"\", sentence)\n",
    "        return sentence\n",
    "\n",
    "    def kanji2romaji(self, sentence):\n",
    "        try:\n",
    "            sentence = self.katsu.romaji(sentence)\n",
    "            sentence = sentence.replace(\" \", \"\").lower()\n",
    "        except:\n",
    "            sentence = None\n",
    "        return sentence\n",
    "\n",
    "    def get_length(self, path):\n",
    "        path = os.path.join(self.args.main_dir, 'wav_cleaned', path)\n",
    "        y, sr = librosa.load(path, sr=None)\n",
    "        return len(y)\n",
    "\n",
    "# data = Dataset(args).data\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 4))\n",
    "# sns.histplot(x=data['length'], hue=data['corpus'], palette=\"bright\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, args):\n",
    "        tokenizer = Wav2Vec2CTCTokenizer(\n",
    "            vocab_file=f\"{args.main_dir}/vocab.json\",\n",
    "            do_lower_case=False)\n",
    "\n",
    "        feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "            feature_size=1,\n",
    "            sampling_rate=args.sample_rate,\n",
    "            padding_value=0.0,\n",
    "            do_normalize=False,\n",
    "            return_attention_mask=False)\n",
    "\n",
    "        self.processor = Wav2Vec2Processor(\n",
    "            feature_extractor=feature_extractor,\n",
    "            tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFRWriter():\n",
    "    def __init__(self, args):\n",
    "        self.data = pd.read_csv(os.path.join(args.main_dir, \"ASRDataset.csv\"), encoding=\"utf-8\")\n",
    "        self.args = args\n",
    "        self.config = Config(args)\n",
    "\n",
    "    def _bytes_feature(self, value):\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    def _int64_feature(self, value):\n",
    "        \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "    def _float_feature(self, value):\n",
    "        \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "    def serialize_example(self, *args):\n",
    "        feature = {\n",
    "            'input_values': self._bytes_feature(args[0]),\n",
    "            'labels': self._bytes_feature(args[1])}\n",
    "\n",
    "        example_proto = tf.train.Example(\n",
    "            features=tf.train.Features(feature=feature))\n",
    "        return example_proto.SerializeToString()\n",
    "\n",
    "    def get_labels(self, sample):\n",
    "        labels = self.data.loc[self.data['path']==sample, \"romaji\"].item()\n",
    "        labels = (self.config.processor.tokenizer.bos_token + labels + \n",
    "            self.config.processor.tokenizer.eos_token)\n",
    "        labels = self.config.processor.tokenizer(labels, is_split_into_words=True).input_ids\n",
    "        return tf.convert_to_tensor(labels, dtype=tf.int32)\n",
    "\n",
    "    def get_audio(self, sample):\n",
    "        path = os.path.join(self.args.main_dir, \"wav_cleaned\", sample)\n",
    "        audio = librosa.load(path, sr=None)[0]\n",
    "        return tf.convert_to_tensor(audio, dtype=tf.float32)\n",
    "\n",
    "    def get_shards(self):\n",
    "        skf = KFold(n_splits=self.args.n_shards, shuffle=False)\n",
    "        return [\n",
    "            list(map(lambda x: self.data['path'][x], j))\n",
    "            for i, j in skf.split(self.data['path'])]\n",
    "\n",
    "    def get_shard_data(self, samples):\n",
    "        for sample in samples:\n",
    "            audio = self.get_audio(sample)\n",
    "            labels = self.get_labels(sample)\n",
    "            yield {\n",
    "                'input_values': tf.io.serialize_tensor(audio),\n",
    "                'labels': tf.io.serialize_tensor(labels)}\n",
    "\n",
    "    def write(self):\n",
    "        for shard, samples in tqdm(enumerate(self.get_shards()), total=self.args.n_shards):\n",
    "            with tf.io.TFRecordWriter(f\"{self.args.main_dir}/wav2vec2_tfrec/shard_{shard+1}.tfrec\") as f:\n",
    "                for sample in self.get_shard_data(samples):\n",
    "                    example = self.serialize_example(\n",
    "                        sample['input_values'],\n",
    "                        sample['labels'])\n",
    "                    f.write(example)\n",
    "\n",
    "# TFRWriter(args).write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, args):\n",
    "        self.files = glob.glob(args.main_dir + \"/wav2vec2_tfrec/*.tfrec\")\n",
    "        self.args = args\n",
    "        self.AUTOTUNE = tf.data.AUTOTUNE\n",
    "        self.train_files, self.val_files = train_test_split(\n",
    "            self.files, test_size=args.test_size, shuffle=True, \n",
    "            random_state=args.random_state)\n",
    "        self.train = self.get_train()\n",
    "        self.val = self.get_val()\n",
    "\n",
    "    def read_tfrecord(self, example):\n",
    "        feature_description = {\n",
    "            'input_values': tf.io.FixedLenFeature([], tf.string),\n",
    "            'labels': tf.io.FixedLenFeature([], tf.string)}\n",
    "        \n",
    "        example = tf.io.parse_single_example(example, feature_description)\n",
    "        example['input_values'] = tf.io.parse_tensor(\n",
    "            example['input_values'], out_type=tf.float32)\n",
    "        example['labels'] = tf.io.parse_tensor(\n",
    "            example['labels'], out_type=tf.int32)\n",
    "        return example\n",
    "\n",
    "    def load_dataset(self, files):\n",
    "        ignore_order = tf.data.Options()\n",
    "        ignore_order.experimental_deterministic = False\n",
    "        dataset = tf.data.TFRecordDataset(files)\n",
    "        dataset = dataset.with_options(ignore_order)\n",
    "        dataset = dataset.map(self.read_tfrecord, num_parallel_calls=self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_train(self):\n",
    "        dataset = self.load_dataset(self.train_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_values': [None],\n",
    "                'labels': [None]},\n",
    "            padding_values={\n",
    "                'input_values': tf.constant(0, dtype=tf.float32), \n",
    "                'labels': tf.constant(-100, dtype=tf.int32)})        \n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_val(self):\n",
    "        dataset = self.load_dataset(self.val_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_values': [None],\n",
    "                'labels': [None]},\n",
    "            padding_values={\n",
    "                'input_values': tf.constant(0, dtype=tf.float32),\n",
    "                'labels': tf.constant(-100, dtype=tf.int32)})\n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "# train = DataLoader(args).train\n",
    "# output = next(iter(train))\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16,10))\n",
    "# for i, array in enumerate(train.take(16)):\n",
    "#     plt.subplot(4, 4, i+1)\n",
    "#     y = array['input_values'].numpy()\n",
    "#     librosa.display.waveplot(y=y, sr=16000)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class PERMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"PER\", **kwargs):\n",
    "        super(PERMetric, self).__init__(name=name,  **kwargs)\n",
    "        self.accumulator = self.add_weight(name=\"total_per\", initializer=\"zeros\")\n",
    "        self.counter = self.add_weight(name=\"per_count\", initializer=\"zeros\")    \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        hypothesis = tf.cast(tf.sparse.from_dense(y_pred), dtype=tf.int32)\n",
    "\n",
    "        # Convert dense to sparse tensor for edit_distance function\n",
    "        truth = tf.RaggedTensor.from_tensor(y_true, padding=0).to_sparse()\n",
    "\n",
    "        # Calculate Levenshtein distance\n",
    "        distance = tf.edit_distance(hypothesis, truth, normalize=True)\n",
    "\n",
    "        # Add distance and number of samples to variables\n",
    "        self.accumulator.assign_add(tf.reduce_sum(distance))\n",
    "        self.counter.assign_add(len(y_true))\n",
    "\n",
    "    def result(self):\n",
    "        # Divides accumulated distance scores against number of samples passed,\n",
    "        # mimics mean reduction over batch\n",
    "        return tf.math.divide_no_nan(self.accumulator, self.counter)   \n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.accumulator.assign(0.0)\n",
    "        self.counter.assign(0.0)\n",
    "\n",
    "class CosineDecayWithWarmup(LearningRateSchedule):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "    def __call__(self, epoch):  \n",
    "        if epoch < self.args.warmup_epochs:\n",
    "            lr = ((self.args.lr_max - self.args.lr_start) / self.args.warmup_epochs) * epoch + self.args.lr_start\n",
    "        elif epoch < (self.args.warmup_epochs + self.args.sustain_epochs):\n",
    "            lr = self.args.lr_max\n",
    "        else:\n",
    "            progress = ((epoch - self.args.warmup_epochs - self.args.sustain_epochs) / \n",
    "            (self.args.epochs - self.args.warmup_epochs - self.args.sustain_epochs))\n",
    "            lr = (self.args.lr_max-self.args.lr_min) * (0.5 * (1.0 + tf.math.cos((22/7) * \n",
    "                self.args.n_cycles * 2.0 * progress)))\n",
    "            if self.args.lr_min is not None:\n",
    "                lr = tf.math.maximum(self.args.lr_min, lr)\n",
    "        return lr\n",
    "\n",
    "    def plot(self):\n",
    "        epochs = range(self.args.epochs+1)\n",
    "        lr = [self(epoch) for epoch in epochs]\n",
    "        plt.plot(epochs, lr)\n",
    "        plt.xlabel(\"learning_rate\")\n",
    "        plt.ylabel(\"epochs\")\n",
    "        plt.show()\n",
    "\n",
    "CosineDecayWithWarmup(args).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\transformers\\configuration_utils.py:340: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from epoch 3...\n",
      "Epoch 3/30: Learning rate @ 4.01e-06\n",
      " 6506/11250 [================>.............] - ETA: 1:56:14 - loss: 51.3951 - per: 0.3433"
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.config = Config(args)\n",
    "        self.train_dataset = DataLoader(args).train\n",
    "        self.val_dataset = DataLoader(args).val\n",
    "        schedule = CosineDecayWithWarmup(args)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(schedule)\n",
    "        self.per_metrics = PERMetric()\n",
    "        self.model = TFWav2Vec2ForCTC.from_pretrained(\n",
    "            args.model_name,\n",
    "            from_pt=True,\n",
    "            ctc_loss_reduction=\"mean\",\n",
    "            gradient_checkpointing=True,\n",
    "            final_dropout=0.1,\n",
    "            pad_token_id=self.config.processor.tokenizer.pad_token_id,\n",
    "            vocab_size=len(self.config.processor.tokenizer))\n",
    "        self.model.freeze_feature_extractor()\n",
    "        \n",
    "        self.model_name = f\"model_{int(self.args.n_samples/1000)}k\"\n",
    "        self.log_path = f\"{self.args.main_dir}/model_weights/{self.model_name}.csv\"\n",
    "        if not os.path.exists(self.log_path):\n",
    "            print(\"Log file created.\")\n",
    "            columns = \"epoch,loss,per,val_loss,val_per\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(columns)\n",
    "\n",
    "    def decoder(self, labels, logits):\n",
    "        labels = tf.where(labels == -100, x=0, y=labels)\n",
    "        logits = tf.argmax(logits, axis=-1)\n",
    "        return labels, logits\n",
    "\n",
    "    def display(self, epoch, t_labels, t_logits, v_labels, v_logits):\n",
    "        t_labels = self.config.processor.batch_decode(\n",
    "            t_labels, skip_special_tokens=True, group_tokens=False)\n",
    "        v_labels = self.config.processor.batch_decode(\n",
    "            v_labels, skip_special_tokens=True, group_tokens=False)\n",
    "        t_logits = self.config.processor.batch_decode(\n",
    "            t_logits, skip_special_tokens=True, group_tokens=False)\n",
    "        v_logits = self.config.processor.batch_decode(\n",
    "            v_logits, skip_special_tokens=True, group_tokens=False)\n",
    "\n",
    "        print(\"-\" * 129)\n",
    "        print(\"Training\")\n",
    "        for y_true, y_pred in zip(t_labels, t_logits):\n",
    "            print(f\"Target:    {y_true}\")\n",
    "            print(f\"Predicted: {y_pred}\") \n",
    "\n",
    "        print(\"\\nValidation\")\n",
    "        for y_true, y_pred in zip(v_labels, v_logits):\n",
    "            print(f\"Target:    {y_true}\")\n",
    "            print(f\"Predicted: {y_pred}\")\n",
    "        print(\"-\" * 129)\n",
    "\n",
    "    def fit(self):\n",
    "        # Checkpoint manager\n",
    "        self.ckpt_dir = f\"{self.args.main_dir}/checkpoints\"\n",
    "        self.ckpt = tf.train.Checkpoint(self.model)\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(\n",
    "            checkpoint=self.ckpt, directory=self.ckpt_dir, max_to_keep=5)\n",
    "\n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            self.start_epoch = int(self.ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "            self.ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "            print(f\"Resuming from epoch {self.start_epoch + 1}...\")\n",
    "        else:\n",
    "            self.start_epoch = 0\n",
    "            print(\"Starting from epoch 1...\")\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.args.epochs+1):\n",
    "            print(f\"Epoch {epoch+1}/{self.args.epochs}: Learning rate @ {self.optimizer.lr(epoch):.2e}\")\n",
    "            stateful_metrics = [\"loss\", \"per\", \"val_loss\", \"val_per\"]\n",
    "            progbar = tf.keras.utils.Progbar(\n",
    "                self.args.train_steps, interval=0.05,\n",
    "                stateful_metrics=stateful_metrics)\n",
    "\n",
    "            # Training loop\n",
    "            for step, t_batch in enumerate(self.train_dataset):\n",
    "                t_inputs = t_batch['input_values']\n",
    "                t_labels = t_batch['labels']\n",
    "                with tf.GradientTape() as tape:\n",
    "                    t_loss, t_logits = self.model(\n",
    "                        input_values=t_inputs, labels=t_labels, training=True)[:2]\n",
    "                gradients = tape.gradient(t_loss, self.model.trainable_weights)  \n",
    "                self.optimizer.apply_gradients(zip(gradients, self.model.trainable_weights))  \n",
    "\n",
    "                t_labels, t_logits = self.decoder(t_labels, t_logits)\n",
    "                self.per_metrics.update_state(t_labels, t_logits) \n",
    "                t_per = self.per_metrics.result()\n",
    "                t_values = [(\"loss\", t_loss), (\"per\", t_per)]\n",
    "                progbar.update(step, values=t_values, finalize=False)\n",
    "            self.per_metrics.reset_states()\n",
    "            \n",
    "            # Validation loop\n",
    "            for v_batch in self.val_dataset:\n",
    "                v_inputs = v_batch['input_values']\n",
    "                v_labels = v_batch['labels']\n",
    "                v_loss, v_logits = self.model(\n",
    "                    input_values=v_inputs, labels=v_labels, training=False)[:2]       \n",
    "                v_labels, v_logits = self.decoder(v_labels, v_logits)         \n",
    "                self.per_metrics.update_state(v_labels, v_logits)\n",
    "\n",
    "            v_per = self.per_metrics.result()\n",
    "            v_values = [\n",
    "                (\"loss\", t_loss), (\"per\", t_per), (\"val_loss\", v_loss),\n",
    "                (\"val_per\", v_per)]\n",
    "            progbar.update(self.args.train_steps, values=v_values, finalize=True)\n",
    "            self.per_metrics.reset_states()\n",
    "\n",
    "            # Print sample transcriptions for both loops\n",
    "            self.display(epoch, t_labels, t_logits, v_labels, v_logits)\n",
    "\n",
    "            # Checkpointing\n",
    "            self.ckpt.save(file_prefix=f\"{self.ckpt_dir}/{self.model_name}\")\n",
    "\n",
    "            # Logging\n",
    "            log = f\"{epoch+1},{t_loss},{t_per},{v_loss},{v_per}\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(log)\n",
    "\n",
    "            save_path = f\"{self.args.main_dir}/model_weights\"\n",
    "            self.model.save_weights(f\"{save_path}/{self.model_name}_{epoch+1}.h5\")\n",
    "\n",
    "Trainer(args).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = pd.read_csv(\"E:\\Datasets\\ASR-dataset\\model_weights\\model_40k.csv\", index_col=\"epoch\")\n",
    "# history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "# sns.lineplot(x=history.index, y=history['per'], label=\"per\", ax=ax1)\n",
    "# sns.lineplot(x=history.index, y=history['wer'], label=\"wer\", ax=ax2)\n",
    "# sns.lineplot(x=history.index, y=history['val_per'], label=\"val_per\", ax=ax1)\n",
    "# sns.lineplot(x=history.index, y=history['val_wer'], label=\"val_wer\", ax=ax2)\n",
    "# plt.suptitle(\"Acoustic model\")\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"acoustic_history.png\", transparent=False, bbox_inches='tight')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b851d2923cfa3a2562599062e05fd9893d86a7c009c64d8ad3756552e4dd5f41"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('tf-gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
