{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "import re\r\n",
    "import glob\r\n",
    "import json\r\n",
    "import random\r\n",
    "import argparse\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "import librosa\r\n",
    "import librosa.display\r\n",
    "import soundfile as sf\r\n",
    "\r\n",
    "from tqdm import tqdm\r\n",
    "import subprocess\r\n",
    "from functools import partial\r\n",
    "from collections import Counter\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "import MeCab\r\n",
    "import cutlet\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split, KFold\r\n",
    "\r\n",
    "import tensorflow as tf\r\n",
    "import tensorflow_io as tfio\r\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\r\n",
    "from tensorflow.python.client import device_lib\r\n",
    "from tensorflow.keras.mixed_precision import Policy, set_global_policy\r\n",
    "\r\n",
    "from transformers import (\r\n",
    "    Wav2Vec2CTCTokenizer,\r\n",
    "    TFWav2Vec2ForCTC,\r\n",
    "    Wav2Vec2Processor,\r\n",
    "    Wav2Vec2FeatureExtractor)\r\n",
    "\r\n",
    "def seed_everything(SEED):\r\n",
    "    random.seed(SEED)\r\n",
    "    np.random.seed(SEED)\r\n",
    "    tf.random.set_seed(SEED)\r\n",
    "    print(\"Random seed set.\")\r\n",
    "\r\n",
    "seed_everything(42)\r\n",
    "tf.get_logger().setLevel('FATAL')\r\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Random seed set.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class Dataset:\r\n",
    "    def __init__(self):\r\n",
    "        self.main_dir = \"E://Datasets/ASR-dataset\"\r\n",
    "        self.sample_rate = 16000\r\n",
    "        self.n_shards = 10\r\n",
    "        self.data = pd.concat([\r\n",
    "            self.get_kokoro(),\r\n",
    "            self.get_jsut(),\r\n",
    "            self.get_commonvoice()\r\n",
    "            ], \r\n",
    "            ignore_index=True)\r\n",
    "        self.katsu = cutlet.Cutlet()\r\n",
    "        self.wakati = MeCab.Tagger(\"-Owakati\")\r\n",
    "    \r\n",
    "        tqdm.pandas()\r\n",
    "        self.data['sentence'] = self.data['sentence'].progress_apply(self.clean_kanji)\r\n",
    "        self.data['romaji'] = self.data['sentence'].progress_apply(self.katsu.romaji)\r\n",
    "        self.data['romaji'] = self.data['romaji'].progress_apply(self.clean_romaji)\r\n",
    "        self.data['romaji'] = self.data['romaji'].str.lower()\r\n",
    "        self.data['length'] = self.data['path'].progress_apply(self.get_length)\r\n",
    "        self.data = self.data[self.data['sentence'].apply(list).apply(len)>=5]\r\n",
    "        self.data.query(\"(length >= 48000) & (length <= 80000)\", inplace=True)\r\n",
    "        self.data = self.data.dropna()\r\n",
    "        self.data = self.data.sample(n=5000, random_state=42, ignore_index=True)\r\n",
    "        self.data.sort_values(by=\"length\", axis=0, ascending=True, inplace=True, ignore_index=True)\r\n",
    "        self.data.to_csv(f\"{self.main_dir}/ASRDataset.csv\", encoding=\"utf-8\", index=False)\r\n",
    "\r\n",
    "    def get_kokoro(self):\r\n",
    "        in_dir = \"Datasets\\KOKORO-dataset\"\r\n",
    "\r\n",
    "        data = []\r\n",
    "        transcript_path = f\"{in_dir}/transcripts/*.metadata.txt\"\r\n",
    "        for transcript in glob.glob(transcript_path):\r\n",
    "            with open(transcript, \"r\", encoding=\"utf-8\") as f:\r\n",
    "                for line in f.readlines():\r\n",
    "                    data.append(line.split(\"|\"))\r\n",
    "\r\n",
    "        data = pd.DataFrame(\r\n",
    "            data, columns=[\r\n",
    "                'text_id', 'path', 'start_idx', \r\n",
    "                'end_idx', 'sentence', 'phonemes'])       \r\n",
    "\r\n",
    "        # paths = data['path'].unique()\r\n",
    "        # for path in tqdm(paths, total=len(paths)):\r\n",
    "        #     folder_name = path.split(\"_\", 1)[0]\r\n",
    "        #     in_path = os.path.join(in_dir, folder_name, path)\r\n",
    "        #     y, sr = librosa.load(in_path, sr=None)\r\n",
    "        #     for text_id in data.loc[data['path']==path, 'text_id']:\r\n",
    "        #         out_path = os.path.join(self.main_dir, 'wav_cleaned', text_id) + \".wav\"\r\n",
    "        #         if not os.path.exists(out_path):\r\n",
    "        #             start_idx = int(data.loc[data['text_id']==text_id, 'start_idx'].item())\r\n",
    "        #             end_idx = int(data.loc[data['text_id']==text_id, 'end_idx'].item())\r\n",
    "        #             y_slice = librosa.resample(\r\n",
    "        #                 y[start_idx:end_idx], orig_sr=sr, target_sr=self.sample_rate)\r\n",
    "        #             sf.write(out_path, y_slice, samplerate=self.sample_rate, subtype='PCM_16')\r\n",
    "\r\n",
    "        data = data[['text_id', 'sentence']]\r\n",
    "        data['text_id'] = data['text_id'].apply(lambda x: x + \".wav\")\r\n",
    "        data.columns = ['path', 'sentence']\r\n",
    "        data['corpus'] = ['kokoro'] * len(data)\r\n",
    "        return data\r\n",
    "\r\n",
    "    def get_jsut(self):\r\n",
    "        filenames, sentences = [], []\r\n",
    "        for transcript in glob.glob(r\"Datasets/JSUT-dataset/*/transcript_utf8.txt\"):\r\n",
    "            file_path = transcript.rsplit(\"\\\\\", 1)[0]\r\n",
    "            with open(transcript, \"r\", encoding=\"utf-8\") as f:\r\n",
    "                lines = f.readlines()\r\n",
    "                for line in lines: \r\n",
    "                    filename, sentence = line.split(\":\")\r\n",
    "                    filenames.append(os.path.join(file_path, \"wav\", filename) + \".wav\")\r\n",
    "                    sentences.append(sentence.strip(\"\\n\"))\r\n",
    "        data = pd.DataFrame({'path': filenames, 'sentence': sentences}) \r\n",
    "        data['corpus'] = ['jsut'] * len(data)\r\n",
    "        for i, in_path in tqdm(enumerate(data['path']), total=len(data['path'])):\r\n",
    "            in_path = in_path.replace(\"\\\\\", \"/\")\r\n",
    "            out_path = f\"{self.main_dir}\\wav_cleaned\"\r\n",
    "            filename = in_path.rsplit(\"/\", 1)[-1]\r\n",
    "            out_path = os.path.join(out_path, filename)\r\n",
    "            if not os.path.exists(out_path):\r\n",
    "                subprocess.call([\r\n",
    "                    \"ffmpeg\", \"-i\", in_path,\"-acodec\", \"pcm_s16le\", \r\n",
    "                    \"-ar\", str(self.sample_rate), out_path])\r\n",
    "            data['path'][i] = filename\r\n",
    "        return data\r\n",
    "\r\n",
    "    def get_commonvoice(self):\r\n",
    "        data = pd.read_csv(r\"Datasets/CommonVoice-dataset/validated.tsv\", sep=\"\\t\")\r\n",
    "        data = data[['path', 'sentence']]    \r\n",
    "        data['path'] = data['path'].apply(\r\n",
    "            lambda x: r\"Datasets/CommonVoice-dataset/mp3/\" + x)\r\n",
    "        data['corpus'] = ['common_voice'] * len(data)\r\n",
    "        for i, in_path in tqdm(enumerate(data['path']), total=len(data['path'])):\r\n",
    "            in_path = in_path.replace(\"\\\\\", \"/\")\r\n",
    "            out_path = f\"{self.main_dir}\\wav_cleaned\"\r\n",
    "            filename = in_path.rsplit(\"/\", 1)[-1]\r\n",
    "            filename = filename.replace(\"mp3\", \"wav\")\r\n",
    "            out_path = os.path.join(out_path, filename)\r\n",
    "            if not os.path.exists(out_path):\r\n",
    "                subprocess.call([\r\n",
    "                    \"ffmpeg\", \"-i\", in_path,\"-acodec\", \"pcm_s16le\", \r\n",
    "                    \"-ar\", str(self.sample_rate), out_path])\r\n",
    "            data['path'][i] = filename\r\n",
    "        return data\r\n",
    "\r\n",
    "    def clean_kanji(self, sentence):\r\n",
    "        symbols = r\"[（.*?）！-～.,;..._。、-〿・■（）：ㇰ-ㇿ㈠-㉃㊀-㋾㌀-㍿「」『』→ー -~‘–※π—ゐ’“”]\"\r\n",
    "        sentence = re.sub(symbols, \"\", sentence)\r\n",
    "        sentence = self.wakati.parse(sentence).strip(\"\\n\")          \r\n",
    "        return sentence\r\n",
    "\r\n",
    "    def clean_romaji(self, sentence):\r\n",
    "        return re.sub(r'[.,\"\\'\\/?]', \"\", sentence)\r\n",
    "\r\n",
    "    def get_length(self, path):\r\n",
    "        path = os.path.join(self.main_dir, 'wav_cleaned', path)\r\n",
    "        y, sr = librosa.load(path, sr=None)\r\n",
    "        return len(y)\r\n",
    "\r\n",
    "# data = Dataset().data\r\n",
    "# data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# fig, ax = plt.subplots(1,1,figsize=(10, 4))\r\n",
    "# sns.histplot(x=data['length'], hue=data['corpus'], ax=ax, palette=\"bright\")\r\n",
    "# plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Arguments"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def ArgParser():\r\n",
    "    parser = argparse.ArgumentParser()\r\n",
    "\r\n",
    "    # DataLoader\r\n",
    "    parser.add_argument(\"--main_dir\", default=\"E://Datasets/ASR-dataset\")\r\n",
    "    parser.add_argument(\"--sample_rate\", default=16000)\r\n",
    "    parser.add_argument(\"--test_size\", default=0.1)\r\n",
    "    parser.add_argument(\"--random_state\", default=42)\r\n",
    "    parser.add_argument(\"--batch_size\", default=4)\r\n",
    "    parser.add_argument(\"--n_shards\", default=10)\r\n",
    "    parser.add_argument(\"--buffer_size\", default=512)\r\n",
    "\r\n",
    "    # Trainer\r\n",
    "    parser.add_argument(\"--model_name\", default=\"facebook/wav2vec2-base\")\r\n",
    "    parser.add_argument(\"--epochs\", default=30)\r\n",
    "    parser.add_argument(\"--learning_rate\", default=5e-5)\r\n",
    "    parser.add_argument(\"--beam_width\", default=20)\r\n",
    "    parser.add_argument(\"--top_paths\", default=1)\r\n",
    "\r\n",
    "    # Scheduler\r\n",
    "    parser.add_argument(\"--lr_start\", default=5e-5)\r\n",
    "    parser.add_argument(\"--lr_min\", default=1e-4)\r\n",
    "    parser.add_argument(\"--lr_max\", default=1e-4)\r\n",
    "    parser.add_argument(\"--n_cycles\", default=0.5)\r\n",
    "    parser.add_argument(\"--warmup_epochs\", default=4)\r\n",
    "    parser.add_argument(\"--sustain_epochs\", default=2)    \r\n",
    "\r\n",
    "    args = parser.parse_known_args()[0]\r\n",
    "\r\n",
    "    with open(f\"{args.main_dir}/vocab.json\", \"r\") as f:\r\n",
    "        vocab_size = len(json.load(f))\r\n",
    "   \r\n",
    "    n_samples = len(pd.read_csv(os.path.join(args.main_dir, \"ASRDataset.csv\")))\r\n",
    "    n_train = int(n_samples * (1 - args.test_size))\r\n",
    "    n_val = int(n_samples * args.test_size)\r\n",
    "    train_steps = int(np.ceil(n_train / args.batch_size))\r\n",
    "    val_steps = int(np.ceil(n_val / args.batch_size))\r\n",
    "\r\n",
    "    parser.add_argument(\"--vocab_size\", default=vocab_size)\r\n",
    "    parser.add_argument(\"--n_samples\", default=n_samples)\r\n",
    "    parser.add_argument(\"--n_train\", default=n_train)\r\n",
    "    parser.add_argument(\"--n_val\", default=n_val)\r\n",
    "    parser.add_argument(\"--train_steps\", default=train_steps)  \r\n",
    "    parser.add_argument(\"--val_steps\", default=val_steps)  \r\n",
    "    \r\n",
    "    return parser.parse_known_args()[0]\r\n",
    "\r\n",
    "args = ArgParser()\r\n",
    "args"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Namespace(batch_size=4, beam_width=20, buffer_size=512, epochs=30, learning_rate=5e-05, lr_max=0.0001, lr_min=0.0001, lr_start=5e-05, main_dir='E://Datasets/ASR-dataset', model_name='facebook/wav2vec2-base', n_cycles=0.5, n_samples=5000, n_shards=10, n_train=4500, n_val=500, random_state=42, sample_rate=16000, sustain_epochs=2, test_size=0.1, top_paths=1, train_steps=1125, val_steps=125, vocab_size=37, warmup_epochs=4)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Loading"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class Config:\r\n",
    "    def __init__(self, args):\r\n",
    "        tokenizer = Wav2Vec2CTCTokenizer(\r\n",
    "            vocab_file=f\"{args.main_dir}/vocab.json\",\r\n",
    "            unk_token=\"<unk>\",\r\n",
    "            pad_token=\"<pad>\",\r\n",
    "            bos_token=\"<s>\",\r\n",
    "            eos_token=\"</s>\",\r\n",
    "            word_delimiter_token=\" \",\r\n",
    "            do_lower_case=False\r\n",
    "        )\r\n",
    "\r\n",
    "        feature_extractor = Wav2Vec2FeatureExtractor(\r\n",
    "            feature_size=1,\r\n",
    "            sampling_rate=args.sample_rate,\r\n",
    "            padding_value=0.0,\r\n",
    "            do_normalize=True,\r\n",
    "            return_attention_mask=False\r\n",
    "        )\r\n",
    "\r\n",
    "        self.processor = Wav2Vec2Processor(\r\n",
    "            feature_extractor=feature_extractor, \r\n",
    "            tokenizer=tokenizer\r\n",
    "        )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class TFRWriter():\r\n",
    "    def __init__(self, args):\r\n",
    "        self.data = pd.read_csv(os.path.join(args.main_dir, \"ASRDataset.csv\"))\r\n",
    "        self.args = args\r\n",
    "        self.tokenizer = Config(args).processor.tokenizer\r\n",
    "\r\n",
    "    def _bytes_feature(self, value):\r\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\r\n",
    "        if isinstance(value, type(tf.constant(0))):\r\n",
    "            value = value.numpy()\r\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\r\n",
    "\r\n",
    "    def _int64_feature(self, value):\r\n",
    "        \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\r\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\r\n",
    "\r\n",
    "    def _float_feature(self, value):\r\n",
    "        \"\"\"Returns a float_list from a float / double.\"\"\"\r\n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\r\n",
    "\r\n",
    "    def serialize_example(self, *args):\r\n",
    "        feature = {\r\n",
    "            'input_values': self._bytes_feature(args[0]),\r\n",
    "            'labels': self._bytes_feature(args[1]),\r\n",
    "            }\r\n",
    "\r\n",
    "        example_proto = tf.train.Example(\r\n",
    "            features=tf.train.Features(feature=feature))\r\n",
    "        return example_proto.SerializeToString()\r\n",
    "\r\n",
    "    def get_labels(self, sample):\r\n",
    "        labels = self.data.loc[self.data['path']==sample, \"romaji\"].item()\r\n",
    "        labels = (self.tokenizer.bos_token + labels + \r\n",
    "            self.tokenizer.eos_token)\r\n",
    "        labels = self.tokenizer(labels)['input_ids']\r\n",
    "        return tf.convert_to_tensor(labels, dtype=tf.int32)\r\n",
    "\r\n",
    "    def get_audio(self, sample):\r\n",
    "        path = os.path.join(self.args.main_dir, \"wav_cleaned\", sample)\r\n",
    "        audio = librosa.load(path, sr=None)[0]\r\n",
    "        return tf.convert_to_tensor(audio, dtype=tf.float32)\r\n",
    "\r\n",
    "    def get_shards(self):\r\n",
    "        skf = KFold(n_splits=self.args.n_shards, shuffle=False)\r\n",
    "        return [\r\n",
    "            list(map(lambda x: self.data['path'][x], j))\r\n",
    "            for i, j in skf.split(self.data['path'])]\r\n",
    "\r\n",
    "    def get_shard_data(self, samples):\r\n",
    "        for sample in samples:\r\n",
    "            audio = self.get_audio(sample)\r\n",
    "            labels = self.get_labels(sample)\r\n",
    "            yield {\r\n",
    "                'input_values': tf.io.serialize_tensor(audio),\r\n",
    "                'labels': tf.io.serialize_tensor(labels),\r\n",
    "            }\r\n",
    "\r\n",
    "    def write(self):\r\n",
    "        for shard, samples in tqdm(enumerate(self.get_shards()), total=self.args.n_shards):\r\n",
    "            with tf.io.TFRecordWriter(f\"{self.args.main_dir}/wav2vec2_tfrec/shard_{shard+1}.tfrec\") as f:\r\n",
    "                for sample in self.get_shard_data(samples):\r\n",
    "                    example = self.serialize_example(\r\n",
    "                        sample['input_values'], \r\n",
    "                        sample['labels'], \r\n",
    "                        )\r\n",
    "                    f.write(example)\r\n",
    "\r\n",
    "# TFRWriter(args).write()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class DataLoader:\r\n",
    "    def __init__(self, args):\r\n",
    "        self.files = glob.glob(args.main_dir + \"/wav2vec2_tfrec/*.tfrec\")\r\n",
    "        self.args = args\r\n",
    "        self.AUTOTUNE = tf.data.AUTOTUNE\r\n",
    "        self.train_files, self.val_files = train_test_split(\r\n",
    "            self.files, test_size=args.test_size, shuffle=True, \r\n",
    "            random_state=args.random_state)\r\n",
    "        self.train = self.get_train()\r\n",
    "        self.val = self.get_val()     \r\n",
    "\r\n",
    "    def read_tfrecord(self, example):\r\n",
    "        feature_description = {\r\n",
    "            'input_values': tf.io.FixedLenFeature([], tf.string),\r\n",
    "            'labels': tf.io.FixedLenFeature([], tf.string),\r\n",
    "            }\r\n",
    "        \r\n",
    "        example = tf.io.parse_single_example(example, feature_description)\r\n",
    "        example['input_values'] = tf.io.parse_tensor(\r\n",
    "            example['input_values'], out_type=tf.float32)\r\n",
    "        example['labels'] = tf.io.parse_tensor(\r\n",
    "            example['labels'], out_type=tf.int32)\r\n",
    "        return example\r\n",
    "\r\n",
    "    def load_dataset(self, files):\r\n",
    "        ignore_order = tf.data.Options()\r\n",
    "        ignore_order.experimental_deterministic = False\r\n",
    "        dataset = tf.data.TFRecordDataset(files)\r\n",
    "        dataset = dataset.with_options(ignore_order)\r\n",
    "        dataset = dataset.map(self.read_tfrecord, num_parallel_calls=self.AUTOTUNE)\r\n",
    "        return dataset\r\n",
    "\r\n",
    "    def get_train(self):\r\n",
    "        dataset = self.load_dataset(self.train_files)\r\n",
    "        dataset = dataset.padded_batch(\r\n",
    "            self.args.batch_size,\r\n",
    "            padded_shapes={\r\n",
    "                'input_values': [None],\r\n",
    "                'labels': [None]\r\n",
    "            },\r\n",
    "            padding_values={\r\n",
    "                'input_values': tf.constant(0, dtype=tf.float32), \r\n",
    "                'labels': tf.constant(-100, dtype=tf.int32)\r\n",
    "            })\r\n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\r\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\r\n",
    "        return dataset\r\n",
    "\r\n",
    "    def get_val(self):\r\n",
    "        dataset = self.load_dataset(self.val_files)\r\n",
    "        dataset = dataset.padded_batch(\r\n",
    "            self.args.batch_size,\r\n",
    "            padded_shapes={\r\n",
    "                'input_values': [None],\r\n",
    "                'labels': [None]\r\n",
    "            },\r\n",
    "            padding_values={\r\n",
    "                'input_values': tf.constant(0, dtype=tf.float32), \r\n",
    "                'labels': tf.constant(-100, dtype=tf.int32)\r\n",
    "            })\r\n",
    "        dataset = dataset.cache()\r\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\r\n",
    "        return dataset\r\n",
    "\r\n",
    "train = DataLoader(args).train\r\n",
    "next(iter(train))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_values': <tf.Tensor: shape=(4, 66240), dtype=float32, numpy=\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>,\n",
       " 'labels': <tf.Tensor: shape=(4, 55), dtype=int32, numpy=\n",
       " array([[   1,   15,    5,   12,    6,    8,    4,   19,    6,    4,   15,\n",
       "            5,    9,   24,    8,    8,    4,   18,    8,   13,    5,   14,\n",
       "            8,    4,   12,    6,   16,    7,    6,    4,   19,    6,    4,\n",
       "           24,    8,    8,    4,   18,    6,    4,   18,    8,   13,    5,\n",
       "           14,    8,    4,    7,   13,   10,   14,    5,   15,    8,    2],\n",
       "        [   1,   29,    6,    8,    4,   19,    5,    4,    5,   15,    5,\n",
       "            4,   11,    5,   13,    5,    4,   20,    8,    8,   18,    5,\n",
       "           12,    5,    4,   14,    5,   17,   10,    4,   24,    8,   31,\n",
       "            6,    8,    4,   18,    5,    4,    5,   13,    7,   14,    5,\n",
       "           16,    7,   12,    5,    2, -100, -100, -100, -100, -100, -100],\n",
       "        [   1,   28,    8,   13,    7,   11,    5,   10,   12,   12,   10,\n",
       "            4,   14,    7,   13,    8,   12,    6,    4,   20,    6,    7,\n",
       "            4,    6,   14,    6,    7,   17,   10,    4,   18,    5,    4,\n",
       "            6,    6,   11,    5,   12,   12,    5,    2, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
       "        [   1,   29,    6,    8,    4,   19,    5,    4,    9,    7,   25,\n",
       "            7,   20,    6,    8,    4,   22,    7,    4,   17,   10,   15,\n",
       "            8,   18,    5,    4,   11,    5,    7,   16,    5,    4,   10,\n",
       "            4,    7,   11,    7,   14,    5,   15,    8,    2, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]])>}"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class PER(tf.keras.metrics.Metric):\r\n",
    "    \"\"\"Phone Error Rate\r\n",
    "\r\n",
    "    This metric calculates the normalized error rate based on phonemes.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        beam_width: (Optional)\r\n",
    "        top_paths: (Optional)\r\n",
    "        name: (Optional) string name of the metric instance\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, beam_width, top_paths, name=\"PER\", **kwargs):\r\n",
    "        super(PER, self).__init__(name=name,  **kwargs)\r\n",
    "        self.beam_width = beam_width\r\n",
    "        self.top_paths = top_paths\r\n",
    "        self.per_accumulator = self.add_weight(name=\"total_per\", initializer=\"zeros\")\r\n",
    "        self.counter = self.add_weight(name=\"per_count\", initializer=\"zeros\")        \r\n",
    "\r\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\r\n",
    "        \"\"\"\r\n",
    "        Function takes in model output logits and target labels and updates\r\n",
    "        accumulator globally.\r\n",
    "\r\n",
    "        Args: \r\n",
    "            y_true shape: [batch_size, sequence_length]\r\n",
    "            y_pred shape: [batch_size, sequence_length, num_features]\r\n",
    "\r\n",
    "        Returns:\r\n",
    "            None\r\n",
    "\r\n",
    "        \"\"\"\r\n",
    "        batch_size, sequence_length, num_features = tf.shape(y_pred)\r\n",
    "        y_pred = tf.reshape(y_pred, [sequence_length, batch_size, num_features])\r\n",
    "        sequence_length = tf.repeat(sequence_length, batch_size)\r\n",
    "\r\n",
    "        # Decode logits into sparse tensor using beam search decoder\r\n",
    "        hypothesis = tf.nn.ctc_beam_search_decoder(\r\n",
    "            y_pred, sequence_length=sequence_length, beam_width=self.beam_width,\r\n",
    "            top_paths=self.top_paths)[0][0]\r\n",
    "        hypothesis = tf.cast(hypothesis, dtype=tf.int32)\r\n",
    "\r\n",
    "        # Convert dense to sparse tensor for edit_distance function\r\n",
    "        y_true = tf.where(y_true == -100, x=0, y=y_true)\r\n",
    "        truth = tf.sparse.from_dense(y_true)\r\n",
    "\r\n",
    "        # Calculate Levenshtein distance\r\n",
    "        distance = tf.edit_distance(hypothesis, truth, normalize=True)\r\n",
    "\r\n",
    "        # Add distance and number of samples to variables\r\n",
    "        self.per_accumulator.assign_add(tf.reduce_sum(distance))\r\n",
    "        self.counter.assign_add(len(y_true))\r\n",
    "\r\n",
    "    def result(self):\r\n",
    "        # Divides accumulated distance scores against number of samples passed,\r\n",
    "        # mimics mean reduction over batch\r\n",
    "        return tf.math.divide_no_nan(self.per_accumulator, self.counter)   \r\n",
    "    \r\n",
    "    def reset_states(self):\r\n",
    "        self.per_accumulator.assign(0.0)\r\n",
    "        self.counter.assign(0.0)\r\n",
    "\r\n",
    "class CosineDecayWithWarmup(LearningRateSchedule):\r\n",
    "    def __init__(self, args):\r\n",
    "        self.args = args\r\n",
    "\r\n",
    "    def __call__(self, epoch):  \r\n",
    "        if epoch < self.args.warmup_epochs:\r\n",
    "            lr = ((self.args.lr_max - self.args.lr_start) / self.args.warmup_epochs) * epoch + self.args.lr_start\r\n",
    "        elif epoch < (self.args.warmup_epochs + self.args.sustain_epochs):\r\n",
    "            lr = self.args.lr_max\r\n",
    "        else:\r\n",
    "            progress = ((epoch - self.args.warmup_epochs - self.args.sustain_epochs) / \r\n",
    "            (self.args.epochs - self.args.warmup_epochs - self.args.sustain_epochs))\r\n",
    "            lr = (self.args.lr_max-self.args.lr_min) * (0.5 * (1.0 + tf.math.cos((22/7) * \r\n",
    "                self.args.n_cycles * 2.0 * progress)))\r\n",
    "            if self.args.lr_min is not None:\r\n",
    "                lr = tf.math.maximum(self.args.lr_min, lr)\r\n",
    "        return lr"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "class Trainer:\r\n",
    "    def __init__(self, args):\r\n",
    "        self.args = args\r\n",
    "        self.config = Config(args)\r\n",
    "        self.train_dataset = DataLoader(args).train\r\n",
    "        self.val_dataset = DataLoader(args).val\r\n",
    "        self.schedule = CosineDecayWithWarmup(args)\r\n",
    "        self.optimizer = tf.keras.optimizers.Adam(self.schedule)\r\n",
    "        self.metrics = PER(beam_width=args.beam_width, top_paths=args.top_paths)\r\n",
    "        self.model = TFWav2Vec2ForCTC.from_pretrained(\r\n",
    "            args.model_name,\r\n",
    "            from_pt=True,\r\n",
    "            ctc_loss_reduction=\"mean\",\r\n",
    "            pad_token_id=self.config.processor.tokenizer.pad_token_id,\r\n",
    "            vocab_size=len(self.config.processor.tokenizer))\r\n",
    "        self.model.freeze_feature_extractor()\r\n",
    "        self.history = {\r\n",
    "            \"loss\": [],\r\n",
    "            \"per\": [],\r\n",
    "            \"val_loss\": [],\r\n",
    "            \"val_per\": []\r\n",
    "        }\r\n",
    "    \r\n",
    "    def train_step(self, batch):\r\n",
    "        X_train = batch['input_values']\r\n",
    "        y_train = batch['labels']\r\n",
    "        with tf.GradientTape() as tape:\r\n",
    "            loss, logits = self.model(\r\n",
    "                input_values=X_train, labels=y_train, training=True)[:2]\r\n",
    "        gradients = tape.gradient(loss, self.model.trainable_weights)\r\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_weights))\r\n",
    "        self.metrics.update_state(y_train, logits)\r\n",
    "        return loss, logits\r\n",
    "\r\n",
    "    def val_step(self, batch):\r\n",
    "        X_val = batch['input_values']\r\n",
    "        y_val = batch['labels']\r\n",
    "        loss, logits = self.model(\r\n",
    "            input_values=X_val, labels=y_val, training=False)[:2]\r\n",
    "        return loss, logits\r\n",
    "\r\n",
    "    def display(self, epoch, t_labels, t_logits, v_labels, v_logits):\r\n",
    "        if epoch % 5 != 0:\r\n",
    "            return\r\n",
    "\r\n",
    "        # Training loop\r\n",
    "        t_labels = tf.where(t_labels == -100, x=0, y=t_labels)\r\n",
    "        t_labels = self.config.processor.batch_decode(t_labels)\r\n",
    "        t_logits = tf.argmax(t_logits, axis=-1)\r\n",
    "        t_logits = self.config.processor.batch_decode(t_logits)\r\n",
    "\r\n",
    "        print(\"-\" * 129)\r\n",
    "        print(\"Training\")\r\n",
    "        for y_true, y_pred in zip(t_labels, t_logits):\r\n",
    "            print(f\"Target:    {y_true}\")\r\n",
    "            print(f\"Predicted: {y_pred}\") \r\n",
    "\r\n",
    "        # Validation loop\r\n",
    "        v_labels = tf.where(v_labels == -100, x=0, y=v_labels)\r\n",
    "        v_labels = self.config.processor.batch_decode(v_labels)\r\n",
    "        v_logits = tf.argmax(v_logits, axis=-1)\r\n",
    "        v_logits = self.config.processor.batch_decode(v_logits)   \r\n",
    "\r\n",
    "        print(\"\\nValidation\")\r\n",
    "        for y_true, y_pred in zip(v_labels, v_logits):\r\n",
    "            print(f\"Target:    {y_true}\")\r\n",
    "            print(f\"Predicted: {y_pred}\")\r\n",
    "        print(\"-\" * 129)\r\n",
    "\r\n",
    "    def fit(self):\r\n",
    "        for epoch in range(self.args.epochs):\r\n",
    "            print(f\"Epoch {epoch+1}/{self.args.epochs}: Learning rate @ {self.optimizer.lr(epoch):.2e}\")\r\n",
    "            stateful_metrics = [\"loss\", \"per\", \"val_loss\", \"val_per\"]\r\n",
    "            progbar = tf.keras.utils.Progbar(\r\n",
    "                self.args.train_steps, interval=0.05,\r\n",
    "                stateful_metrics=stateful_metrics)\r\n",
    "\r\n",
    "            # Training loop\r\n",
    "            for step, t_batch in enumerate(self.train_dataset):\r\n",
    "                t_loss, t_logits = self.train_step(t_batch)\r\n",
    "                t_per = self.metrics.result()\r\n",
    "                t_values = [(\"loss\", t_loss), (\"per\", t_per)]\r\n",
    "                progbar.update(step, values=t_values, finalize=False)\r\n",
    "            self.metrics.reset_states()\r\n",
    "            \r\n",
    "            # Validation loop\r\n",
    "            for v_batch in self.val_dataset:\r\n",
    "                v_loss, v_logits = self.val_step(v_batch)                         \r\n",
    "                self.metrics.update_state(v_batch['labels'], v_logits)\r\n",
    "\r\n",
    "            v_per = self.metrics.result()\r\n",
    "            v_values = [\r\n",
    "                (\"loss\", t_loss), (\"per\", t_per), (\"val_loss\", v_loss),\r\n",
    "                (\"val_per\", v_per)]\r\n",
    "            progbar.update(self.args.train_steps, values=v_values, finalize=True)\r\n",
    "            self.metrics.reset_states()\r\n",
    "\r\n",
    "            # Print sample transcriptions for both loops\r\n",
    "            self.display(\r\n",
    "                epoch, t_batch['labels'], t_logits, v_batch['labels'], v_logits)\r\n",
    "\r\n",
    "            # Checkpointing\r\n",
    "            self.model.save_weights(\"{}\\checkpoints\\model{}k_{}of{}.h5\".format(\r\n",
    "                self.args.main_dir, self.args.n_samples//1000, epoch+1, self.args.epochs))\r\n",
    "\r\n",
    "            # Logging\r\n",
    "            self.history['loss'].append(t_loss)\r\n",
    "            self.history['per'].append(t_per)\r\n",
    "            self.history['val_loss'].append(v_loss)\r\n",
    "            self.history['val_per'].append(v_per)\r\n",
    "\r\n",
    "        return history\r\n",
    "\r\n",
    "history = Trainer(args).fit()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\transformers\\configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFWav2Vec2ForCTC: ['project_q.weight', 'quantizer.weight_proj.weight', 'project_q.bias', 'quantizer.codevectors', 'quantizer.weight_proj.bias', 'project_hid.weight', 'project_hid.bias']\n",
      "- This IS expected if you are initializing TFWav2Vec2ForCTC from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFWav2Vec2ForCTC from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFWav2Vec2ForCTC were not initialized from the PyTorch model and are newly initialized: ['lm_head.weight', 'lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30: Learning rate @ 5.00e-05\n",
      "1125/1125 [==============================] - 1809s 2s/step - loss: 28.4677 - per: 1.5232 - val_loss: 31.7124 - val_per: 1.8317\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Training\n",
      "Target:    <s>shouyu wotsukete sashimi wo tabemasu</s>\n",
      "Predicted: <s>shouyuotsukete sashimi n sabemasuu</s>\n",
      "Target:    <s>yamadasan wa ochichisan ga isha desu</s>\n",
      "Predicted: <s>kamuzasan wa otou san gae isha desu</s>\n",
      "Target:    <s>kore hodo touitsu no aru haigou mo nai kore hodoshizen de</s>\n",
      "Predicted: <s>kore hodo touitsu no aru haigou mo naikore hodoshimende</s>\n",
      "Target:    <s>sona koto wa ikan yo kimi da teshikata ga nai sa</s>\n",
      "Predicted: <s>sonna koto wa ikai wo kimi dat deshikata ga naisa</s>\n",
      "\n",
      "Validation\n",
      "Target:    <s>sensei wa yamadasan no namae wo yobimashita</s>\n",
      "Predicted: <s>senseu wa yamatasan no namahae wo yobimashita</s>\n",
      "Target:    <s>saudi no seiji jousei no hoka no bunsekisha wa isou utagai wo idaite imashita</s>\n",
      "Predicted: <s>kauji no seijijou sei no hokarobunsekisha wa issou daga y wu iraiteimashita</s>\n",
      "Target:    <s>sono bamen wa pora no kao no up kara hajimarimasu</s>\n",
      "Predicted: <s>sono bameu wa poura no kawu no appu kara hachimarimasu</s>\n",
      "Target:    <s>erabareru to wa omotenaiga moshi kashitara to omoteshimau</s>\n",
      "Predicted: <s>erabareru to wa omotte nai ga moshi kashitara to omotteshimau</s>\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 2/30: Learning rate @ 6.25e-05\n",
      "1125/1125 [==============================] - 1557s 1s/step - loss: 24.0530 - per: 1.7008 - val_loss: 25.2545 - val_per: 1.6570\n",
      "Epoch 3/30: Learning rate @ 7.50e-05\n",
      "1125/1125 [==============================] - 1585s 1s/step - loss: 28.0872 - per: 1.6667 - val_loss: 23.7508 - val_per: 1.6007\n",
      "Epoch 4/30: Learning rate @ 8.75e-05\n",
      "1125/1125 [==============================] - 1561s 1s/step - loss: 30.3576 - per: 1.6589 - val_loss: 20.9569 - val_per: 1.6578\n",
      "Epoch 5/30: Learning rate @ 1.00e-04\n",
      " 162/1125 [===>..........................] - ETA: 20:27 - loss: 22.0882 - per: 1.6657"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('tf-gpu': conda)"
  },
  "interpreter": {
   "hash": "b851d2923cfa3a2562599062e05fd9893d86a7c009c64d8ad3756552e4dd5f41"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}