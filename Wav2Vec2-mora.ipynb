{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import regex as re\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import groupby\n",
    "from convert_romaji import Romaji2Kana\n",
    "\n",
    "import cutlet\n",
    "import jiwer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "\n",
    "from transformers import (\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2Processor,\n",
    "    TFWav2Vec2ForCTC,\n",
    "    logging)\n",
    "\n",
    "from convert_romaji import Romaji2Kana\n",
    "\n",
    "def seed_everything(SEED):\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    print(\"Random seed set.\")\n",
    "\n",
    "seed_everything(42)\n",
    "tf.get_logger().setLevel('FATAL')\n",
    "logging.set_verbosity_error()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=4, buffer_size=512, epochs=15, learning_rate=1e-05, lr_max=1e-05, lr_min=1e-10, lr_start=1e-08, main_dir='E://Datasets/Acoustic_model', model_name='facebook/wav2vec2-base', n_cycles=0.5, n_samples=40000, n_shards=40, n_train=36000, n_val=4000, random_state=42, sample_rate=16000, sustain_epochs=0, test_size=0.1, train_steps=9000, val_steps=1000, vocab_size=37, warmup_epochs=3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ArgParser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # DataLoader\n",
    "    parser.add_argument(\"--main_dir\", default=\"E://Datasets/Acoustic_model\")\n",
    "    parser.add_argument(\"--sample_rate\", default=16000)\n",
    "    parser.add_argument(\"--test_size\", default=0.1)\n",
    "    parser.add_argument(\"--random_state\", default=42)\n",
    "    parser.add_argument(\"--batch_size\", default=4)\n",
    "    parser.add_argument(\"--n_shards\", default=40)\n",
    "    parser.add_argument(\"--buffer_size\", default=512)\n",
    "    parser.add_argument(\"--n_samples\", default=40000)\n",
    "\n",
    "    # Trainer\n",
    "    parser.add_argument(\"--model_name\", default=\"facebook/wav2vec2-base\")\n",
    "    parser.add_argument(\"--epochs\", default=15)\n",
    "\n",
    "    # Scheduler\n",
    "    parser.add_argument(\"--learning_rate\", default=1e-5)\n",
    "    parser.add_argument(\"--lr_start\", default=1e-8)\n",
    "    parser.add_argument(\"--lr_min\", default=1e-10)\n",
    "    parser.add_argument(\"--lr_max\", default=1e-5)\n",
    "    parser.add_argument(\"--n_cycles\", default=0.5)\n",
    "    parser.add_argument(\"--warmup_epochs\", default=3)\n",
    "    parser.add_argument(\"--sustain_epochs\", default=0)\n",
    "\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    with open(f\"{args.main_dir}/vocab.json\", \"r\") as f:\n",
    "        vocab_size = len(json.load(f))\n",
    "   \n",
    "    n_train = int(args.n_samples * (1 - args.test_size))\n",
    "    n_val = int(args.n_samples * args.test_size)\n",
    "    train_steps = int(np.ceil(n_train / args.batch_size))\n",
    "    val_steps = int(np.ceil(n_val / args.batch_size))\n",
    "\n",
    "    parser.add_argument(\"--vocab_size\", default=vocab_size)\n",
    "    parser.add_argument(\"--n_train\", default=n_train)\n",
    "    parser.add_argument(\"--n_val\", default=n_val)\n",
    "    parser.add_argument(\"--train_steps\", default=train_steps)  \n",
    "    parser.add_argument(\"--val_steps\", default=val_steps)\n",
    "    \n",
    "    return parser.parse_known_args()[0]\n",
    "\n",
    "args = ArgParser()\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.data_dir = \"E:/Datasets/Acoustic_model/raw_data\"\n",
    "        self.data = pd.concat([\n",
    "            self.get_kokoro(),\n",
    "            self.get_jsut(),\n",
    "            self.get_commonvoice()], \n",
    "            ignore_index=True)\n",
    "        self.kanji_unicode = self.get_kanji_unicode()\n",
    "        self.katsu = cutlet.Cutlet()\n",
    "        self.katsu.use_foreign_spelling = False\n",
    "        self.vocab = self.get_vocab()\n",
    "    \n",
    "        tqdm.pandas()\n",
    "        # Remove words within parenthesis\n",
    "        parenthesis =  r\"\\（.*\\）|\\(.*\\)|\\「.*\\」|\\『.*\\』\"\n",
    "        self.data = self.data[~self.data['sentence'].str.contains(parenthesis)]\n",
    "\n",
    "        # Remove punctuations from sentences\n",
    "        self.data['sentence'] = self.data['sentence'].progress_apply(self.clean_kanji)\n",
    "        self.data['romaji'] = self.data['sentence'].progress_apply(self.kanji2romaji)\n",
    "        self.data['length'] = self.data['path'].progress_apply(self.get_length)\n",
    "        self.data = self.data[self.data['length'].between(50000, 85000)]\n",
    "        self.data = self.data.reset_index(drop=True)\n",
    "        self.data = self.data.sample(n=self.args.n_samples, random_state=42, ignore_index=True)\n",
    "        self.data.sort_values(by=\"length\", axis=0, ascending=True, inplace=True, ignore_index=True)\n",
    "        self.data.to_csv(\n",
    "            f\"{self.args.main_dir}/ASRDataset.csv\", \n",
    "            encoding=\"utf-8\", index=False)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        with open(r\"E:\\Datasets\\Acoustic_model\\vocab.json\") as f:\n",
    "            vocab = \"|\".join(list(json.load(f).keys())[4:])\n",
    "        return vocab\n",
    "\n",
    "    def get_kokoro(self):\n",
    "        data = []\n",
    "        transcript_path = f\"{self.data_dir}/KOKORO-dataset/transcripts/*.metadata.txt\"\n",
    "        for transcript in glob.glob(transcript_path):\n",
    "            with open(transcript, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f.readlines():\n",
    "                    data.append(line.split(\"|\"))\n",
    "\n",
    "        data = pd.DataFrame(\n",
    "            data, columns=[\n",
    "                'text_id', 'path', 'start_idx', \n",
    "                'end_idx', 'sentence', 'phonemes'])       \n",
    "\n",
    "        # paths = data['path'].unique()\n",
    "        # for path in tqdm(paths, total=len(paths)):\n",
    "        #     folder_name = path.split(\"_\", 1)[0]\n",
    "        #     in_path = os.path.join(f\"{self.data_dir}/KOKORO-dataset\", folder_name, path)\n",
    "        #     y, sr = librosa.load(in_path, sr=None)\n",
    "        #     for text_id in data.loc[data['path']==path, 'text_id']:\n",
    "        #         out_path = os.path.join(self.args.main_dir, 'wav_cleaned', text_id) + \".wav\"\n",
    "        #         if not os.path.exists(out_path):\n",
    "        #             start_idx = int(data.loc[data['text_id']==text_id, 'start_idx'].item())\n",
    "        #             end_idx = int(data.loc[data['text_id']==text_id, 'end_idx'].item())\n",
    "        #             y_slice = librosa.resample(\n",
    "        #                 y[start_idx:end_idx], orig_sr=sr, target_sr=self.args.sample_rate)\n",
    "        #             sf.write(out_path, y_slice, samplerate=self.args.sample_rate, subtype='PCM_16')\n",
    "\n",
    "        data = data[['text_id', 'sentence']]\n",
    "        data['text_id'] = data['text_id'].apply(lambda x: x + \".wav\")\n",
    "        data.columns = ['path', 'sentence']\n",
    "        data['corpus'] = ['kokoro'] * len(data)\n",
    "        return data\n",
    "\n",
    "    def get_jsut(self):\n",
    "        filenames, sentences = [], []\n",
    "        for transcript in glob.glob(f\"{self.data_dir}/JSUT-dataset/*/transcript_utf8.txt\"):\n",
    "            file_path = transcript.rsplit(\"\\\\\", 1)[0]\n",
    "            with open(transcript, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines: \n",
    "                    filename, sentence = line.split(\":\")\n",
    "                    filenames.append(os.path.join(file_path, \"wav\", filename) + \".wav\")\n",
    "                    sentences.append(sentence.strip(\"\\n\"))\n",
    "        data = pd.DataFrame({'path': filenames, 'sentence': sentences}) \n",
    "        data['corpus'] = ['jsut'] * len(data)\n",
    "        for i, in_path in tqdm(enumerate(data['path']), total=len(data['path'])):\n",
    "            in_path = in_path.replace(\"\\\\\", \"/\")\n",
    "            out_path = f\"{self.args.main_dir}\\wav_cleaned\"\n",
    "            filename = in_path.rsplit(\"/\", 1)[-1]\n",
    "            out_path = os.path.join(out_path, filename)\n",
    "            if not os.path.exists(out_path):\n",
    "                subprocess.call([\n",
    "                    \"ffmpeg\", \"-i\", in_path,\"-acodec\", \"pcm_s16le\", \n",
    "                    \"-ar\", str(self.args.sample_rate), out_path])\n",
    "            data['path'][i] = filename\n",
    "        return data\n",
    "\n",
    "    def get_commonvoice(self):\n",
    "        data = pd.read_csv(f\"{self.data_dir}/CommonVoice-dataset/validated.tsv\", sep=\"\\t\")\n",
    "        data = data[['path', 'sentence']]    \n",
    "        data['path'] = data['path'].apply(\n",
    "            lambda x: f\"{self.data_dir}/CommonVoice-dataset/clips/\" + x)\n",
    "        data['corpus'] = ['common_voice'] * len(data)\n",
    "        for i, in_path in tqdm(enumerate(data['path']), total=len(data['path'])):\n",
    "            in_path = in_path.replace(\"\\\\\", \"/\")\n",
    "            out_path = f\"{self.args.main_dir}\\wav_cleaned\"\n",
    "            filename = in_path.rsplit(\"/\", 1)[-1]\n",
    "            filename = filename.replace(\"mp3\", \"wav\")\n",
    "            out_path = os.path.join(out_path, filename)\n",
    "            if not os.path.exists(out_path):\n",
    "                subprocess.call([\n",
    "                    \"ffmpeg\", \"-i\", in_path,\"-acodec\", \"pcm_s16le\", \n",
    "                    \"-ar\", str(self.args.sample_rate), out_path])\n",
    "            data['path'][i] = filename\n",
    "        return data\n",
    "\n",
    "    def get_kanji_unicode(self):\n",
    "        vocab = set()\n",
    "        with open(\n",
    "            f\"{self.data_dir}/kanji_unicode.txt\", \n",
    "            encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                for char in line.split()[1:]:\n",
    "                    vocab.add(char)\n",
    "        return \"\".join(sorted(vocab))\n",
    "        \n",
    "    def clean_kanji(self, sentence):\n",
    "        sentence = \"\".join(sentence.split())\n",
    "        pattern = f\"[^{self.kanji_unicode}]\"\n",
    "        sentence = re.sub(pattern, \"\", sentence)\n",
    "        return sentence\n",
    "\n",
    "    def kanji2romaji(self, sentence):\n",
    "        try:\n",
    "            sentence = self.katsu.romaji(sentence)\n",
    "            sentence = re.sub(r\"'|-| \", \"\", sentence).lower()\n",
    "        except:\n",
    "            sentence = None\n",
    "        return sentence\n",
    "\n",
    "    def get_length(self, path):\n",
    "        path = os.path.join(self.args.main_dir, 'wav_cleaned', path)\n",
    "        y = librosa.load(path, sr=None)[0]\n",
    "        return len(y)\n",
    "\n",
    "# data = Dataset(args).data\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 4))\n",
    "# sns.histplot(x=data['length'], hue=data['corpus'], palette=\"bright\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, args):\n",
    "        tokenizer = Wav2Vec2CTCTokenizer(\n",
    "            vocab_file=f\"{args.main_dir}/vocab.json\",\n",
    "            do_lower_case=False)\n",
    "\n",
    "        feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "            feature_size=1,\n",
    "            sampling_rate=args.sample_rate,\n",
    "            padding_value=0.0,\n",
    "            do_normalize=False,\n",
    "            return_attention_mask=False)\n",
    "\n",
    "        self.processor = Wav2Vec2Processor(\n",
    "            feature_extractor=feature_extractor,\n",
    "            tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFRWriter():\n",
    "    def __init__(self, args):\n",
    "        self.data = pd.read_csv(os.path.join(args.main_dir, \"ASRDataset.csv\"), encoding=\"utf-8\")\n",
    "        self.args = args\n",
    "        self.config = Config(args)\n",
    "\n",
    "    def _bytes_feature(self, value):\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    def _int64_feature(self, value):\n",
    "        \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "    def _float_feature(self, value):\n",
    "        \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "    def serialize_example(self, *args):\n",
    "        feature = {\n",
    "            'input_values': self._bytes_feature(args[0]),\n",
    "            'labels': self._bytes_feature(args[1])}\n",
    "\n",
    "        example_proto = tf.train.Example(\n",
    "            features=tf.train.Features(feature=feature))\n",
    "        return example_proto.SerializeToString()\n",
    "\n",
    "    def get_labels(self, sample):\n",
    "        labels = self.data.loc[self.data['path']==sample, \"romaji\"].item()\n",
    "        labels = (self.config.processor.tokenizer.bos_token + labels + \n",
    "            self.config.processor.tokenizer.eos_token)\n",
    "        labels = self.config.processor.tokenizer(labels, is_split_into_words=False).input_ids\n",
    "        return tf.convert_to_tensor(labels, dtype=tf.int32)\n",
    "\n",
    "    def get_audio(self, sample):\n",
    "        path = os.path.join(self.args.main_dir, \"wav_cleaned\", sample)\n",
    "        audio = librosa.load(path, sr=None)[0]\n",
    "        return tf.convert_to_tensor(audio, dtype=tf.float32)\n",
    "\n",
    "    def get_shards(self):\n",
    "        skf = KFold(n_splits=self.args.n_shards, shuffle=False)\n",
    "        return [\n",
    "            list(map(lambda x: self.data['path'][x], j))\n",
    "            for _, j in skf.split(self.data['path'])]\n",
    "\n",
    "    def get_shard_data(self, samples):\n",
    "        for sample in samples:\n",
    "            audio = self.get_audio(sample)\n",
    "            labels = self.get_labels(sample)\n",
    "            yield {\n",
    "                'input_values': tf.io.serialize_tensor(audio),\n",
    "                'labels': tf.io.serialize_tensor(labels)}\n",
    "\n",
    "    def write(self):\n",
    "        for shard, samples in tqdm(enumerate(self.get_shards()), total=self.args.n_shards):\n",
    "            with tf.io.TFRecordWriter(f\"{self.args.main_dir}/wav2vec2_tfrec/shard_{shard+1}.tfrec\") as f:\n",
    "                for sample in self.get_shard_data(samples):\n",
    "                    example = self.serialize_example(\n",
    "                        sample['input_values'],\n",
    "                        sample['labels'])\n",
    "                    f.write(example)\n",
    "\n",
    "# TFRWriter(args).write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, args):\n",
    "        self.files = glob.glob(args.main_dir + \"/wav2vec2_tfrec/*.tfrec\")\n",
    "        self.args = args\n",
    "        self.AUTOTUNE = tf.data.AUTOTUNE\n",
    "        self.train_files, self.val_files = train_test_split(\n",
    "            self.files, test_size=args.test_size, shuffle=True, \n",
    "            random_state=args.random_state)\n",
    "        self.train = self.get_train()\n",
    "        self.val = self.get_val()\n",
    "\n",
    "    def read_tfrecord(self, example):\n",
    "        feature_description = {\n",
    "            'input_values': tf.io.FixedLenFeature([], tf.string),\n",
    "            'labels': tf.io.FixedLenFeature([], tf.string)}\n",
    "        \n",
    "        example = tf.io.parse_single_example(example, feature_description)\n",
    "        example['input_values'] = tf.io.parse_tensor(\n",
    "            example['input_values'], out_type=tf.float32)\n",
    "        example['labels'] = tf.io.parse_tensor(\n",
    "            example['labels'], out_type=tf.int32)\n",
    "        return example\n",
    "\n",
    "    def load_dataset(self, files):\n",
    "        ignore_order = tf.data.Options()\n",
    "        ignore_order.experimental_deterministic = False\n",
    "        dataset = tf.data.TFRecordDataset(files)\n",
    "        dataset = dataset.with_options(ignore_order)\n",
    "        dataset = dataset.map(self.read_tfrecord, num_parallel_calls=self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_train(self):\n",
    "        dataset = self.load_dataset(self.train_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_values': [None],\n",
    "                'labels': [None]},\n",
    "            padding_values={\n",
    "                'input_values': tf.constant(0, dtype=tf.float32), \n",
    "                'labels': tf.constant(-100, dtype=tf.int32)})        \n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_val(self):\n",
    "        dataset = self.load_dataset(self.val_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_values': [None],\n",
    "                'labels': [None]},\n",
    "            padding_values={\n",
    "                'input_values': tf.constant(0, dtype=tf.float32),\n",
    "                'labels': tf.constant(-100, dtype=tf.int32)})\n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "# train = DataLoader(args).train\n",
    "# output = next(iter(train))['labels']\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16,10))\n",
    "# for i, array in enumerate(train.take(16)):\n",
    "#     plt.subplot(4, 4, i+1)\n",
    "#     y = array['input_values'].numpy()\n",
    "#     librosa.display.waveplot(y=y, sr=16000)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class PERMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"PER\", **kwargs):\n",
    "        super(PERMetric, self).__init__(name=name,  **kwargs)\n",
    "        self.accumulator = self.add_weight(name=\"total_per\", initializer=\"zeros\")\n",
    "        self.counter = self.add_weight(name=\"per_count\", initializer=\"zeros\")    \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        hypothesis = tf.cast(tf.sparse.from_dense(y_pred), dtype=tf.int32)\n",
    "\n",
    "        # Convert dense to sparse tensor for edit_distance function\n",
    "        truth = tf.RaggedTensor.from_tensor(y_true, padding=0).to_sparse()\n",
    "\n",
    "        # Calculate Levenshtein distance\n",
    "        distance = tf.edit_distance(hypothesis, truth, normalize=True)\n",
    "\n",
    "        # Add distance and number of samples to variables\n",
    "        self.accumulator.assign_add(tf.reduce_sum(distance))\n",
    "        self.counter.assign_add(len(y_true))\n",
    "\n",
    "    def result(self):\n",
    "        # Divides accumulated distance scores against number of samples passed,\n",
    "        # mimics mean reduction over batch\n",
    "        return tf.math.divide_no_nan(self.accumulator, self.counter)   \n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.accumulator.assign(0.0)\n",
    "        self.counter.assign(0.0)\n",
    "\n",
    "class CosineDecayWithWarmup(LearningRateSchedule):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "    def __call__(self, epoch):  \n",
    "        if epoch < self.args.warmup_epochs:\n",
    "            lr = ((self.args.lr_max - self.args.lr_start) / self.args.warmup_epochs) * epoch + self.args.lr_start\n",
    "        elif epoch < (self.args.warmup_epochs + self.args.sustain_epochs):\n",
    "            lr = self.args.lr_max\n",
    "        else:\n",
    "            progress = ((epoch - self.args.warmup_epochs - self.args.sustain_epochs) / \n",
    "            (self.args.epochs - self.args.warmup_epochs - self.args.sustain_epochs))\n",
    "            lr = (self.args.lr_max-self.args.lr_min) * (0.5 * (1.0 + tf.math.cos((22/7) * \n",
    "                self.args.n_cycles * 2.0 * progress)))\n",
    "            if self.args.lr_min is not None:\n",
    "                lr = tf.math.maximum(self.args.lr_min, lr)\n",
    "        return lr\n",
    "\n",
    "    def plot(self):\n",
    "        epochs = range(self.args.epochs+1)\n",
    "        lr = [self(epoch) for epoch in epochs]\n",
    "        plt.plot(epochs, lr)\n",
    "        plt.xlabel(\"learning_rate\")\n",
    "        plt.ylabel(\"epochs\")\n",
    "        plt.show()\n",
    "\n",
    "CosineDecayWithWarmup(args).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\transformers\\configuration_utils.py:341: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from epoch 2...\n",
      "Epoch 2/15: Learning rate @ 3.34e-06\n",
      "9000/9000 [==============================] - 10199s 1s/step - loss: 11.3088 - per: 0.4736 - val_loss: 10.5777 - val_per: 0.3924\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Training\n",
      "Target:    kusariwokittenigerukotogadekinaitokini\n",
      "Predicted: kanaarrwokitttteeniigerukotogaadekkinaaitokiinii\n",
      "Target:    surutoushironohoudeinugakyuunihoedashita\n",
      "Predicted: suruutoshiiroonnohooudeiinigaakykyuuniihoorewaashshittaa\n",
      "Target:    sonouesenseinoiukotonosenseitoshite\n",
      "Predicted: soonoouueessennseeinnoyuukotonnoossenseiiwoshshittee\n",
      "Target:    oodooriworeinohosoiouraiekiretakarewa\n",
      "Predicted: ooouriwooreinoohhossoioooraiieekiirrewaakarewaa\n",
      "\n",
      "Validation\n",
      "Target:    sekkakunodougiwokourinosokoehourikonderiyoushinainodesu\n",
      "Predicted: ssekkakunodouugiiwwokourinossokoeehhorikonderuyoouushiinaainodeessuu\n",
      "Target:    doujinikattenoyokonitsuiteirugejoheyanotowoaketa\n",
      "Predicted: doujiiniikaatttennoyyokkoniitsuiiteeiirrugejoobaanootowwooaaketaa\n",
      "Target:    sokoniwakarenoyokitoorishiroishiitsunitsutsumaretafutonga\n",
      "Predicted: sokkoniwakaareenoyokidoorishshiroiishshiiitsunniitsutsuumarreetaafutonngga\n",
      "Target:    sonouewatakushiwairosewaninarutoiukoujitsunomotoni\n",
      "Predicted: soonoeeewaatakushiwwaaiiroirosewanninaarrutoiiiuukoouujitstsunoomotonni\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 3/15: Learning rate @ 6.67e-06\n",
      "9000/9000 [==============================] - 10350s 1s/step - loss: 8.0029 - per: 0.4110 - val_loss: 10.4274 - val_per: 0.3229\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Training\n",
      "Target:    sukaatonosusoworyoutedemotsushigusagaamariniyuugada\n",
      "Predicted: ssukaaaatonoosuusoouryiooutedemotsushshiigusaaggaamariniyuuuugara\n",
      "Target:    nipponekurutokikuukoudeshashinwotorimashita\n",
      "Predicted: niphooonnekuurrutokikuuukouudeshaashinnotorimassittaa\n",
      "Target:    korekaraatatakakunattekurukisetsunichoudoiifukusou\n",
      "Predicted: korekaraaaatatakakunaattttekuruukkiiseetstsuuniichchoooudooiiifukssouu\n",
      "Target:    gaikokueikutokipasupootowomotteikimasu\n",
      "Predicted: gaaikkokuueeiikuutookiiipasupooottooomoottteiikimaasuuu\n",
      "\n",
      "Validation\n",
      "Target:    sekkakunodougiwokourinosokoehourikonderiyoushinainodesu\n",
      "Predicted: ssekkakunodougiwwokourinossokoeehorikondderuyyoouushiinainodeesuu\n",
      "Target:    doujinikattenoyokonitsuiteirugejoheyanotowoaketa\n",
      "Predicted: doujiiniikaatttennoyyokooniitsuuiiteiirruyejobaanootowwwoaaketaa\n",
      "Target:    sokoniwakarenoyokitoorishiroishiitsunitsutsumaretafutonga\n",
      "Predicted: sokooniwakareenoyyokidoorishshiroiishshiitsunnitsutsumarreetaafutonngga\n",
      "Target:    sonouewatakushiwairosewaninarutoiukoujitsunomotoni\n",
      "Predicted: soonoeewaatakuushiwaaaiiroirosewaninaarrutoiiiuukooujitstsunoomotonni\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 4/15: Learning rate @ 1.00e-05\n",
      "9000/9000 [==============================] - 11191s 1s/step - loss: 13.3452 - per: 0.3680 - val_loss: 8.7314 - val_per: 0.2464\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Training\n",
      "Target:    ishakarakanjoukakiwoshoseigamottekitanowo\n",
      "Predicted: ishshakaraakanjogakiwwooshoosseigaamottekkitanowwo\n",
      "Target:    onnawashinmiritoshitachoushidesonofubono\n",
      "Predicted: oonnaawwaashiinnmiitoshshitaachoushiiddesonochichino\n",
      "Target:    taiteiwaatamanigomuseinozukinwokabutte\n",
      "Predicted: taitteiwaaaatammaniigomuseinozukinokaabutee\n",
      "Target:    shizentokubomunijoubakarinoiwanonakani\n",
      "Predicted: shshizenntokubomunijoouubakarrinnoiwaannonaakanii\n",
      "\n",
      "Validation\n",
      "Target:    sekkakunodougiwokourinosokoehourikonderiyoushinainodesu\n",
      "Predicted: seekkakunodougiwookourinossokoehhourikondderuyyouushinainodeesuu\n",
      "Target:    doujinikattenoyokonitsuiteirugejoheyanotowoaketa\n",
      "Predicted: doujiiniikatttennoyyokoniitsuiiteiirrugejobyanotowwoaketa\n",
      "Target:    sokoniwakarenoyokitoorishiroishiitsunitsutsumaretafutonga\n",
      "Predicted: sokooniwakarenoyokidoorishshiroishiiitsunnitsutsumarreetafutonngga\n",
      "Target:    sonouewatakushiwairosewaninarutoiukoujitsunomotoni\n",
      "Predicted: soonouewatakushiwaaiiroirosewaninarrutoiiiukoujitstsunoomotonni\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 5/15: Learning rate @ 9.83e-06\n",
      "2976/9000 [========>.....................] - ETA: 2:09:27 - loss: 30.1020 - per: 0.3565"
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.config = Config(args)\n",
    "        self.train_dataset = DataLoader(args).train\n",
    "        self.val_dataset = DataLoader(args).val\n",
    "        schedule = CosineDecayWithWarmup(args)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(schedule)\n",
    "        self.per_metrics = PERMetric()\n",
    "        self.model = TFWav2Vec2ForCTC.from_pretrained(\n",
    "            args.model_name,\n",
    "            from_pt=True,\n",
    "            ctc_loss_reduction=\"mean\",\n",
    "            gradient_checkpointing=True,\n",
    "            pad_token_id=self.config.processor.tokenizer.pad_token_id,\n",
    "            vocab_size=len(self.config.processor.tokenizer))\n",
    "        self.model.freeze_feature_extractor()\n",
    "        \n",
    "        self.model_name = f\"model_{int(self.args.n_samples/1000)}k\"\n",
    "        self.log_path = f\"{self.args.main_dir}/model_weights/{self.model_name}.csv\"\n",
    "        if not os.path.exists(self.log_path):\n",
    "            print(\"Log file created.\")\n",
    "            columns = \"epoch,loss,per,val_loss,val_per\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(columns)\n",
    "    \n",
    "    def decoder(self, labels, logits):\n",
    "        labels = tf.where(labels == -100, x=0, y=labels)\n",
    "        logits = tf.argmax(logits, axis=-1)\n",
    "        return labels, logits\n",
    "\n",
    "    def display(self, t_labels, t_logits, v_labels, v_logits):\n",
    "        t_labels = self.config.processor.batch_decode(\n",
    "            t_labels, skip_special_tokens=True, group_tokens=False)\n",
    "        t_logits = self.config.processor.batch_decode(\n",
    "            t_logits, skip_special_tokens=True, group_tokens=False)\n",
    "        v_labels = self.config.processor.batch_decode(\n",
    "            v_labels, skip_special_tokens=True, group_tokens=False)\n",
    "        v_logits = self.config.processor.batch_decode(\n",
    "            v_logits, skip_special_tokens=True, group_tokens=False)\n",
    "        \n",
    "        print(\"-\" * 129)\n",
    "        print(\"Training\")\n",
    "        for y_true, y_pred in zip(t_labels, t_logits):\n",
    "            print(f\"Target:    {y_true}\")\n",
    "            print(f\"Predicted: {y_pred}\") \n",
    "\n",
    "        print(\"\\nValidation\")\n",
    "        for y_true, y_pred in zip(v_labels, v_logits):\n",
    "            print(f\"Target:    {y_true}\")\n",
    "            print(f\"Predicted: {y_pred}\")\n",
    "        print(\"-\" * 129)\n",
    "\n",
    "    def fit(self):\n",
    "        # Checkpoint manager\n",
    "        self.ckpt_dir = f\"{self.args.main_dir}/checkpoints\"\n",
    "        self.ckpt = tf.train.Checkpoint(self.model)\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(\n",
    "            checkpoint=self.ckpt, directory=self.ckpt_dir, max_to_keep=5)\n",
    "\n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            self.start_epoch = int(self.ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "            self.ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "            print(f\"Resuming from epoch {self.start_epoch + 1}...\")\n",
    "        else:\n",
    "            self.start_epoch = 0\n",
    "            print(\"Starting from epoch 1...\")\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.args.epochs+1):\n",
    "            print(f\"Epoch {epoch+1}/{self.args.epochs}: Learning rate @ {self.optimizer.lr(epoch):.2e}\")\n",
    "            stateful_metrics = [\"loss\", \"per\", \"val_loss\", \"val_per\"]\n",
    "            progbar = tf.keras.utils.Progbar(\n",
    "                self.args.train_steps, interval=0.05,\n",
    "                stateful_metrics=stateful_metrics)\n",
    "\n",
    "            # Training loop\n",
    "            for step, t_batch in enumerate(self.train_dataset):\n",
    "                t_inputs = t_batch['input_values']\n",
    "                t_labels = t_batch['labels']\n",
    "                with tf.GradientTape() as tape:\n",
    "                    t_loss, t_logits = self.model(\n",
    "                        input_values=t_inputs, labels=t_labels, training=True)[:2]\n",
    "                gradients = tape.gradient(t_loss, self.model.trainable_weights)  \n",
    "                self.optimizer.apply_gradients(zip(gradients, self.model.trainable_weights))  \n",
    "                t_labels, t_logits = self.decoder(t_labels, t_logits)\n",
    "                self.per_metrics.update_state(t_labels, t_logits) \n",
    "                t_per = self.per_metrics.result()\n",
    "                t_values = [(\"loss\", t_loss), (\"per\", t_per)]\n",
    "                progbar.update(step, values=t_values, finalize=False)\n",
    "            self.per_metrics.reset_states()\n",
    "            \n",
    "            # Validation loop\n",
    "            for v_batch in self.val_dataset:\n",
    "                v_inputs = v_batch['input_values']\n",
    "                v_labels = v_batch['labels']\n",
    "                v_loss, v_logits = self.model(\n",
    "                    input_values=v_inputs, labels=v_labels, training=False)[:2]       \n",
    "                v_labels, v_logits = self.decoder(v_labels, v_logits)         \n",
    "                self.per_metrics.update_state(v_labels, v_logits)\n",
    "\n",
    "            v_per = self.per_metrics.result()\n",
    "            v_values = [\n",
    "                (\"loss\", t_loss), (\"per\", t_per), (\"val_loss\", v_loss),\n",
    "                (\"val_per\", v_per)]\n",
    "            progbar.update(self.args.train_steps, values=v_values, finalize=True)\n",
    "            self.per_metrics.reset_states()\n",
    "\n",
    "            # Print sample transcriptions for both loops\n",
    "            self.display(t_labels, t_logits, v_labels, v_logits)\n",
    "\n",
    "            # Checkpointing\n",
    "            self.ckpt.save(file_prefix=f\"{self.ckpt_dir}/{self.model_name}\")\n",
    "\n",
    "            # Logging\n",
    "            log = f\"{epoch+1},{t_loss},{t_per},{v_loss},{v_per}\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(log)\n",
    "\n",
    "            save_path = f\"{self.args.main_dir}/model_weights\"\n",
    "            self.model.save_weights(f\"{save_path}/{self.model_name}_{epoch+1}.h5\")\n",
    "\n",
    "Trainer(args).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = pd.read_csv(\"E:\\Datasets\\ASR-dataset\\model_weights\\model_40k.csv\", index_col=\"epoch\")\n",
    "# history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "# sns.lineplot(x=history.index, y=history['per'], label=\"per\", ax=ax1)\n",
    "# sns.lineplot(x=history.index, y=history['wer'], label=\"wer\", ax=ax2)\n",
    "# sns.lineplot(x=history.index, y=history['val_per'], label=\"val_per\", ax=ax1)\n",
    "# sns.lineplot(x=history.index, y=history['val_wer'], label=\"val_wer\", ax=ax2)\n",
    "# plt.suptitle(\"Acoustic model\")\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"acoustic_history.png\", transparent=False, bbox_inches='tight')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b851d2923cfa3a2562599062e05fd9893d86a7c009c64d8ad3756552e4dd5f41"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('tf-gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
