{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "from convert_romaji import Romaji2Kana\n",
    "\n",
    "import cutlet\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "\n",
    "from transformers import (\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2Processor,\n",
    "    TFWav2Vec2ForCTC,\n",
    "    logging)\n",
    "\n",
    "from convert_romaji import Romaji2Kana\n",
    "\n",
    "def seed_everything(SEED):\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    print(\"Random seed set.\")\n",
    "\n",
    "seed_everything(42)\n",
    "tf.get_logger().setLevel('FATAL')\n",
    "logging.set_verbosity_error()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=4, buffer_size=512, epochs=20, learning_rate=5e-05, lr_max=5e-05, lr_min=1e-08, lr_start=1e-08, main_dir='E://Datasets/Acoustic_model', model_name='facebook/wav2vec2-base', n_cycles=0.5, n_samples=50000, n_shards=40, n_train=37500, n_val=12500, random_state=42, sample_rate=16000, sustain_epochs=0, test_size=0.25, train_steps=9375, val_steps=3125, vocab_size=37, warmup_epochs=5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ArgParser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # DataLoader\n",
    "    parser.add_argument(\"--main_dir\", default=\"E://Datasets/Acoustic_model\")\n",
    "    parser.add_argument(\"--sample_rate\", default=16000)\n",
    "    parser.add_argument(\"--test_size\", default=0.25)\n",
    "    parser.add_argument(\"--random_state\", default=42)\n",
    "    parser.add_argument(\"--batch_size\", default=4)\n",
    "    parser.add_argument(\"--n_shards\", default=40)\n",
    "    parser.add_argument(\"--buffer_size\", default=512)\n",
    "    parser.add_argument(\"--n_samples\", default=50000)\n",
    "\n",
    "    # Trainer\n",
    "    parser.add_argument(\"--model_name\", default=\"facebook/wav2vec2-base\")\n",
    "    parser.add_argument(\"--epochs\", default=20)\n",
    "\n",
    "    # Scheduler\n",
    "    parser.add_argument(\"--learning_rate\", default=5e-5)\n",
    "    parser.add_argument(\"--lr_start\", default=1e-8)\n",
    "    parser.add_argument(\"--lr_min\", default=1e-8)\n",
    "    parser.add_argument(\"--lr_max\", default=5e-5)\n",
    "    parser.add_argument(\"--n_cycles\", default=0.5)\n",
    "    parser.add_argument(\"--warmup_epochs\", default=5)\n",
    "    parser.add_argument(\"--sustain_epochs\", default=0)\n",
    "\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    with open(f\"{args.main_dir}/vocab.json\", \"r\") as f:\n",
    "        vocab_size = len(json.load(f))\n",
    "   \n",
    "    n_train = int(args.n_samples * (1 - args.test_size))\n",
    "    n_val = int(args.n_samples * args.test_size)\n",
    "    train_steps = int(np.ceil(n_train / args.batch_size))\n",
    "    val_steps = int(np.ceil(n_val / args.batch_size))\n",
    "\n",
    "    parser.add_argument(\"--vocab_size\", default=vocab_size)\n",
    "    parser.add_argument(\"--n_train\", default=n_train)\n",
    "    parser.add_argument(\"--n_val\", default=n_val)\n",
    "    parser.add_argument(\"--train_steps\", default=train_steps)  \n",
    "    parser.add_argument(\"--val_steps\", default=val_steps)\n",
    "    \n",
    "    return parser.parse_known_args()[0]\n",
    "\n",
    "args = ArgParser()\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.data_dir = \"E:/Datasets/Acoustic_model/raw_data\"\n",
    "        self.data = pd.concat([\n",
    "            self.get_kokoro(),\n",
    "            self.get_jsut(),\n",
    "            self.get_commonvoice()], \n",
    "            ignore_index=True)\n",
    "        self.kanji_unicode = self.get_kanji_unicode()\n",
    "        self.katsu = cutlet.Cutlet()\n",
    "        self.katsu.use_foreign_spelling = False\n",
    "    \n",
    "        tqdm.pandas()\n",
    "        # Remove rows that contains non-kanji characters\n",
    "        self.data = self.data[self.data['sentence'].progress_apply(self.check_kanji)] \n",
    "\n",
    "        # Remove words within parenthesis\n",
    "        parenthesis =  r\"\\（.*\\）|\\(.*\\)|\\「.*\\」|\\『.*\\』\"\n",
    "        self.data = self.data[~self.data['sentence'].str.contains(parenthesis)]\n",
    "\n",
    "        # Remove punctuations from sentences\n",
    "        self.data['sentence'] = self.data['sentence'].progress_apply(self.clean_kanji)\n",
    "        self.data['romaji'] = self.data['sentence'].progress_apply(self.kanji2romaji)\n",
    "        self.data['length'] = self.data['path'].progress_apply(self.get_length)\n",
    "        self.data = self.data[self.data['length'].between(48000, 100000)]\n",
    "        self.data = self.data.reset_index(drop=True)\n",
    "        self.data = self.data.sample(n=self.args.n_samples, random_state=42, ignore_index=True)\n",
    "        self.data.sort_values(by=\"length\", axis=0, ascending=True, inplace=True, ignore_index=True)\n",
    "        self.data.to_csv(\n",
    "            f\"{self.args.main_dir}/ASRDataset.csv\", \n",
    "            encoding=\"utf-8\", index=False)\n",
    "\n",
    "    def get_kokoro(self):\n",
    "        data = []\n",
    "        transcript_path = f\"{self.data_dir}/KOKORO-dataset/transcripts/*.metadata.txt\"\n",
    "        for transcript in glob.glob(transcript_path):\n",
    "            with open(transcript, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f.readlines():\n",
    "                    data.append(line.split(\"|\"))\n",
    "\n",
    "        data = pd.DataFrame(\n",
    "            data, columns=[\n",
    "                'text_id', 'path', 'start_idx', \n",
    "                'end_idx', 'sentence', 'phonemes'])       \n",
    "\n",
    "        # paths = data['path'].unique()\n",
    "        # for path in tqdm(paths, total=len(paths)):\n",
    "        #     folder_name = path.split(\"_\", 1)[0]\n",
    "        #     in_path = os.path.join(f\"{self.data_dir}/KOKORO-dataset\", folder_name, path)\n",
    "        #     y, sr = librosa.load(in_path, sr=None)\n",
    "        #     for text_id in data.loc[data['path']==path, 'text_id']:\n",
    "        #         out_path = os.path.join(self.args.main_dir, 'wav_cleaned', text_id) + \".wav\"\n",
    "        #         if not os.path.exists(out_path):\n",
    "        #             start_idx = int(data.loc[data['text_id']==text_id, 'start_idx'].item())\n",
    "        #             end_idx = int(data.loc[data['text_id']==text_id, 'end_idx'].item())\n",
    "        #             y_slice = librosa.resample(\n",
    "        #                 y[start_idx:end_idx], orig_sr=sr, target_sr=self.args.sample_rate)\n",
    "        #             sf.write(out_path, y_slice, samplerate=self.args.sample_rate, subtype='PCM_16')\n",
    "\n",
    "        data = data[['text_id', 'sentence']]\n",
    "        data['text_id'] = data['text_id'].apply(lambda x: x + \".wav\")\n",
    "        data.columns = ['path', 'sentence']\n",
    "        data['corpus'] = ['kokoro'] * len(data)\n",
    "        return data\n",
    "\n",
    "    def get_jsut(self):\n",
    "        filenames, sentences = [], []\n",
    "        for transcript in glob.glob(f\"{self.data_dir}/JSUT-dataset/*/transcript_utf8.txt\"):\n",
    "            file_path = transcript.rsplit(\"\\\\\", 1)[0]\n",
    "            with open(transcript, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines: \n",
    "                    filename, sentence = line.split(\":\")\n",
    "                    filenames.append(os.path.join(file_path, \"wav\", filename) + \".wav\")\n",
    "                    sentences.append(sentence.strip(\"\\n\"))\n",
    "        data = pd.DataFrame({'path': filenames, 'sentence': sentences}) \n",
    "        data['corpus'] = ['jsut'] * len(data)\n",
    "        for i, in_path in tqdm(enumerate(data['path']), total=len(data['path'])):\n",
    "            in_path = in_path.replace(\"\\\\\", \"/\")\n",
    "            out_path = f\"{self.args.main_dir}\\wav_cleaned\"\n",
    "            filename = in_path.rsplit(\"/\", 1)[-1]\n",
    "            out_path = os.path.join(out_path, filename)\n",
    "            if not os.path.exists(out_path):\n",
    "                subprocess.call([\n",
    "                    \"ffmpeg\", \"-i\", in_path,\"-acodec\", \"pcm_s16le\", \n",
    "                    \"-ar\", str(self.args.sample_rate), out_path])\n",
    "            data['path'][i] = filename\n",
    "        return data\n",
    "\n",
    "    def get_commonvoice(self):\n",
    "        data = pd.read_csv(f\"{self.data_dir}/CommonVoice-dataset/validated.tsv\", sep=\"\\t\")\n",
    "        data = data[['path', 'sentence']]    \n",
    "        data['path'] = data['path'].apply(\n",
    "            lambda x: f\"{self.data_dir}/CommonVoice-dataset/clips/\" + x)\n",
    "        data['corpus'] = ['common_voice'] * len(data)\n",
    "        for i, in_path in tqdm(enumerate(data['path']), total=len(data['path'])):\n",
    "            in_path = in_path.replace(\"\\\\\", \"/\")\n",
    "            out_path = f\"{self.args.main_dir}\\wav_cleaned\"\n",
    "            filename = in_path.rsplit(\"/\", 1)[-1]\n",
    "            filename = filename.replace(\"mp3\", \"wav\")\n",
    "            out_path = os.path.join(out_path, filename)\n",
    "            if not os.path.exists(out_path):\n",
    "                subprocess.call([\n",
    "                    \"ffmpeg\", \"-i\", in_path,\"-acodec\", \"pcm_s16le\", \n",
    "                    \"-ar\", str(self.args.sample_rate), out_path])\n",
    "            data['path'][i] = filename\n",
    "        return data\n",
    "\n",
    "    def get_kanji_unicode(self):\n",
    "        vocab = set()\n",
    "        with open(\n",
    "            f\"{self.data_dir}/kanji_unicode.txt\", \n",
    "            encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                for char in line.split()[1:]:\n",
    "                    vocab.add(char)\n",
    "        return \"\".join(sorted(vocab))\n",
    "    \n",
    "    def check_kanji(self, sentence):\n",
    "        pattern = f\"[^{self.kanji_unicode}]\"\n",
    "        if re.match(pattern, sentence) == None:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def clean_kanji(self, sentence):\n",
    "        sentence = \"\".join(sentence.split())\n",
    "        pattern = r\"[・\\。\\！\\.\\？\\、]\"\n",
    "        sentence = re.sub(pattern, \"\", sentence)\n",
    "        return sentence\n",
    "\n",
    "    def kanji2romaji(self, sentence):\n",
    "        try:\n",
    "            sentence = self.katsu.romaji(sentence)\n",
    "            sentence = sentence.replace(\" \", \"\").lower()\n",
    "        except:\n",
    "            sentence = None\n",
    "        return sentence\n",
    "\n",
    "    def get_length(self, path):\n",
    "        path = os.path.join(self.args.main_dir, 'wav_cleaned', path)\n",
    "        y, sr = librosa.load(path, sr=None)\n",
    "        return len(y)\n",
    "\n",
    "# data = Dataset(args).data\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 4))\n",
    "# sns.histplot(x=data['length'], hue=data['corpus'], palette=\"bright\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, args):\n",
    "        tokenizer = Wav2Vec2CTCTokenizer(\n",
    "            vocab_file=f\"{args.main_dir}/vocab.json\",\n",
    "            do_lower_case=False)\n",
    "\n",
    "        feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "            feature_size=1,\n",
    "            sampling_rate=args.sample_rate,\n",
    "            padding_value=0.0,\n",
    "            do_normalize=False,\n",
    "            return_attention_mask=False)\n",
    "\n",
    "        self.processor = Wav2Vec2Processor(\n",
    "            feature_extractor=feature_extractor,\n",
    "            tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFRWriter():\n",
    "    def __init__(self, args):\n",
    "        self.data = pd.read_csv(os.path.join(args.main_dir, \"ASRDataset.csv\"), encoding=\"utf-8\")\n",
    "        self.args = args\n",
    "        self.config = Config(args)\n",
    "\n",
    "    def _bytes_feature(self, value):\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    def _int64_feature(self, value):\n",
    "        \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "    def _float_feature(self, value):\n",
    "        \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "    def serialize_example(self, *args):\n",
    "        feature = {\n",
    "            'input_values': self._bytes_feature(args[0]),\n",
    "            'labels': self._bytes_feature(args[1])}\n",
    "\n",
    "        example_proto = tf.train.Example(\n",
    "            features=tf.train.Features(feature=feature))\n",
    "        return example_proto.SerializeToString()\n",
    "\n",
    "    def get_labels(self, sample):\n",
    "        labels = self.data.loc[self.data['path']==sample, \"romaji\"].item()\n",
    "        labels = (self.config.processor.tokenizer.bos_token + labels + \n",
    "            self.config.processor.tokenizer.eos_token)\n",
    "        labels = self.config.processor.tokenizer(labels, is_split_into_words=True).input_ids\n",
    "        return tf.convert_to_tensor(labels, dtype=tf.int32)\n",
    "\n",
    "    def get_audio(self, sample):\n",
    "        path = os.path.join(self.args.main_dir, \"wav_cleaned\", sample)\n",
    "        audio = librosa.load(path, sr=None)[0]\n",
    "        return tf.convert_to_tensor(audio, dtype=tf.float32)\n",
    "\n",
    "    def get_shards(self):\n",
    "        skf = KFold(n_splits=self.args.n_shards, shuffle=False)\n",
    "        return [\n",
    "            list(map(lambda x: self.data['path'][x], j))\n",
    "            for i, j in skf.split(self.data['path'])]\n",
    "\n",
    "    def get_shard_data(self, samples):\n",
    "        for sample in samples:\n",
    "            audio = self.get_audio(sample)\n",
    "            labels = self.get_labels(sample)\n",
    "            yield {\n",
    "                'input_values': tf.io.serialize_tensor(audio),\n",
    "                'labels': tf.io.serialize_tensor(labels)}\n",
    "\n",
    "    def write(self):\n",
    "        for shard, samples in tqdm(enumerate(self.get_shards()), total=self.args.n_shards):\n",
    "            with tf.io.TFRecordWriter(f\"{self.args.main_dir}/wav2vec2_tfrec/shard_{shard+1}.tfrec\") as f:\n",
    "                for sample in self.get_shard_data(samples):\n",
    "                    example = self.serialize_example(\n",
    "                        sample['input_values'],\n",
    "                        sample['labels'])\n",
    "                    f.write(example)\n",
    "\n",
    "# TFRWriter(args).write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, args):\n",
    "        self.files = glob.glob(args.main_dir + \"/wav2vec2_tfrec/*.tfrec\")\n",
    "        self.args = args\n",
    "        self.AUTOTUNE = tf.data.AUTOTUNE\n",
    "        self.train_files, self.val_files = train_test_split(\n",
    "            self.files, test_size=args.test_size, shuffle=True, \n",
    "            random_state=args.random_state)\n",
    "        self.train = self.get_train()\n",
    "        self.val = self.get_val()\n",
    "\n",
    "    def read_tfrecord(self, example):\n",
    "        feature_description = {\n",
    "            'input_values': tf.io.FixedLenFeature([], tf.string),\n",
    "            'labels': tf.io.FixedLenFeature([], tf.string)}\n",
    "        \n",
    "        example = tf.io.parse_single_example(example, feature_description)\n",
    "        example['input_values'] = tf.io.parse_tensor(\n",
    "            example['input_values'], out_type=tf.float32)\n",
    "        example['labels'] = tf.io.parse_tensor(\n",
    "            example['labels'], out_type=tf.int32)\n",
    "        return example\n",
    "\n",
    "    def load_dataset(self, files):\n",
    "        ignore_order = tf.data.Options()\n",
    "        ignore_order.experimental_deterministic = False\n",
    "        dataset = tf.data.TFRecordDataset(files)\n",
    "        dataset = dataset.with_options(ignore_order)\n",
    "        dataset = dataset.map(self.read_tfrecord, num_parallel_calls=self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_train(self):\n",
    "        dataset = self.load_dataset(self.train_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_values': [None],\n",
    "                'labels': [None]},\n",
    "            padding_values={\n",
    "                'input_values': tf.constant(0, dtype=tf.float32), \n",
    "                'labels': tf.constant(-100, dtype=tf.int32)})        \n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_val(self):\n",
    "        dataset = self.load_dataset(self.val_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_values': [None],\n",
    "                'labels': [None]},\n",
    "            padding_values={\n",
    "                'input_values': tf.constant(0, dtype=tf.float32),\n",
    "                'labels': tf.constant(-100, dtype=tf.int32)})\n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "# train = DataLoader(args).train\n",
    "# output = next(iter(train))\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16,10))\n",
    "# for i, array in enumerate(train.take(16)):\n",
    "#     plt.subplot(4, 4, i+1)\n",
    "#     y = array['input_values'].numpy()\n",
    "#     librosa.display.waveplot(y=y, sr=16000)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class PERMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"PER\", **kwargs):\n",
    "        super(PERMetric, self).__init__(name=name,  **kwargs)\n",
    "        self.accumulator = self.add_weight(name=\"total_per\", initializer=\"zeros\")\n",
    "        self.counter = self.add_weight(name=\"per_count\", initializer=\"zeros\")    \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        hypothesis = tf.cast(tf.sparse.from_dense(y_pred), dtype=tf.int32)\n",
    "\n",
    "        # Convert dense to sparse tensor for edit_distance function\n",
    "        truth = tf.RaggedTensor.from_tensor(y_true, padding=0).to_sparse()\n",
    "\n",
    "        # Calculate Levenshtein distance\n",
    "        distance = tf.edit_distance(hypothesis, truth, normalize=True)\n",
    "\n",
    "        # Add distance and number of samples to variables\n",
    "        self.accumulator.assign_add(tf.reduce_sum(distance))\n",
    "        self.counter.assign_add(len(y_true))\n",
    "\n",
    "    def result(self):\n",
    "        # Divides accumulated distance scores against number of samples passed,\n",
    "        # mimics mean reduction over batch\n",
    "        return tf.math.divide_no_nan(self.accumulator, self.counter)   \n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.accumulator.assign(0.0)\n",
    "        self.counter.assign(0.0)\n",
    "\n",
    "class CosineDecayWithWarmup(LearningRateSchedule):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "    def __call__(self, epoch):  \n",
    "        if epoch < self.args.warmup_epochs:\n",
    "            lr = ((self.args.lr_max - self.args.lr_start) / self.args.warmup_epochs) * epoch + self.args.lr_start\n",
    "        elif epoch < (self.args.warmup_epochs + self.args.sustain_epochs):\n",
    "            lr = self.args.lr_max\n",
    "        else:\n",
    "            progress = ((epoch - self.args.warmup_epochs - self.args.sustain_epochs) / \n",
    "            (self.args.epochs - self.args.warmup_epochs - self.args.sustain_epochs))\n",
    "            lr = (self.args.lr_max-self.args.lr_min) * (0.5 * (1.0 + tf.math.cos((22/7) * \n",
    "                self.args.n_cycles * 2.0 * progress)))\n",
    "            if self.args.lr_min is not None:\n",
    "                lr = tf.math.maximum(self.args.lr_min, lr)\n",
    "        return lr\n",
    "\n",
    "    def plot(self):\n",
    "        epochs = range(self.args.epochs+1)\n",
    "        lr = [self(epoch) for epoch in epochs]\n",
    "        plt.plot(epochs, lr)\n",
    "        plt.xlabel(\"learning_rate\")\n",
    "        plt.ylabel(\"epochs\")\n",
    "        plt.show()\n",
    "\n",
    "CosineDecayWithWarmup(args).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\transformers\\configuration_utils.py:340: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from epoch 1...\n",
      "Epoch 1/20: Learning rate @ 1.00e-08\n",
      "  66/9375 [..............................] - ETA: 3:35:22 - loss: 542.7144 - per: 1.3807"
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.config = Config(args)\n",
    "        self.train_dataset = DataLoader(args).train\n",
    "        self.val_dataset = DataLoader(args).val\n",
    "        schedule = CosineDecayWithWarmup(args)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(schedule)\n",
    "        self.per_metrics = PERMetric()\n",
    "        self.model = TFWav2Vec2ForCTC.from_pretrained(\n",
    "            args.model_name,\n",
    "            from_pt=True,\n",
    "            ctc_loss_reduction=\"mean\",\n",
    "            gradient_checkpointing=True,\n",
    "            pad_token_id=self.config.processor.tokenizer.pad_token_id,\n",
    "            vocab_size=len(self.config.processor.tokenizer),\n",
    "            use_bfloat16=True)\n",
    "        self.model.freeze_feature_extractor()\n",
    "        \n",
    "        self.model_name = f\"model_{int(self.args.n_samples/1000)}k\"\n",
    "        self.log_path = f\"{self.args.main_dir}/model_weights/{self.model_name}.csv\"\n",
    "        if not os.path.exists(self.log_path):\n",
    "            print(\"Log file created.\")\n",
    "            columns = \"epoch,loss,per,val_loss,val_per\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(columns)\n",
    "\n",
    "    def decoder(self, labels, logits):\n",
    "        labels = tf.where(labels == -100, x=0, y=labels)\n",
    "        logits = tf.argmax(logits, axis=-1)\n",
    "        return labels, logits\n",
    "\n",
    "    def display(self, epoch, t_labels, t_logits, v_labels, v_logits):\n",
    "        t_labels = self.config.processor.batch_decode(\n",
    "            t_labels, skip_special_tokens=True, group_tokens=False)\n",
    "        v_labels = self.config.processor.batch_decode(\n",
    "            v_labels, skip_special_tokens=True, group_tokens=False)\n",
    "        t_logits = self.config.processor.batch_decode(\n",
    "            t_logits, skip_special_tokens=True, group_tokens=False)\n",
    "        v_logits = self.config.processor.batch_decode(\n",
    "            v_logits, skip_special_tokens=True, group_tokens=False)\n",
    "\n",
    "        print(\"-\" * 129)\n",
    "        print(\"Training\")\n",
    "        for y_true, y_pred in zip(t_labels, t_logits):\n",
    "            print(f\"Target:    {y_true}\")\n",
    "            print(f\"Predicted: {y_pred}\") \n",
    "\n",
    "        print(\"\\nValidation\")\n",
    "        for y_true, y_pred in zip(v_labels, v_logits):\n",
    "            print(f\"Target:    {y_true}\")\n",
    "            print(f\"Predicted: {y_pred}\")\n",
    "        print(\"-\" * 129)\n",
    "\n",
    "    def fit(self):\n",
    "        # Checkpoint manager\n",
    "        self.ckpt_dir = f\"{self.args.main_dir}/checkpoints\"\n",
    "        self.ckpt = tf.train.Checkpoint(self.model)\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(\n",
    "            checkpoint=self.ckpt, directory=self.ckpt_dir, max_to_keep=5)\n",
    "\n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            self.start_epoch = int(self.ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "            self.ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "            print(f\"Resuming from epoch {self.start_epoch + 1}...\")\n",
    "        else:\n",
    "            self.start_epoch = 0\n",
    "            print(\"Starting from epoch 1...\")\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.args.epochs+1):\n",
    "            print(f\"Epoch {epoch+1}/{self.args.epochs}: Learning rate @ {self.optimizer.lr(epoch):.2e}\")\n",
    "            stateful_metrics = [\"loss\", \"per\", \"val_loss\", \"val_per\"]\n",
    "            progbar = tf.keras.utils.Progbar(\n",
    "                self.args.train_steps, interval=0.05,\n",
    "                stateful_metrics=stateful_metrics)\n",
    "\n",
    "            # Training loop\n",
    "            for step, t_batch in enumerate(self.train_dataset):\n",
    "                t_inputs = t_batch['input_values']\n",
    "                t_labels = t_batch['labels']\n",
    "                with tf.GradientTape() as tape:\n",
    "                    t_loss, t_logits = self.model(\n",
    "                        input_values=t_inputs, labels=t_labels, training=True)[:2]\n",
    "                gradients = tape.gradient(t_loss, self.model.trainable_weights)  \n",
    "                self.optimizer.apply_gradients(zip(gradients, self.model.trainable_weights))  \n",
    "\n",
    "                t_labels, t_logits = self.decoder(t_labels, t_logits)\n",
    "                self.per_metrics.update_state(t_labels, t_logits) \n",
    "                t_per = self.per_metrics.result()\n",
    "                t_values = [(\"loss\", t_loss), (\"per\", t_per)]\n",
    "                progbar.update(step, values=t_values, finalize=False)\n",
    "            self.per_metrics.reset_states()\n",
    "            \n",
    "            # Validation loop\n",
    "            for v_batch in self.val_dataset:\n",
    "                v_inputs = v_batch['input_values']\n",
    "                v_labels = v_batch['labels']\n",
    "                v_loss, v_logits = self.model(\n",
    "                    input_values=v_inputs, labels=v_labels, training=False)[:2]       \n",
    "                v_labels, v_logits = self.decoder(v_labels, v_logits)         \n",
    "                self.per_metrics.update_state(v_labels, v_logits)\n",
    "\n",
    "            v_per = self.per_metrics.result()\n",
    "            v_values = [\n",
    "                (\"loss\", t_loss), (\"per\", t_per), (\"val_loss\", v_loss),\n",
    "                (\"val_per\", v_per)]\n",
    "            progbar.update(self.args.train_steps, values=v_values, finalize=True)\n",
    "            self.per_metrics.reset_states()\n",
    "\n",
    "            # Print sample transcriptions for both loops\n",
    "            self.display(epoch, t_labels, t_logits, v_labels, v_logits)\n",
    "\n",
    "            # Checkpointing\n",
    "            self.ckpt.save(file_prefix=f\"{self.ckpt_dir}/{self.model_name}\")\n",
    "\n",
    "            # Logging\n",
    "            log = f\"{epoch+1},{t_loss},{t_per},{v_loss},{v_per}\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(log)\n",
    "\n",
    "            save_path = f\"{self.args.main_dir}/model_weights\"\n",
    "            self.model.save_weights(f\"{save_path}/{self.model_name}_{epoch+1}.h5\")\n",
    "\n",
    "Trainer(args).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = pd.read_csv(\"E:\\Datasets\\ASR-dataset\\model_weights\\model_40k.csv\", index_col=\"epoch\")\n",
    "# history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "# sns.lineplot(x=history.index, y=history['per'], label=\"per\", ax=ax1)\n",
    "# sns.lineplot(x=history.index, y=history['wer'], label=\"wer\", ax=ax2)\n",
    "# sns.lineplot(x=history.index, y=history['val_per'], label=\"val_per\", ax=ax1)\n",
    "# sns.lineplot(x=history.index, y=history['val_wer'], label=\"val_wer\", ax=ax2)\n",
    "# plt.suptitle(\"Acoustic model\")\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"acoustic_history.png\", transparent=False, bbox_inches='tight')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b851d2923cfa3a2562599062e05fd9893d86a7c009c64d8ad3756552e4dd5f41"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('tf-gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
