{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from convert_romaji import Romaji2Kana\n",
    "\n",
    "import cutlet\n",
    "import jiwer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2Processor,\n",
    "    TFWav2Vec2ForCTC,\n",
    "    logging)\n",
    "\n",
    "from convert_romaji import Romaji2Kana\n",
    "\n",
    "def seed_everything(SEED):\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    print(\"Random seed set.\")\n",
    "\n",
    "seed_everything(42)\n",
    "tf.get_logger().setLevel('FATAL')\n",
    "logging.set_verbosity_error()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=4, buffer_size=1024, epochs=20, learning_rate=0.0001, lr_max=0.0001, lr_min=1e-07, lr_start=1e-07, main_dir='E://Datasets/Acoustic_model', model_name='facebook/wav2vec2-base', n_cycles=0.5, n_samples=40000, n_shards=20, n_train=30000, n_val=10000, random_state=42, sample_rate=16000, sustain_epochs=1, test_size=0.25, train_steps=7500, val_steps=2500, vocab_size=38, warmup_epochs=2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ArgParser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # DataLoader\n",
    "    parser.add_argument(\"--main_dir\", default=\"E://Datasets/Acoustic_model\")\n",
    "    parser.add_argument(\"--sample_rate\", default=16000)\n",
    "    parser.add_argument(\"--test_size\", default=0.25)\n",
    "    parser.add_argument(\"--random_state\", default=42)\n",
    "    parser.add_argument(\"--batch_size\", default=4)\n",
    "    parser.add_argument(\"--n_shards\", default=20)\n",
    "    parser.add_argument(\"--buffer_size\", default=1024)\n",
    "    parser.add_argument(\"--n_samples\", default=40000)\n",
    "\n",
    "    # Trainer\n",
    "    parser.add_argument(\"--model_name\", default=\"facebook/wav2vec2-base\")\n",
    "    parser.add_argument(\"--epochs\", default=20)\n",
    "\n",
    "    # Scheduler\n",
    "    parser.add_argument(\"--learning_rate\", default=1e-4)\n",
    "    parser.add_argument(\"--lr_start\", default=1e-7)\n",
    "    parser.add_argument(\"--lr_min\", default=1e-7)\n",
    "    parser.add_argument(\"--lr_max\", default=1e-4)\n",
    "    parser.add_argument(\"--n_cycles\", default=0.5)\n",
    "    parser.add_argument(\"--warmup_epochs\", default=2)\n",
    "    parser.add_argument(\"--sustain_epochs\", default=1)\n",
    "\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    with open(f\"{args.main_dir}/vocab.json\", \"r\") as f:\n",
    "        vocab_size = len(json.load(f))\n",
    "   \n",
    "    n_train = int(args.n_samples * (1 - args.test_size))\n",
    "    n_val = int(args.n_samples * args.test_size)\n",
    "    train_steps = int(np.ceil(n_train / args.batch_size))\n",
    "    val_steps = int(np.ceil(n_val / args.batch_size))\n",
    "\n",
    "    parser.add_argument(\"--vocab_size\", default=vocab_size)\n",
    "    parser.add_argument(\"--n_train\", default=n_train)\n",
    "    parser.add_argument(\"--n_val\", default=n_val)\n",
    "    parser.add_argument(\"--train_steps\", default=train_steps)  \n",
    "    parser.add_argument(\"--val_steps\", default=val_steps)\n",
    "    \n",
    "    return parser.parse_known_args()[0]\n",
    "\n",
    "args = ArgParser()\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.data_dir = \"E:/Datasets/Acoustic_model/raw_data\"\n",
    "        self.data = pd.concat([\n",
    "            self.get_kokoro(),\n",
    "            self.get_jsut(),\n",
    "            self.get_commonvoice()], \n",
    "            ignore_index=True)\n",
    "        self.kanji_unicode = self.get_kanji_unicode()\n",
    "        self.katsu = cutlet.Cutlet()\n",
    "        self.katsu.use_foreign_spelling = False\n",
    "    \n",
    "        tqdm.pandas()\n",
    "        # Remove rows that contains non-kanji characters\n",
    "        self.data = self.data[self.data['sentence'].progress_apply(self.check_kanji)] \n",
    "\n",
    "        # Remove words within parenthesis\n",
    "        parenthesis =  r\"\\（.*\\）|\\(.*\\)|\\「.*\\」|\\『.*\\』\"\n",
    "        self.data = self.data[~self.data['sentence'].str.contains(parenthesis)]\n",
    "\n",
    "        # Remove punctuations from sentences\n",
    "        self.data['sentence'] = self.data['sentence'].progress_apply(self.clean_kanji)\n",
    "        self.data['romaji'] = self.data['sentence'].progress_apply(self.kanji2romaji)\n",
    "        self.data['length'] = self.data['path'].progress_apply(self.get_length)\n",
    "        self.data = self.data[self.data['length'].between(48000, 85000)]\n",
    "        self.data = self.data.reset_index(drop=True)\n",
    "        self.data = self.data.sample(n=self.args.n_samples, random_state=42, ignore_index=True)\n",
    "        self.data.sort_values(by=\"length\", axis=0, ascending=True, inplace=True, ignore_index=True)\n",
    "        self.data.to_csv(\n",
    "            f\"{self.args.main_dir}/ASRDataset.csv\", \n",
    "            encoding=\"utf-8\", index=False)\n",
    "\n",
    "    def get_kokoro(self):\n",
    "        data = []\n",
    "        transcript_path = f\"{self.data_dir}/KOKORO-dataset/transcripts/*.metadata.txt\"\n",
    "        for transcript in glob.glob(transcript_path):\n",
    "            with open(transcript, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f.readlines():\n",
    "                    data.append(line.split(\"|\"))\n",
    "\n",
    "        data = pd.DataFrame(\n",
    "            data, columns=[\n",
    "                'text_id', 'path', 'start_idx', \n",
    "                'end_idx', 'sentence', 'phonemes'])       \n",
    "\n",
    "        # paths = data['path'].unique()\n",
    "        # for path in tqdm(paths, total=len(paths)):\n",
    "        #     folder_name = path.split(\"_\", 1)[0]\n",
    "        #     in_path = os.path.join(f\"{self.data_dir}/KOKORO-dataset\", folder_name, path)\n",
    "        #     y, sr = librosa.load(in_path, sr=None)\n",
    "        #     for text_id in data.loc[data['path']==path, 'text_id']:\n",
    "        #         out_path = os.path.join(self.args.main_dir, 'wav_cleaned', text_id) + \".wav\"\n",
    "        #         if not os.path.exists(out_path):\n",
    "        #             start_idx = int(data.loc[data['text_id']==text_id, 'start_idx'].item())\n",
    "        #             end_idx = int(data.loc[data['text_id']==text_id, 'end_idx'].item())\n",
    "        #             y_slice = librosa.resample(\n",
    "        #                 y[start_idx:end_idx], orig_sr=sr, target_sr=self.args.sample_rate)\n",
    "        #             sf.write(out_path, y_slice, samplerate=self.args.sample_rate, subtype='PCM_16')\n",
    "\n",
    "        data = data[['text_id', 'sentence']]\n",
    "        data['text_id'] = data['text_id'].apply(lambda x: x + \".wav\")\n",
    "        data.columns = ['path', 'sentence']\n",
    "        data['corpus'] = ['kokoro'] * len(data)\n",
    "        return data\n",
    "\n",
    "    def get_jsut(self):\n",
    "        filenames, sentences = [], []\n",
    "        for transcript in glob.glob(f\"{self.data_dir}/JSUT-dataset/*/transcript_utf8.txt\"):\n",
    "            file_path = transcript.rsplit(\"\\\\\", 1)[0]\n",
    "            with open(transcript, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines: \n",
    "                    filename, sentence = line.split(\":\")\n",
    "                    filenames.append(os.path.join(file_path, \"wav\", filename) + \".wav\")\n",
    "                    sentences.append(sentence.strip(\"\\n\"))\n",
    "        data = pd.DataFrame({'path': filenames, 'sentence': sentences}) \n",
    "        data['corpus'] = ['jsut'] * len(data)\n",
    "        for i, in_path in tqdm(enumerate(data['path']), total=len(data['path'])):\n",
    "            in_path = in_path.replace(\"\\\\\", \"/\")\n",
    "            out_path = f\"{self.args.main_dir}\\wav_cleaned\"\n",
    "            filename = in_path.rsplit(\"/\", 1)[-1]\n",
    "            out_path = os.path.join(out_path, filename)\n",
    "            if not os.path.exists(out_path):\n",
    "                subprocess.call([\n",
    "                    \"ffmpeg\", \"-i\", in_path,\"-acodec\", \"pcm_s16le\", \n",
    "                    \"-ar\", str(self.args.sample_rate), out_path])\n",
    "            data['path'][i] = filename\n",
    "        return data\n",
    "\n",
    "    def get_commonvoice(self):\n",
    "        data = pd.read_csv(f\"{self.data_dir}/CommonVoice-dataset/validated.tsv\", sep=\"\\t\")\n",
    "        data = data[['path', 'sentence']]    \n",
    "        data['path'] = data['path'].apply(\n",
    "            lambda x: f\"{self.data_dir}/CommonVoice-dataset/clips/\" + x)\n",
    "        data['corpus'] = ['common_voice'] * len(data)\n",
    "        for i, in_path in tqdm(enumerate(data['path']), total=len(data['path'])):\n",
    "            in_path = in_path.replace(\"\\\\\", \"/\")\n",
    "            out_path = f\"{self.args.main_dir}\\wav_cleaned\"\n",
    "            filename = in_path.rsplit(\"/\", 1)[-1]\n",
    "            filename = filename.replace(\"mp3\", \"wav\")\n",
    "            out_path = os.path.join(out_path, filename)\n",
    "            if not os.path.exists(out_path):\n",
    "                subprocess.call([\n",
    "                    \"ffmpeg\", \"-i\", in_path,\"-acodec\", \"pcm_s16le\", \n",
    "                    \"-ar\", str(self.args.sample_rate), out_path])\n",
    "            data['path'][i] = filename\n",
    "        return data\n",
    "\n",
    "    def get_kanji_unicode(self):\n",
    "        vocab = set()\n",
    "        with open(\n",
    "            f\"{self.data_dir}/kanji_unicode.txt\", \n",
    "            encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                for char in line.split()[1:]:\n",
    "                    vocab.add(char)\n",
    "        return \"|\".join(sorted(vocab))\n",
    "    \n",
    "    def check_kanji(self, sentence):\n",
    "        pattern = f\"[^{self.kanji_unicode}]\"\n",
    "        if re.match(pattern, sentence) == None:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def clean_kanji(self, sentence):\n",
    "        sentence = \"\".join(sentence.split())\n",
    "        pattern = r\"[・\\。\\！\\.\\？\\、]\"\n",
    "        sentence = re.sub(pattern, \"\", sentence)\n",
    "        return sentence\n",
    "\n",
    "    def kanji2romaji(self, sentence):\n",
    "        try:\n",
    "            sentence = self.katsu.romaji(sentence)\n",
    "            sentence = sentence.replace(\" \", \"|\").lower()\n",
    "        except:\n",
    "            sentence = None\n",
    "        return sentence\n",
    "\n",
    "    def get_length(self, path):\n",
    "        path = os.path.join(self.args.main_dir, 'wav_cleaned', path)\n",
    "        y, sr = librosa.load(path, sr=None)\n",
    "        return len(y)\n",
    "\n",
    "# data = Dataset(args).data\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 4))\n",
    "# sns.histplot(x=data['length'], hue=data['corpus'], palette=\"bright\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, args):\n",
    "        tokenizer = Wav2Vec2CTCTokenizer(\n",
    "            vocab_file=f\"{args.main_dir}/vocab.json\",\n",
    "            do_lower_case=False)\n",
    "\n",
    "        feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "            feature_size=1,\n",
    "            sampling_rate=args.sample_rate,\n",
    "            padding_value=0.0,\n",
    "            do_normalize=False,\n",
    "            return_attention_mask=False)\n",
    "\n",
    "        self.processor = Wav2Vec2Processor(\n",
    "            feature_extractor=feature_extractor,\n",
    "            tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFRWriter():\n",
    "    def __init__(self, args):\n",
    "        self.data = pd.read_csv(os.path.join(args.main_dir, \"ASRDataset.csv\"), encoding=\"utf-8\")\n",
    "        self.args = args\n",
    "        self.config = Config(args)\n",
    "\n",
    "    def _bytes_feature(self, value):\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    def _int64_feature(self, value):\n",
    "        \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "    def _float_feature(self, value):\n",
    "        \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "    def serialize_example(self, *args):\n",
    "        feature = {\n",
    "            'input_values': self._bytes_feature(args[0]),\n",
    "            'labels': self._bytes_feature(args[1])}\n",
    "\n",
    "        example_proto = tf.train.Example(\n",
    "            features=tf.train.Features(feature=feature))\n",
    "        return example_proto.SerializeToString()\n",
    "\n",
    "    def get_labels(self, sample):\n",
    "        labels = self.data.loc[self.data['path']==sample, \"romaji\"].item()\n",
    "        labels = (self.config.processor.tokenizer.bos_token + labels + \n",
    "            self.config.processor.tokenizer.eos_token)\n",
    "        labels = self.config.processor.tokenizer(labels, is_split_into_words=True).input_ids\n",
    "        return tf.convert_to_tensor(labels, dtype=tf.int32)\n",
    "\n",
    "    def get_audio(self, sample):\n",
    "        path = os.path.join(self.args.main_dir, \"wav_cleaned\", sample)\n",
    "        audio = librosa.load(path, sr=None)[0]\n",
    "        audio /= audio.max()\n",
    "        return tf.convert_to_tensor(audio, dtype=tf.float32)\n",
    "\n",
    "    def get_shards(self):\n",
    "        skf = KFold(n_splits=self.args.n_shards, shuffle=False)\n",
    "        return [\n",
    "            list(map(lambda x: self.data['path'][x], j))\n",
    "            for i, j in skf.split(self.data['path'])]\n",
    "\n",
    "    def get_shard_data(self, samples):\n",
    "        for sample in samples:\n",
    "            audio = self.get_audio(sample)\n",
    "            labels = self.get_labels(sample)\n",
    "            yield {\n",
    "                'input_values': tf.io.serialize_tensor(audio),\n",
    "                'labels': tf.io.serialize_tensor(labels)}\n",
    "\n",
    "    def write(self):\n",
    "        for shard, samples in tqdm(enumerate(self.get_shards()), total=self.args.n_shards):\n",
    "            with tf.io.TFRecordWriter(f\"{self.args.main_dir}/wav2vec2_tfrec/shard_{shard+1}.tfrec\") as f:\n",
    "                for sample in self.get_shard_data(samples):\n",
    "                    example = self.serialize_example(\n",
    "                        sample['input_values'],\n",
    "                        sample['labels'])\n",
    "                    f.write(example)\n",
    "\n",
    "# TFRWriter(args).write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, args):\n",
    "        self.files = glob.glob(args.main_dir + \"/wav2vec2_tfrec/*.tfrec\")\n",
    "        self.args = args\n",
    "        self.AUTOTUNE = tf.data.AUTOTUNE\n",
    "        self.train_files, self.val_files = train_test_split(\n",
    "            self.files, test_size=args.test_size, shuffle=True, \n",
    "            random_state=args.random_state)\n",
    "        self.train = self.get_train()\n",
    "        self.val = self.get_val()\n",
    "\n",
    "    def read_tfrecord(self, example):\n",
    "        feature_description = {\n",
    "            'input_values': tf.io.FixedLenFeature([], tf.string),\n",
    "            'labels': tf.io.FixedLenFeature([], tf.string)}\n",
    "        \n",
    "        example = tf.io.parse_single_example(example, feature_description)\n",
    "        example['input_values'] = tf.io.parse_tensor(\n",
    "            example['input_values'], out_type=tf.float32)\n",
    "        example['labels'] = tf.io.parse_tensor(\n",
    "            example['labels'], out_type=tf.int32)\n",
    "        return example\n",
    "\n",
    "    def load_dataset(self, files):\n",
    "        ignore_order = tf.data.Options()\n",
    "        ignore_order.experimental_deterministic = False\n",
    "        dataset = tf.data.TFRecordDataset(files)\n",
    "        dataset = dataset.with_options(ignore_order)\n",
    "        dataset = dataset.map(self.read_tfrecord, num_parallel_calls=self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_train(self):\n",
    "        dataset = self.load_dataset(self.train_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_values': [None],\n",
    "                'labels': [None]},\n",
    "            padding_values={\n",
    "                'input_values': tf.constant(0, dtype=tf.float32), \n",
    "                'labels': tf.constant(-100, dtype=tf.int32)})        \n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_val(self):\n",
    "        dataset = self.load_dataset(self.val_files)\n",
    "        dataset = dataset.padded_batch(\n",
    "            self.args.batch_size,\n",
    "            padded_shapes={\n",
    "                'input_values': [None],\n",
    "                'labels': [None]},\n",
    "            padding_values={\n",
    "                'input_values': tf.constant(0, dtype=tf.float32),\n",
    "                'labels': tf.constant(-100, dtype=tf.int32)})\n",
    "        dataset = dataset.shuffle(self.args.buffer_size)\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "# train = DataLoader(args).train\n",
    "# output = next(iter(train))\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16,10))\n",
    "# for i, array in enumerate(train.take(16)):\n",
    "#     plt.subplot(4, 4, i+1)\n",
    "#     y = array['input_values'].numpy()\n",
    "#     librosa.display.waveplot(y=y, sr=16000)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class PERMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, tokenizer, name=\"PER\", **kwargs):\n",
    "        super(PERMetric, self).__init__(name=name,  **kwargs)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.accumulator = self.add_weight(name=\"total_per\", initializer=\"zeros\")\n",
    "        self.counter = self.add_weight(name=\"per_count\", initializer=\"zeros\")    \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Re-encode logits\n",
    "        try:\n",
    "            y_pred = self.tokenizer(y_pred, is_split_into_words=False).input_ids\n",
    "        except:\n",
    "            y_pred = [[self.tokenizer.pad_token_id] for _ in range(len(y_pred))]\n",
    "\n",
    "        hypothesis = tf.ragged.constant(y_pred).to_sparse()\n",
    "\n",
    "        # Convert dense to sparse tensor for edit_distance function\n",
    "        truth = tf.RaggedTensor.from_tensor(y_true, padding=0).to_sparse()\n",
    "\n",
    "        # Calculate Levenshtein distance\n",
    "        distance = tf.edit_distance(hypothesis, truth, normalize=True)\n",
    "\n",
    "        # Add distance and number of samples to variables\n",
    "        self.accumulator.assign_add(tf.reduce_mean(distance))\n",
    "        self.counter.assign_add(len(y_true))\n",
    "\n",
    "    def result(self):\n",
    "        # Divides accumulated distance scores against number of samples passed,\n",
    "        # mimics mean reduction over batch\n",
    "        return tf.math.divide_no_nan(self.accumulator, self.counter)   \n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.accumulator.assign(0.0)\n",
    "        self.counter.assign(0.0)\n",
    "\n",
    "class WERMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"WER\", **kwargs):\n",
    "        super(WERMetric, self).__init__(name=name,  **kwargs)\n",
    "        self.accumulator = self.add_weight(name=\"total_wer\", initializer=\"zeros\")\n",
    "        self.counter = self.add_weight(name=\"wer_count\", initializer=\"zeros\")    \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Calculate wer score\n",
    "        wer = jiwer.wer(y_true, y_pred)\n",
    "\n",
    "        # Add distance and number of batches to variables\n",
    "        self.accumulator.assign_add(wer)\n",
    "        self.counter.assign_add(1)\n",
    "\n",
    "    def result(self):\n",
    "        # Divides accumulated distance scores against number of batches passed,\n",
    "        # mimics mean reduction over batch\n",
    "        return tf.math.divide_no_nan(self.accumulator, self.counter)   \n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.accumulator.assign(0.0)\n",
    "        self.counter.assign(0.0)\n",
    "\n",
    "class CosineDecayWithWarmup(LearningRateSchedule):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "    def __call__(self, epoch):  \n",
    "        if epoch < self.args.warmup_epochs:\n",
    "            lr = ((self.args.lr_max - self.args.lr_start) / self.args.warmup_epochs) * epoch + self.args.lr_start\n",
    "        elif epoch < (self.args.warmup_epochs + self.args.sustain_epochs):\n",
    "            lr = self.args.lr_max\n",
    "        else:\n",
    "            progress = ((epoch - self.args.warmup_epochs - self.args.sustain_epochs) / \n",
    "            (self.args.epochs - self.args.warmup_epochs - self.args.sustain_epochs))\n",
    "            lr = (self.args.lr_max-self.args.lr_min) * (0.5 * (1.0 + tf.math.cos((22/7) * \n",
    "                self.args.n_cycles * 2.0 * progress)))\n",
    "            if self.args.lr_min is not None:\n",
    "                lr = tf.math.maximum(self.args.lr_min, lr)\n",
    "        return lr\n",
    "\n",
    "    def plot(self):\n",
    "        epochs = range(self.args.epochs+1)\n",
    "        lr = [self(epoch) for epoch in epochs]\n",
    "        plt.plot(epochs, lr)\n",
    "        plt.xlabel(\"learning_rate\")\n",
    "        plt.ylabel(\"epochs\")\n",
    "        plt.show()\n",
    "\n",
    "CosineDecayWithWarmup(args).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\transformers\\configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from epoch 4...\n",
      "Epoch 4/20: Learning rate @ 9.99e-05\n",
      "7500/7500 [==============================] - 15968s 2s/step - loss: 19.2818 - per: 0.0337 - wer: 0.3355 - val_loss: 2.5875 - val_per: 0.0280 - val_wer: 0.2580\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Training\n",
      "Target:    san busaku wa hinode kara hinoiri made maru ichi nichi ni watatte junban ni okonawaremashita\n",
      "Predicted: sanbusake wa hinodekara hinoiri made maruichinchi ni watate junban wokonawaremashita\n",
      "Target:    shikashi kyoukai no kenchikukatachi wa shinrou no iriguchi no mae ni heya wo tate tsuzukemashita\n",
      "Predicted: shikashikinokae no kenchikukatachi wa shinrou no irigechiumai ni heya wo tatezuzukemashita\n",
      "Target:    yue ni shakai wa honshitsuteki ni ideyateki na mono wo fukumanakereba naranai\n",
      "Predicted: yue ni shakai wa honshitsuteki ni iyateki na mono wo fukumanakereba naranai\n",
      "Target:    ken taikai ni dete kara kounai no chuumoku wo atsumeru you ni natta\n",
      "Predicted: kentaikai ni dete kara konai no chumoku wo atsumeru you ni nata\n",
      "\n",
      "Validation\n",
      "Target:    hikaeme ni itte mo kare wa akunin no you sa\n",
      "Predicted: hikaeme ni ite mo kare wa akunin no yousa\n",
      "Target:    kare ni hitori de au to wa kanojo mo ii dokyou da\n",
      "Predicted: kare ni hitori de au to wa kanojo mo i dokyou da\n",
      "Target:    musumetachi no oshaberi wa itsu made mo tsuzuita\n",
      "Predicted: musumetachi no oshaberi wa itsu made mo tsuzuita\n",
      "Target:    sumi da gawa de mo nakanu to wa hoshou ga dekin\n",
      "Predicted: sumidaga de mo nakanu to wa hoshou ga dekin\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 5/20: Learning rate @ 9.90e-05\n",
      "7499/7500 [============================>.] - ETA: 1s - loss: 11.1670 - per: 0.0319 - wer: 0.3084"
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.config = Config(args)\n",
    "        self.train_dataset = DataLoader(args).train\n",
    "        self.val_dataset = DataLoader(args).val\n",
    "        schedule = CosineDecayWithWarmup(args)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(schedule)\n",
    "        self.per_metrics = PERMetric(tokenizer=self.config.processor.tokenizer)\n",
    "        self.wer_metrics = WERMetric()\n",
    "        self.model = TFWav2Vec2ForCTC.from_pretrained(\n",
    "            args.model_name,\n",
    "            from_pt=True,\n",
    "            ctc_loss_reduction=\"mean\",\n",
    "            gradient_checkpointing=True,\n",
    "            pad_token_id=self.config.processor.tokenizer.pad_token_id,\n",
    "            vocab_size=len(self.config.processor.tokenizer),\n",
    "            use_bfloat16=True)\n",
    "        self.model.freeze_feature_extractor()\n",
    "        \n",
    "        self.model_name = f\"model_{int(self.args.n_samples/1000)}k\"\n",
    "        self.log_path = f\"{self.args.main_dir}/model_weights/{self.model_name}.csv\"\n",
    "        if not os.path.exists(self.log_path):\n",
    "            print(\"Log file created.\")\n",
    "            columns = \"epoch,loss,per,wer,val_loss,val_per,val_wer\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(columns)\n",
    "\n",
    "    def decoder(self, labels, logits):\n",
    "        labels = tf.where(labels == -100, x=0, y=labels)\n",
    "        logits = tf.argmax(logits, axis=-1)\n",
    "        logits = self.config.processor.tokenizer.batch_decode(\n",
    "            logits, group_tokens=True, skip_special_tokens=True)\n",
    "        return labels, logits\n",
    "\n",
    "    def display(self, epoch, t_labels, t_logits, v_labels, v_logits):\n",
    "        print(\"-\" * 129)\n",
    "        print(\"Training\")\n",
    "        for y_true, y_pred in zip(t_labels, t_logits):\n",
    "            print(f\"Target:    {y_true}\")\n",
    "            print(f\"Predicted: {y_pred}\") \n",
    "\n",
    "        print(\"\\nValidation\")\n",
    "        for y_true, y_pred in zip(v_labels, v_logits):\n",
    "            print(f\"Target:    {y_true}\")\n",
    "            print(f\"Predicted: {y_pred}\")\n",
    "        print(\"-\" * 129)\n",
    "\n",
    "    def fit(self):\n",
    "        # Checkpoint manager\n",
    "        self.ckpt_dir = f\"{self.args.main_dir}/checkpoints\"\n",
    "        self.ckpt = tf.train.Checkpoint(self.model)\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(\n",
    "            checkpoint=self.ckpt, directory=self.ckpt_dir, max_to_keep=5)\n",
    "\n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            self.start_epoch = int(self.ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "            self.ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "            print(f\"Resuming from epoch {self.start_epoch + 1}...\")\n",
    "        else:\n",
    "            self.start_epoch = 0\n",
    "            print(\"Starting from epoch 1...\")\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.args.epochs+1):\n",
    "            print(f\"Epoch {epoch+1}/{self.args.epochs}: Learning rate @ {self.optimizer.lr(epoch):.2e}\")\n",
    "            stateful_metrics = [\"loss\", \"per\", \"wer\", \"val_loss\", \"val_per\", \"val_wer\"]\n",
    "            progbar = tf.keras.utils.Progbar(\n",
    "                self.args.train_steps, interval=0.05,\n",
    "                stateful_metrics=stateful_metrics)\n",
    "\n",
    "            # Training loop\n",
    "            for step, t_batch in enumerate(self.train_dataset):\n",
    "                t_inputs = t_batch['input_values']\n",
    "                t_labels = t_batch['labels']\n",
    "                with tf.GradientTape() as tape:\n",
    "                    t_loss, t_logits = self.model(\n",
    "                        input_values=t_inputs, labels=t_labels, training=True)[:2]\n",
    "                gradients = tape.gradient(t_loss, self.model.trainable_weights)  \n",
    "                self.optimizer.apply_gradients(zip(gradients, self.model.trainable_weights))    \n",
    "                t_labels, t_logits = self.decoder(t_labels, t_logits)\n",
    "                self.per_metrics.update_state(t_labels, t_logits)\n",
    "                t_labels = self.config.processor.tokenizer.batch_decode(\n",
    "                    t_labels, group_tokens=False, skip_special_tokens=True)   \n",
    "                self.wer_metrics.update_state(t_labels, t_logits)\n",
    "                t_per = self.per_metrics.result()\n",
    "                t_wer = self.wer_metrics.result()\n",
    "                t_values = [(\"loss\", t_loss), (\"per\", t_per), (\"wer\", t_wer)]\n",
    "                progbar.update(step, values=t_values, finalize=False)\n",
    "\n",
    "            self.per_metrics.reset_states()\n",
    "            self.wer_metrics.reset_states()\n",
    "            \n",
    "            # Validation loop\n",
    "            for v_batch in self.val_dataset:\n",
    "                v_inputs = v_batch['input_values']\n",
    "                v_labels = v_batch['labels']\n",
    "                v_loss, v_logits = self.model(\n",
    "                    input_values=v_inputs, labels=v_labels, training=False)[:2]       \n",
    "                v_labels, v_logits = self.decoder(v_labels, v_logits)               \n",
    "                self.per_metrics.update_state(v_labels, v_logits)\n",
    "                v_labels = self.config.processor.batch_decode(\n",
    "                    v_labels, group_tokens=False, skip_special_tokens=True)\n",
    "                self.wer_metrics.update_state(v_labels, v_logits)\n",
    "\n",
    "            v_per = self.per_metrics.result()\n",
    "            v_wer = self.wer_metrics.result()\n",
    "            v_values = [\n",
    "                (\"loss\", t_loss), (\"per\", t_per), (\"wer\", t_wer), (\"val_loss\", v_loss),\n",
    "                (\"val_per\", v_per), (\"val_wer\", v_wer)]\n",
    "            progbar.update(self.args.train_steps, values=v_values, finalize=True)\n",
    "            self.per_metrics.reset_states()\n",
    "            self.wer_metrics.reset_states()\n",
    "\n",
    "            # Print sample transcriptions for both loops\n",
    "            self.display(epoch, t_labels, t_logits, v_labels, v_logits)\n",
    "\n",
    "            # Checkpointing\n",
    "            self.ckpt.save(file_prefix=f\"{self.ckpt_dir}/{self.model_name}\")\n",
    "\n",
    "            # Logging\n",
    "            log = f\"{epoch+1},{t_loss},{t_per},{t_wer},{v_loss},{v_per},{v_wer}\\n\"\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(log)\n",
    "\n",
    "            save_path = f\"{self.args.main_dir}/model_weights\"\n",
    "            self.model.save_weights(f\"{save_path}/{self.model_name}_{epoch+1}.h5\")\n",
    "\n",
    "Trainer(args).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pd.read_csv(\"E:\\Datasets\\ASR-dataset\\model_weights\\model_40k.csv\", index_col=\"epoch\")\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "sns.lineplot(x=history.index, y=history['per'], label=\"per\", ax=ax1)\n",
    "sns.lineplot(x=history.index, y=history['wer'], label=\"wer\", ax=ax2)\n",
    "sns.lineplot(x=history.index, y=history['val_per'], label=\"val_per\", ax=ax1)\n",
    "sns.lineplot(x=history.index, y=history['val_wer'], label=\"val_wer\", ax=ax2)\n",
    "plt.suptitle(\"Acoustic model\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"acoustic_history.png\", transparent=False, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b851d2923cfa3a2562599062e05fd9893d86a7c009c64d8ad3756552e4dd5f41"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('tf-gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
