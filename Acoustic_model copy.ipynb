{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "import re\r\n",
    "import glob\r\n",
    "import json\r\n",
    "import random\r\n",
    "import argparse\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "import librosa\r\n",
    "import librosa.display\r\n",
    "import soundfile as sf\r\n",
    "\r\n",
    "from tqdm import tqdm\r\n",
    "import subprocess\r\n",
    "from functools import partial\r\n",
    "from collections import Counter\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "import MeCab\r\n",
    "import cutlet\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split, KFold\r\n",
    "\r\n",
    "import tensorflow as tf\r\n",
    "import tensorflow_io as tfio\r\n",
    "from tensorflow.keras.layers import *\r\n",
    "\r\n",
    "from transformers import (\r\n",
    "    Wav2Vec2CTCTokenizer,\r\n",
    "    TFWav2Vec2ForCTC,\r\n",
    "    Wav2Vec2Processor,\r\n",
    "    Wav2Vec2FeatureExtractor)\r\n",
    "\r\n",
    "def seed_everything(SEED):\r\n",
    "    random.seed(SEED)\r\n",
    "    np.random.seed(SEED)\r\n",
    "    tf.random.set_seed(SEED)\r\n",
    "    print(\"Random seed set.\")\r\n",
    "\r\n",
    "seed_everything(42)\r\n",
    "tf.get_logger().setLevel('FATAL')\r\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Dataset:\r\n",
    "    def __init__(self):\r\n",
    "        self.main_dir = \"E://Datasets/ASR-dataset\"\r\n",
    "        self.sample_rate = 16000\r\n",
    "        self.n_shards = 20\r\n",
    "        self.data = pd.concat([\r\n",
    "            self.get_kokoro(),\r\n",
    "            self.get_jsut(),\r\n",
    "            self.get_commonvoice()\r\n",
    "            ], \r\n",
    "            ignore_index=True)\r\n",
    "        self.katsu = cutlet.Cutlet()\r\n",
    "        self.wakati = MeCab.Tagger(\"-Owakati\")\r\n",
    "    \r\n",
    "        tqdm.pandas()\r\n",
    "        self.data['sentence'] = self.data['sentence'].progress_apply(self.clean_kanji)\r\n",
    "        self.data['romaji'] = self.data['sentence'].progress_apply(self.katsu.romaji)\r\n",
    "        self.data['romaji'] = self.data['romaji'].progress_apply(self.clean_romaji)\r\n",
    "        self.data['romaji'] = self.data['romaji'].str.lower()\r\n",
    "        self.data['length'] = self.data['path'].progress_apply(self.get_length)\r\n",
    "        self.data.query(\"(length >= 48000) & (length <= 80000)\", inplace=True)\r\n",
    "        self.data = self.data[self.data['sentence'].apply(list).apply(len)>=5]\r\n",
    "        self.data = self.data.dropna()\r\n",
    "        self.data = self.data.sample(n=10000, random_state=42, ignore_index=True)\r\n",
    "        self.data.sort_values(by=\"length\", axis=0, ascending=False, inplace=True, ignore_index=True)\r\n",
    "        self.data.to_csv(f\"{self.main_dir}/ASRDataset.csv\", encoding=\"utf-8\", index=False)\r\n",
    "\r\n",
    "    def get_kokoro(self):\r\n",
    "        in_dir = \"Datasets\\KOKORO-dataset\"\r\n",
    "\r\n",
    "        data = []\r\n",
    "        transcript_path = f\"{in_dir}/transcripts/*.metadata.txt\"\r\n",
    "        for transcript in glob.glob(transcript_path):\r\n",
    "            with open(transcript, \"r\", encoding=\"utf-8\") as f:\r\n",
    "                for line in f.readlines():\r\n",
    "                    data.append(line.split(\"|\"))\r\n",
    "\r\n",
    "        data = pd.DataFrame(\r\n",
    "            data, columns=[\r\n",
    "                'text_id', 'path', 'start_idx', \r\n",
    "                'end_idx', 'sentence', 'phonemes'])       \r\n",
    "\r\n",
    "        # paths = data['path'].unique()\r\n",
    "        # for path in tqdm(paths, total=len(paths)):\r\n",
    "        #     folder_name = path.split(\"_\", 1)[0]\r\n",
    "        #     in_path = os.path.join(in_dir, folder_name, path)\r\n",
    "        #     y, sr = librosa.load(in_path, sr=None)\r\n",
    "        #     for text_id in data.loc[data['path']==path, 'text_id']:\r\n",
    "        #         out_path = os.path.join(self.main_dir, 'wav_cleaned', text_id) + \".wav\"\r\n",
    "        #         if not os.path.exists(out_path):\r\n",
    "        #             start_idx = int(data.loc[data['text_id']==text_id, 'start_idx'].item())\r\n",
    "        #             end_idx = int(data.loc[data['text_id']==text_id, 'end_idx'].item())\r\n",
    "        #             y_slice = librosa.resample(\r\n",
    "        #                 y[start_idx:end_idx], orig_sr=sr, target_sr=self.sample_rate)\r\n",
    "        #             sf.write(out_path, y_slice, samplerate=self.sample_rate, subtype='PCM_16')\r\n",
    "\r\n",
    "        data = data[['text_id', 'sentence']]\r\n",
    "        data['text_id'] = data['text_id'].apply(lambda x: x + \".wav\")\r\n",
    "        data.columns = ['path', 'sentence']\r\n",
    "        data['corpus'] = ['kokoro'] * len(data)\r\n",
    "        return data\r\n",
    "\r\n",
    "    def get_jsut(self):\r\n",
    "        filenames, sentences = [], []\r\n",
    "        for transcript in glob.glob(r\"Datasets/JSUT-dataset/*/transcript_utf8.txt\"):\r\n",
    "            file_path = transcript.rsplit(\"\\\\\", 1)[0]\r\n",
    "            with open(transcript, \"r\", encoding=\"utf-8\") as f:\r\n",
    "                lines = f.readlines()\r\n",
    "                for line in lines: \r\n",
    "                    filename, sentence = line.split(\":\")\r\n",
    "                    filenames.append(os.path.join(file_path, \"wav\", filename) + \".wav\")\r\n",
    "                    sentences.append(sentence.strip(\"\\n\"))\r\n",
    "        data = pd.DataFrame({'path': filenames, 'sentence': sentences}) \r\n",
    "        data['corpus'] = ['jsut'] * len(data)\r\n",
    "        for i, in_path in tqdm(enumerate(data['path']), total=len(data['path'])):\r\n",
    "            in_path = in_path.replace(\"\\\\\", \"/\")\r\n",
    "            out_path = f\"{self.main_dir}\\wav_cleaned\"\r\n",
    "            filename = in_path.rsplit(\"/\", 1)[-1]\r\n",
    "            out_path = os.path.join(out_path, filename)\r\n",
    "            if not os.path.exists(out_path):\r\n",
    "                subprocess.call([\r\n",
    "                    \"ffmpeg\", \"-i\", in_path,\"-acodec\", \"pcm_s16le\", \r\n",
    "                    \"-ar\", str(self.sample_rate), out_path])\r\n",
    "            data['path'][i] = filename\r\n",
    "        return data\r\n",
    "\r\n",
    "    def get_commonvoice(self):\r\n",
    "        data = pd.read_csv(r\"Datasets/CommonVoice-dataset/validated.tsv\", sep=\"\\t\")\r\n",
    "        data = data[['path', 'sentence']]    \r\n",
    "        data['path'] = data['path'].apply(\r\n",
    "            lambda x: r\"Datasets/CommonVoice-dataset/mp3/\" + x)\r\n",
    "        data['corpus'] = ['common_voice'] * len(data)\r\n",
    "        for i, in_path in tqdm(enumerate(data['path']), total=len(data['path'])):\r\n",
    "            in_path = in_path.replace(\"\\\\\", \"/\")\r\n",
    "            out_path = f\"{self.main_dir}\\wav_cleaned\"\r\n",
    "            filename = in_path.rsplit(\"/\", 1)[-1]\r\n",
    "            filename = filename.replace(\"mp3\", \"wav\")\r\n",
    "            out_path = os.path.join(out_path, filename)\r\n",
    "            if not os.path.exists(out_path):\r\n",
    "                subprocess.call([\r\n",
    "                    \"ffmpeg\", \"-i\", in_path,\"-acodec\", \"pcm_s16le\", \r\n",
    "                    \"-ar\", str(self.sample_rate), out_path])\r\n",
    "            data['path'][i] = filename\r\n",
    "        return data\r\n",
    "\r\n",
    "    def clean_kanji(self, sentence):\r\n",
    "        symbols = r\"[（.*?）！-～.,;..._。、-〿・■（）：ㇰ-ㇿ㈠-㉃㊀-㋾㌀-㍿「」『』→ー -~‘–※π—ゐ’“”]\"\r\n",
    "        sentence = re.sub(symbols, \"\", sentence)\r\n",
    "        sentence = self.wakati.parse(sentence).strip(\"\\n\")          \r\n",
    "        return sentence\r\n",
    "\r\n",
    "    def clean_romaji(self, sentence):\r\n",
    "        return re.sub(r'[.,\"\\'\\/?]', \"\", sentence)\r\n",
    "\r\n",
    "    def get_length(self, path):\r\n",
    "        path = os.path.join(self.main_dir, 'wav_cleaned', path)\r\n",
    "        y, sr = librosa.load(path, sr=None)\r\n",
    "        return len(y)\r\n",
    "\r\n",
    "data = Dataset().data\r\n",
    "data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# fig, ax = plt.subplots(1,1,figsize=(10, 4))\r\n",
    "# sns.histplot(x=data['length'], hue=data['corpus'], ax=ax, palette=\"bright\")\r\n",
    "# plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Arguments"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def ArgParser():\r\n",
    "    parser = argparse.ArgumentParser()\r\n",
    "\r\n",
    "    # DataLoader\r\n",
    "    parser.add_argument(\"--main_dir\", default=\"E://Datasets/ASR-dataset\")\r\n",
    "    parser.add_argument(\"--sample_rate\", default=16000)\r\n",
    "    parser.add_argument(\"--test_size\", default=0.1)\r\n",
    "    parser.add_argument(\"--random_state\", default=42)\r\n",
    "    parser.add_argument(\"--batch_size\", default=4)\r\n",
    "    parser.add_argument(\"--n_shards\", default=20)\r\n",
    "    parser.add_argument(\"--buffer_size\", default=512)\r\n",
    "\r\n",
    "    # Trainer\r\n",
    "    parser.add_argument(\"--epochs\", default=20)\r\n",
    "    parser.add_argument(\"--learning_rate\", default=1e-4)\r\n",
    "    parser.add_argument(\"--lr_start\", default=1e-7)\r\n",
    "    parser.add_argument(\"--lr_min\", default=1e-7)\r\n",
    "    parser.add_argument(\"--lr_max\", default=1e-4)\r\n",
    "    parser.add_argument(\"--n_cycles\", default=0.5)\r\n",
    "    parser.add_argument(\"--warmup_epochs\", default=4)\r\n",
    "    parser.add_argument(\"--sustain_epochs\", default=0)\r\n",
    "\r\n",
    "    args = parser.parse_known_args()[0]\r\n",
    "\r\n",
    "    with open(f\"{args.main_dir}/vocab.json\", \"r\") as f:\r\n",
    "        vocab_size = len(json.load(f))\r\n",
    "    \r\n",
    "    n_samples = len(pd.read_csv(os.path.join(args.main_dir, \"ASRDataset.csv\")))\r\n",
    "    n_train = int(n_samples * (1 - args.test_size))\r\n",
    "    n_val = int(n_samples * args.test_size)\r\n",
    "    train_steps = int(np.ceil(n_train / args.batch_size))\r\n",
    "\r\n",
    "    parser.add_argument(\"--vocab_size\", default=vocab_size)\r\n",
    "    parser.add_argument(\"--n_samples\", default=n_samples)\r\n",
    "    parser.add_argument(\"--n_train\", default=n_train)\r\n",
    "    parser.add_argument(\"--n_val\", default=n_val)\r\n",
    "    parser.add_argument(\"--train_steps\", default=train_steps)  \r\n",
    "\r\n",
    "    return parser.parse_known_args()[0]\r\n",
    "\r\n",
    "args = ArgParser()\r\n",
    "args"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Loading"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class TFRWriter():\r\n",
    "    def __init__(self, args):\r\n",
    "        self.data = pd.read_csv(os.path.join(args.main_dir, \"ASRDataset.csv\"))\r\n",
    "        self.args = args\r\n",
    "        self.tokenizer = tokenizer = Wav2Vec2CTCTokenizer(\r\n",
    "            f\"{args.main_dir}/vocab.json\",\r\n",
    "            word_delimiter_token=' ',\r\n",
    "            do_lower_case=False)\r\n",
    "\r\n",
    "    def _bytes_feature(self, value):\r\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\r\n",
    "        if isinstance(value, type(tf.constant(0))):\r\n",
    "            value = value.numpy()\r\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\r\n",
    "\r\n",
    "    def _int64_feature(self, value):\r\n",
    "        \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\r\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\r\n",
    "\r\n",
    "    def _float_feature(self, value):\r\n",
    "        \"\"\"Returns a float_list from a float / double.\"\"\"\r\n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\r\n",
    "\r\n",
    "    def serialize_example(self, *args):\r\n",
    "        feature = {\r\n",
    "            'input_values': self._bytes_feature(args[0]),\r\n",
    "            'labels': self._bytes_feature(args[1]),\r\n",
    "            }\r\n",
    "\r\n",
    "        example_proto = tf.train.Example(\r\n",
    "            features=tf.train.Features(feature=feature))\r\n",
    "        return example_proto.SerializeToString()\r\n",
    "\r\n",
    "    def get_labels(self, sample):\r\n",
    "        labels = self.data.loc[self.data['path']==sample, \"romaji\"].item()\r\n",
    "        labels = (self.tokenizer.bos_token + labels + \r\n",
    "            self.tokenizer.eos_token)\r\n",
    "        labels = self.tokenizer(labels)['input_ids']\r\n",
    "        return tf.convert_to_tensor(labels, dtype=tf.int32)\r\n",
    "\r\n",
    "    def get_audio(self, sample):\r\n",
    "        path = os.path.join(self.args.main_dir, \"wav_cleaned\", sample)\r\n",
    "        audio = librosa.load(path, sr=None)[0]\r\n",
    "        return tf.convert_to_tensor(audio, dtype=tf.float32)\r\n",
    "\r\n",
    "    def get_shards(self):\r\n",
    "        skf = KFold(n_splits=self.args.n_shards, shuffle=False)\r\n",
    "        return [\r\n",
    "            list(map(lambda x: self.data['path'][x], j))\r\n",
    "            for i, j in skf.split(self.data['path'])]\r\n",
    "\r\n",
    "    def get_shard_data(self, samples):\r\n",
    "        for sample in samples:\r\n",
    "            audio = self.get_audio(sample)\r\n",
    "            labels = self.get_labels(sample)\r\n",
    "            yield {\r\n",
    "                'input_values': tf.io.serialize_tensor(audio),\r\n",
    "                'labels': tf.io.serialize_tensor(labels),\r\n",
    "            }\r\n",
    "\r\n",
    "    def write(self):\r\n",
    "        for shard, samples in tqdm(enumerate(self.get_shards()), total=self.args.n_shards):\r\n",
    "            with tf.io.TFRecordWriter(f\"{self.args.main_dir}/wav2vec2_tfrec/shard_{shard+1}.tfrec\") as f:\r\n",
    "                for sample in self.get_shard_data(samples):\r\n",
    "                    example = self.serialize_example(\r\n",
    "                        sample['input_values'], \r\n",
    "                        sample['labels'], \r\n",
    "                        )\r\n",
    "                    f.write(example)\r\n",
    "\r\n",
    "# TFRWriter(args).write()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class DataLoader:\r\n",
    "    def __init__(self, args):\r\n",
    "        self.files = glob.glob(args.main_dir + \"/wav2vec2_tfrec/*.tfrec\")\r\n",
    "        self.args = args\r\n",
    "        self.AUTOTUNE = tf.data.AUTOTUNE\r\n",
    "        self.train_files, self.val_files = train_test_split(\r\n",
    "            self.files, test_size=args.test_size, shuffle=True, \r\n",
    "            random_state=args.random_state)\r\n",
    "        self.train = self.get_train()\r\n",
    "        self.val = self.get_val()     \r\n",
    "\r\n",
    "    def read_tfrecord(self, example):\r\n",
    "        feature_description = {\r\n",
    "            'input_values': tf.io.FixedLenFeature([], tf.string),\r\n",
    "            'labels': tf.io.FixedLenFeature([], tf.string),\r\n",
    "            }\r\n",
    "        \r\n",
    "        example = tf.io.parse_single_example(example, feature_description)\r\n",
    "        example['input_values'] = tf.io.parse_tensor(\r\n",
    "            example['input_values'], out_type=tf.float32)\r\n",
    "        example['labels'] = tf.io.parse_tensor(\r\n",
    "            example['labels'], out_type=tf.int32)\r\n",
    "        return example\r\n",
    "\r\n",
    "    def load_dataset(self, files):\r\n",
    "        ignore_order = tf.data.Options()\r\n",
    "        ignore_order.experimental_deterministic = False\r\n",
    "        dataset = tf.data.TFRecordDataset(files)\r\n",
    "        dataset = dataset.with_options(ignore_order)\r\n",
    "        dataset = dataset.map(self.read_tfrecord, num_parallel_calls=self.AUTOTUNE)\r\n",
    "        return dataset\r\n",
    "\r\n",
    "    def get_train(self):\r\n",
    "        dataset = self.load_dataset(self.train_files)\r\n",
    "        dataset = dataset.padded_batch(\r\n",
    "            self.args.batch_size,\r\n",
    "            padded_shapes={\r\n",
    "                'input_values': [None],\r\n",
    "                'labels': [None],\r\n",
    "                },\r\n",
    "            padding_values={\r\n",
    "                'input_values': tf.constant(0, dtype=tf.float32), \r\n",
    "                'labels': tf.constant(-1, dtype=tf.int32),\r\n",
    "                })\r\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\r\n",
    "        return dataset\r\n",
    "\r\n",
    "    def get_val(self):\r\n",
    "        dataset = self.load_dataset(self.val_files)\r\n",
    "        dataset = dataset.padded_batch(\r\n",
    "            self.args.batch_size,\r\n",
    "            padded_shapes={\r\n",
    "                'input_values': [None],\r\n",
    "                'labels': [None],\r\n",
    "                },\r\n",
    "            padding_values={\r\n",
    "                'input_values': tf.constant(0, dtype=tf.float32), \r\n",
    "                'labels': tf.constant(-1, dtype=tf.int32),\r\n",
    "                })\r\n",
    "        dataset = dataset.cache()\r\n",
    "        dataset = dataset.prefetch(self.AUTOTUNE)\r\n",
    "        return dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor(\r\n",
    "    feature_size=1,\r\n",
    "    sampling_rate=16000,\r\n",
    "    padding_value=0.0,\r\n",
    "    do_normalize=True,\r\n",
    "    return_attention_mask=False\r\n",
    ")\r\n",
    "\r\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\r\n",
    "    r\"E:\\Datasets\\ASR-dataset\\vocab.json\",\r\n",
    "    word_delimiter_token=\" \",\r\n",
    "    do_lower_case=False\r\n",
    ")\r\n",
    "\r\n",
    "processor = Wav2Vec2Processor(\r\n",
    "    feature_extractor=feature_extractor, tokenizer=tokenizer\r\n",
    ")\r\n",
    "\r\n",
    "model = TFWav2Vec2ForCTC.from_pretrained(\r\n",
    "    \"facebook/wav2vec2-base\",\r\n",
    "    ctc_loss_reduction=\"mean\",\r\n",
    "    from_pt=True,\r\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\r\n",
    "    vocab_size=len(processor.tokenizer),\r\n",
    ")\r\n",
    "\r\n",
    "model.freeze_feature_extractor()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class PER(tf.keras.metrics.Metric):\r\n",
    "    \"\"\"Phoneme Error Rate\r\n",
    "\r\n",
    "    This metric calculates the normalized error rate based on phonemes.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        beam_width: (Optional)\r\n",
    "        top_paths: (Optional)\r\n",
    "        name: (Optional) string name of the metric instance\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, beam_width=10, top_paths=1, name=\"PER\", **kwargs):\r\n",
    "        super(PER, self).__init__(name=name,  **kwargs)\r\n",
    "        self.beam_width = beam_width\r\n",
    "        self.top_paths = top_paths\r\n",
    "        self.per_accumulator = self.add_weight(name=\"total_per\", initializer=\"zeros\")\r\n",
    "        # self.counter = self.add_weight(name=\"per_count\", initializer=\"zeros\")\r\n",
    "\r\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\r\n",
    "        \"\"\"\r\n",
    "        Function takes in model output logits and target labels and updates\r\n",
    "        accumulator globally.\r\n",
    "\r\n",
    "        Args: \r\n",
    "            y_true shape: [batch_size, sequence_length]\r\n",
    "            y_pred shape: [batch_size, sequence_length, num_features]\r\n",
    "\r\n",
    "        Returns:\r\n",
    "            None\r\n",
    "\r\n",
    "        \"\"\"\r\n",
    "        batch_size, sequence_length, num_features = tf.shape(y_pred)\r\n",
    "        y_pred = tf.reshape(y_pred, [sequence_length, batch_size, num_features])\r\n",
    "        sequence_length = tf.repeat(sequence_length, batch_size)\r\n",
    "\r\n",
    "        # Decode logits into sparse tensor using beam search decoder\r\n",
    "        hypothesis = tf.nn.ctc_beam_search_decoder(\r\n",
    "            y_pred, sequence_length=sequence_length, beam_width=self.beam_width,\r\n",
    "            top_paths=self.top_paths)[0][0]\r\n",
    "        hypothesis = tf.cast(hypothesis, dtype=tf.int32)\r\n",
    "        # Convert dense to sparse tensor for edit_distance function\r\n",
    "        truth = tf.sparse.from_dense(y_true)\r\n",
    "        # Calculate Levenshtein distance\r\n",
    "        distance = tf.edit_distance(hypothesis, truth, normalize=True)\r\n",
    "        self.per_accumulator.assign_add(tf.reduce_sum(distance))\r\n",
    "        # self.counter.assign_add(len(y_true))\r\n",
    "\r\n",
    "    def result(self):\r\n",
    "        # return tf.math.divide_no_nan(self.per_accumulator, self.counter)\r\n",
    "        return self.per_accumulator\r\n",
    "    \r\n",
    "    def reset_states(self):\r\n",
    "        self.per_accumulator.assign(0.0)\r\n",
    "        # self.counter.assign(0.0)\r\n",
    "\r\n",
    "class CosineDecaySchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\r\n",
    "    def __init__(self, args):\r\n",
    "        self.args = args\r\n",
    "\r\n",
    "    def __call__(self, epoch):  \r\n",
    "        if epoch < self.args.warmup_epochs:\r\n",
    "            lr = ((self.args.lr_max - self.args.lr_start) / self.args.warmup_epochs) * epoch + self.args.lr_start\r\n",
    "        elif epoch < (self.args.warmup_epochs + self.args.sustain_epochs):\r\n",
    "            lr = self.args.lr_max\r\n",
    "        else:\r\n",
    "            progress = ((epoch - self.args.warmup_epochs - self.args.sustain_epochs) / \r\n",
    "            (self.args.epochs - self.args.warmup_epochs - self.args.sustain_epochs))\r\n",
    "            lr = (self.args.lr_max-self.args.lr_min) * (0.5 * (1.0 + tf.math.cos((22/7) * \r\n",
    "                self.args.n_cycles * 2.0 * progress)))\r\n",
    "            if self.args.lr_min is not None:\r\n",
    "                lr = tf.math.maximum(self.args.lr_min, lr)\r\n",
    "        return lr"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_dataset = DataLoader(args).train\r\n",
    "val_dataset = DataLoader(args).val\r\n",
    "\r\n",
    "optimizer = tf.keras.optimizers.Adam(CosineDecaySchedule(args))\r\n",
    "loss_metric = tf.keras.metrics.Mean(name=\"loss\")\r\n",
    "per_metric = PER()\r\n",
    "stateful_metrics = ['loss', 'val_loss']\r\n",
    "\r\n",
    "for epoch in range(args.epochs):\r\n",
    "    progbar = tf.keras.utils.Progbar(\r\n",
    "                args.train_steps, interval=0.05,\r\n",
    "                stateful_metrics=stateful_metrics)\r\n",
    "    print(f\"Epoch {epoch+1}/{args.epochs}\")\r\n",
    "\r\n",
    "    # Training loop\r\n",
    "    for step, batch in enumerate(train_dataset):\r\n",
    "        X_train = batch['input_values']\r\n",
    "        y_train = batch['labels']\r\n",
    "        with tf.GradientTape() as tape:\r\n",
    "            t_loss, t_logits = model(\r\n",
    "                input_values=X_train, \r\n",
    "                labels=y_train, training=True)[:2]\r\n",
    "\r\n",
    "        gradients = tape.gradient(t_loss, model.trainable_variables)\r\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n",
    "\r\n",
    "        loss_metric.update_state(t_loss)\r\n",
    "        per_metric.update_state(y_train, t_logits)\r\n",
    "        t_per = per_metric.result()\r\n",
    "\r\n",
    "        values = [('loss', t_loss), ('per', t_per)]\r\n",
    "        progbar.update(step, values=values, finalize=False)\r\n",
    "        per_metric.reset_states()\r\n",
    "   \r\n",
    "    # Validation loop\r\n",
    "    for batch in val_dataset:\r\n",
    "        X_val = batch['input_values']\r\n",
    "        y_val = batch['labels']\r\n",
    "        v_loss, v_logits = model(\r\n",
    "            input_values=X_val, \r\n",
    "            labels=y_val, training=False)[:2]\r\n",
    "        per_metric.update_state(y_val, v_logits)\r\n",
    "        v_per = per_metric.result()\r\n",
    "        per_metric.reset_states()\r\n",
    "\r\n",
    "    values = [\r\n",
    "        ('loss', t_loss), ('per', t_per), \r\n",
    "        ('val_loss', v_loss), ('val_per', v_per)]\r\n",
    "    progbar.update(args.train_steps, values=values)\r\n",
    "\r\n",
    "    print(\"Training\")\r\n",
    "    y_preds = processor.batch_decode(tf.argmax(t_logits, axis=-1))\r\n",
    "    y_trues = processor.batch_decode(y_train)\r\n",
    "    \r\n",
    "    for y_true, y_pred in zip(y_trues, y_preds):\r\n",
    "        print(f\"Target:    {y_true}\")\r\n",
    "        print(f\"Predicted: {y_pred}\\n\")\r\n",
    "\r\n",
    "    print(\"Validation\")\r\n",
    "    y_preds = processor.batch_decode(tf.argmax(v_logits, axis=-1))\r\n",
    "    y_trues = processor.batch_decode(y_val)\r\n",
    "    \r\n",
    "    for y_true, y_pred in zip(y_trues, y_preds):\r\n",
    "        print(f\"Target:    {y_true}\")\r\n",
    "        print(f\"Predicted: {y_pred}\\n\")\r\n",
    "\r\n",
    "    model.save_weights(f\"E:\\Datasets\\ASR-dataset\\checkpoints\\model_{epoch+1:02d}.h5\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('tf-gpu': conda)"
  },
  "interpreter": {
   "hash": "b851d2923cfa3a2562599062e05fd9893d86a7c009c64d8ad3756552e4dd5f41"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}