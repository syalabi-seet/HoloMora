{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "import re\r\n",
    "import glob\r\n",
    "import pandas as pd\r\n",
    "import librosa\r\n",
    "import soundfile as sf\r\n",
    "from tqdm import tqdm\r\n",
    "import subprocess\r\n",
    "from collections import Counter\r\n",
    "import MeCab\r\n",
    "# import pyopenjtalk\r\n",
    "\r\n",
    "import torch\r\n",
    "\r\n",
    "from transformers import (\r\n",
    "    Wav2Vec2ForCTC, \r\n",
    "    Wav2Vec2CTCTokenizer, \r\n",
    "    Wav2Vec2FeatureExtractor,\r\n",
    "    Wav2Vec2Processor)\r\n",
    "\r\n",
    "hira_list = [\r\n",
    "    # Gojuon letters\r\n",
    "    'あ','い','う','え','お' # a-row\r\n",
    "    'か','き','く','け','こ', # ka-row\r\n",
    "    'さ','し','す','せ','そ',# sa-row\r\n",
    "    'た','ち','つ','て','と', # ta-row\r\n",
    "    'な','に','ぬ','ね','の',# na-row\r\n",
    "    'は','ふ','へ','ほ','ひ', # ha-row\r\n",
    "    'ま','め','も','み','む',# ma-row\r\n",
    "    'や','よ','ゆ', # ya-row\r\n",
    "    'ら','り','る','れ','ろ', # ra-row\r\n",
    "    'わ','を', # wa-row\r\n",
    "    'ん', # n-row\r\n",
    "\r\n",
    "    # Dakuon letters\r\n",
    "    'が','ぎ','ぐ','げ','ご', # ga-row\r\n",
    "    'ざ','ぜ','ず','じ','ぞ', # za-row\r\n",
    "    'だ','ど','づ','で','ぢ', # da-row\r\n",
    "    'ぼ','び','ぶ','べ','ば', # ba-row\r\n",
    "\r\n",
    "    # Han-Dakuon letters\r\n",
    "    'ぱ','ぴ','ぷ','ぺ','ぽ',\r\n",
    "\r\n",
    "    # Yoon letters\r\n",
    "    'ょ','ゅ','ゃ',\r\n",
    "    'ぇ','ぁ','ぉ','ぃ','っ'] # half-width"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class ASRDataset:\r\n",
    "    def __init__(self):\r\n",
    "        self.main_dir = \"Datasets/ASR-dataset\"\r\n",
    "        self.data = pd.concat([\r\n",
    "            self.get_kokoro(),\r\n",
    "            self.get_meian(),\r\n",
    "            self.get_jsut(),\r\n",
    "            self.get_commonvoice()\r\n",
    "        ], ignore_index=True)\r\n",
    "        self.data = self.resample(self.data).dropna()\r\n",
    "        self.data['sentence'] = self.data['sentence'].apply(self.CleanKanji)\r\n",
    "        self.data = self.data.dropna().reset_index(drop=True)\r\n",
    "        # self.data['phonemes'] = self.data['sentence'].apply(pyopenjtalk.g2p) \r\n",
    "        self.data.to_csv(f\"{self.main_dir}/ASRDataset.csv\", encoding=\"utf-8\", index=False)\r\n",
    "\r\n",
    "    def get_kokoro(self):\r\n",
    "        data = pd.read_csv(\r\n",
    "            \"Datasets\\KOKORO-dataset\\metadata.csv\", \r\n",
    "            sep=\"|\", encoding=\"utf-8\", header=None)\r\n",
    "        data.columns = [\"path\", \"sentence\", \"transliteration\"]\r\n",
    "        data = data[[\"path\", \"sentence\"]]\r\n",
    "        data['path'] = data['path'].apply(\r\n",
    "            lambda x: r\"Datasets\\KOKORO-dataset\\wav/\" + x + \".wav\")\r\n",
    "        return data\r\n",
    "\r\n",
    "    def get_meian(self):\r\n",
    "        data = pd.read_csv(r\"Datasets\\MEIAN-dataset\\transcript.txt\", sep=\"|\", header=None)\r\n",
    "        data.columns = [\"path\", \"sentence\", \"tranliteration\", \"duration\"]\r\n",
    "        data = data[[\"path\", \"sentence\"]]\r\n",
    "        data['path'] = data['path'].apply(\r\n",
    "            lambda x: r\"Datasets\\MEIAN-dataset\\wav/\" + x.split(\"/\")[-1])\r\n",
    "        return data\r\n",
    "\r\n",
    "    def get_jsut(self):\r\n",
    "        filenames, sentences = [], []\r\n",
    "        for transcript in glob.glob(r\"Datasets\\JSUT-dataset\\*\\transcript_utf8.txt\"):\r\n",
    "            file_path = transcript.rsplit(\"\\\\\", 1)[0]\r\n",
    "            with open(transcript, \"r\", encoding=\"utf-8\") as f:\r\n",
    "                lines = f.readlines()\r\n",
    "                for line in lines: \r\n",
    "                    filename, sentence = line.split(\":\")\r\n",
    "                    filenames.append(os.path.join(file_path, \"wav\", filename) + \".wav\")\r\n",
    "                    sentences.append(sentence.strip(\"\\n\"))\r\n",
    "        data = pd.DataFrame({'path': filenames, 'sentence': sentences}) \r\n",
    "        return data \r\n",
    "\r\n",
    "    def get_commonvoice(self):\r\n",
    "        data = pd.read_csv(r\"Datasets\\CommonVoice-dataset\\validated.tsv\", sep=\"\\t\")\r\n",
    "        data = data[['path', 'sentence']]    \r\n",
    "        data['path'] = data['path'].apply(\r\n",
    "            lambda x: r\"Datasets\\CommonVoice-dataset\\mp3/\" + x)\r\n",
    "        return data\r\n",
    "\r\n",
    "    def resample(self, data):\r\n",
    "        for i, in_path in tqdm(enumerate(data['path']), total=len(data['path'])):\r\n",
    "            in_path = in_path.replace(\"\\\\\", \"/\")\r\n",
    "            out_path = f\"{self.main_dir}/wav_cleaned\"\r\n",
    "            filename = in_path.rsplit(\"/\", 1)[-1]\r\n",
    "            if in_path.endswith(\"mp3\"):\r\n",
    "                filename = filename.replace(\"mp3\", \"wav\")\r\n",
    "                out_path = os.path.join(out_path, filename)\r\n",
    "                if not os.path.exists(out_path):\r\n",
    "                    subprocess.call([\r\n",
    "                        \"ffmpeg\", \"-i\", in_path,\"-acodec\", \"pcm_s16le\", \r\n",
    "                        \"-ar\", \"16000\", out_path])\r\n",
    "            else:\r\n",
    "                sample_rate = librosa.get_samplerate(in_path)\r\n",
    "                out_path = os.path.join(out_path, filename)\r\n",
    "                if not os.path.exists(out_path):\r\n",
    "                    if sample_rate != 16000:\r\n",
    "                        subprocess.call([\r\n",
    "                            \"ffmpeg\", \"-i\", in_path, \"-ar\", \"16000\", out_path])\r\n",
    "            data['path'][i] = filename  \r\n",
    "        return data       \r\n",
    "\r\n",
    "    def CleanKanji(self, sentence):\r\n",
    "        wakati = MeCab.Tagger(\"-Owakati\")\r\n",
    "        symbols = r\"[（.*?）！-～.,;..._。、-〿・■（）：ㇰ-ㇿ㈠-㉃㊀-㋾㌀-㍿「」『』→ー -~‘–※π—ゐ’“”]\"\r\n",
    "        sentence = re.sub(symbols, \"\", sentence)\r\n",
    "        sentence = wakati.parse(sentence).strip(\"\\n\")              \r\n",
    "        return sentence\r\n",
    "\r\n",
    "# data = ASRDataset().data\r\n",
    "# data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "source": [
    "def clean_phonemes(phonemes):   \r\n",
    "    for i, phoneme in enumerate(phonemes):\r\n",
    "        if i < len(phonemes) - 1:\r\n",
    "            if phonemes[i] == \"cl\":\r\n",
    "                if phonemes[i+1] == \"pau\":\r\n",
    "                    phonemes = phonemes[:i] + [phonemes[i+2]] + phonemes[i+2:]\r\n",
    "                else:\r\n",
    "                    phonemes[i] = phonemes[i+1]\r\n",
    "            if phonemes[i] == \"pau\":\r\n",
    "                if phonemes[i+1] == \"N\":\r\n",
    "                    phonemes = phonemes[:i] + phonemes[i+1:]\r\n",
    "    for i, phoneme in enumerate(phonemes):\r\n",
    "        phonemes[i] = phonemes[i].replace(\"U\", \"u\")\r\n",
    "        phonemes[i] = phonemes[i].replace(\"I\", \"i\")\r\n",
    "        phonemes[i] = phonemes[i].replace(\"pau\", \" \")\r\n",
    "    return phonemes\r\n",
    "\r\n",
    "data = pd.read_csv(\"Datasets\\ASR-dataset\\ASRDataset.csv\", encoding=\"utf-8\").dropna()\r\n",
    "data = data[~data['phonemes'].str.contains(\"ty\")]\r\n",
    "data['phonemes'] = data['phonemes'].str.split()\r\n",
    "data['phonemes'] = data['phonemes'].apply(clean_phonemes)\r\n",
    "data['hiragana'] = data['phonemes'].apply(\"\".join)\r\n",
    "data"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>sentence</th>\n",
       "      <th>phonemes</th>\n",
       "      <th>hiragana</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kusamakura-by-soseki-natsume-00001.wav</td>\n",
       "      <td>草枕 夏目 漱石</td>\n",
       "      <td>[k, u, s, a, m, a, k, u, r, a,  , n, a, ts, u,...</td>\n",
       "      <td>kusamakura natsume sooseki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kusamakura-by-soseki-natsume-00002.wav</td>\n",
       "      <td>やま みち を 登り ながら こう 考え たち に 働け ば かど が 立つ</td>\n",
       "      <td>[y, a, m, a,  , m, i, ch, i,  , o,  , n, o, b,...</td>\n",
       "      <td>yama michi o nobori nagara kou kaNgae tachi ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kusamakura-by-soseki-natsume-00003.wav</td>\n",
       "      <td>じょう に さおさせ ば 流さ れる 意地 を とおせ ば きゅうくつ だ</td>\n",
       "      <td>[j, o, o,  , n, i,  , s, a, o, s, a, s, e,  , ...</td>\n",
       "      <td>joo ni saosase ba ryuusa reru iji o toose ba k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kusamakura-by-soseki-natsume-00004.wav</td>\n",
       "      <td>とかく に 人 の 世 は 住み にくい 住み にく さ が こうじる と</td>\n",
       "      <td>[t, o, k, a, k, u,  , n, i,  , h, i, t, o,  , ...</td>\n",
       "      <td>tokaku ni hito no yo wa sumi nikui sumi niku s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kusamakura-by-soseki-natsume-00005.wav</td>\n",
       "      <td>安い 所 へ 引き越し たく なる どこ へ 越し て も</td>\n",
       "      <td>[y, a, s, u, i,  , t, o, k, o, r, o,  , e,  , ...</td>\n",
       "      <td>yasui tokoro e hikikoshi taku naru doko e kosh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44367</th>\n",
       "      <td>common_voice_ja_27369273.wav</td>\n",
       "      <td>月並み な アイデア から ヒント を 見つけ だす</td>\n",
       "      <td>[ts, u, k, i, n, a, m, i,  , n, a,  , a, i, d,...</td>\n",
       "      <td>tsukinami na aidea kara hiNto o mitsuke dasu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44368</th>\n",
       "      <td>common_voice_ja_27388440.wav</td>\n",
       "      <td>直観 と は ただ 我 の 自己 が 世界 の 形成 作用 と し て 世界 の 中 に 含...</td>\n",
       "      <td>[ch, o, k, k, a, N,  , t, o,  , w, a,  , t, a,...</td>\n",
       "      <td>chokkaN to wa tada waga no jiko ga sekai no ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44369</th>\n",
       "      <td>common_voice_ja_27388441.wav</td>\n",
       "      <td>斯く あっ た から 斯く す べし と し て</td>\n",
       "      <td>[k, a, k, u,  , a, t, t, a,  , k, a, r, a,  , ...</td>\n",
       "      <td>kaku atta kara kaku su beshi to shi te</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44370</th>\n",
       "      <td>common_voice_ja_27388443.wav</td>\n",
       "      <td>いつ も 自己 自身 の 中 に 自己 を 越え た もの 超越 的 なる もの を 含む ...</td>\n",
       "      <td>[i, ts, u,  , m, o,  , j, i, k, o,  , j, i, sh...</td>\n",
       "      <td>itsu mo jiko jishiN no naka ni jiko o koe ta m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44371</th>\n",
       "      <td>common_voice_ja_27388457.wav</td>\n",
       "      <td>今回 の バジョンアップ は いい 感じ</td>\n",
       "      <td>[k, o, N, k, a, i,  , n, o,  , b, a, j, o, N, ...</td>\n",
       "      <td>koNkai no bajoNappu wa ii kaNji</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44369 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         path  \\\n",
       "0      kusamakura-by-soseki-natsume-00001.wav   \n",
       "1      kusamakura-by-soseki-natsume-00002.wav   \n",
       "2      kusamakura-by-soseki-natsume-00003.wav   \n",
       "3      kusamakura-by-soseki-natsume-00004.wav   \n",
       "4      kusamakura-by-soseki-natsume-00005.wav   \n",
       "...                                       ...   \n",
       "44367            common_voice_ja_27369273.wav   \n",
       "44368            common_voice_ja_27388440.wav   \n",
       "44369            common_voice_ja_27388441.wav   \n",
       "44370            common_voice_ja_27388443.wav   \n",
       "44371            common_voice_ja_27388457.wav   \n",
       "\n",
       "                                                sentence  \\\n",
       "0                                              草枕 夏目 漱石    \n",
       "1                やま みち を 登り ながら こう 考え たち に 働け ば かど が 立つ    \n",
       "2                 じょう に さおさせ ば 流さ れる 意地 を とおせ ば きゅうくつ だ    \n",
       "3                 とかく に 人 の 世 は 住み にくい 住み にく さ が こうじる と    \n",
       "4                         安い 所 へ 引き越し たく なる どこ へ 越し て も    \n",
       "...                                                  ...   \n",
       "44367                        月並み な アイデア から ヒント を 見つけ だす    \n",
       "44368  直観 と は ただ 我 の 自己 が 世界 の 形成 作用 と し て 世界 の 中 に 含...   \n",
       "44369                          斯く あっ た から 斯く す べし と し て    \n",
       "44370  いつ も 自己 自身 の 中 に 自己 を 越え た もの 超越 的 なる もの を 含む ...   \n",
       "44371                              今回 の バジョンアップ は いい 感じ    \n",
       "\n",
       "                                                phonemes  \\\n",
       "0      [k, u, s, a, m, a, k, u, r, a,  , n, a, ts, u,...   \n",
       "1      [y, a, m, a,  , m, i, ch, i,  , o,  , n, o, b,...   \n",
       "2      [j, o, o,  , n, i,  , s, a, o, s, a, s, e,  , ...   \n",
       "3      [t, o, k, a, k, u,  , n, i,  , h, i, t, o,  , ...   \n",
       "4      [y, a, s, u, i,  , t, o, k, o, r, o,  , e,  , ...   \n",
       "...                                                  ...   \n",
       "44367  [ts, u, k, i, n, a, m, i,  , n, a,  , a, i, d,...   \n",
       "44368  [ch, o, k, k, a, N,  , t, o,  , w, a,  , t, a,...   \n",
       "44369  [k, a, k, u,  , a, t, t, a,  , k, a, r, a,  , ...   \n",
       "44370  [i, ts, u,  , m, o,  , j, i, k, o,  , j, i, sh...   \n",
       "44371  [k, o, N, k, a, i,  , n, o,  , b, a, j, o, N, ...   \n",
       "\n",
       "                                                hiragana  \n",
       "0                             kusamakura natsume sooseki  \n",
       "1      yama michi o nobori nagara kou kaNgae tachi ni...  \n",
       "2      joo ni saosase ba ryuusa reru iji o toose ba k...  \n",
       "3      tokaku ni hito no yo wa sumi nikui sumi niku s...  \n",
       "4      yasui tokoro e hikikoshi taku naru doko e kosh...  \n",
       "...                                                  ...  \n",
       "44367       tsukinami na aidea kara hiNto o mitsuke dasu  \n",
       "44368  chokkaN to wa tada waga no jiko ga sekai no ke...  \n",
       "44369             kaku atta kara kaku su beshi to shi te  \n",
       "44370  itsu mo jiko jishiN no naka ni jiko o koe ta m...  \n",
       "44371                    koNkai no bajoNappu wa ii kaNji  \n",
       "\n",
       "[44369 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 125
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "source": [
    "unique_phonemes = set()\r\n",
    "for sentence in data['phonemes']:\r\n",
    "    for phoneme in sentence:\r\n",
    "        unique_phonemes.add(phoneme)\r\n",
    "\r\n",
    "vowels = ['a', 'e', 'i', 'o', 'u']\r\n",
    "markers = ['pau', 'cl']\r\n",
    "special = ['N']\r\n",
    "consonants = [phoneme for phoneme in unique_phonemes if \r\n",
    "    phoneme not in (vowels + markers + special)]\r\n",
    "\r\n",
    "print(\"Vowels:\\n\", vowels)\r\n",
    "print(\"Consonants:\\n\", consonants)\r\n",
    "print(\"Special:\\n\", special)\r\n",
    "print(\"Markers:\\n\", markers)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vowels:\n",
      " ['a', 'e', 'i', 'o', 'u']\n",
      "Consonants:\n",
      " ['ch', 'm', 'd', 'my', 'hy', 'sh', 'p', 't', 'k', 'r', 'b', 's', 'gy', 'py', 'g', 'n', 'h', 'f', 'ts', 'y', 'v', 'j', 'w', 'ny', 'ky', 'dy', 'by', 'z', ' ', 'ry']\n",
      "Special:\n",
      " ['N']\n",
      "Markers:\n",
      " ['pau', 'cl']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(unique_phonemes)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def phoa(in_path):\r\n",
    "    lines = []\r\n",
    "    with open(in_path, 'r') as f:\r\n",
    "        text = f.read().split(\"\\n\")\r\n",
    "        i = 0\r\n",
    "        while True:\r\n",
    "            start_1, end_1, phoneme_1 = text[i].split(' ')\r\n",
    "            if phoneme_1 in consonants: \r\n",
    "                start_2, end_2, phoneme_2 = text.pop(i+1).split(' ')\r\n",
    "                mora =  (phoneme_1 + phoneme_2).lower()\r\n",
    "                line = \" \".join([str(start_1), str(end_2), mora])\r\n",
    "                lines.append(line)\r\n",
    "            else:\r\n",
    "                if phoneme_1 == 'o':\r\n",
    "                    phoneme_1 = 'wo'\r\n",
    "                line = \" \".join([str(start_1), str(end_1), phoneme_1])\r\n",
    "                lines.append(line)\r\n",
    "            if text[i] == text[-1]:\r\n",
    "                break\r\n",
    "            i += 1\r\n",
    "    lines = \"\\n\".join(lines)\r\n",
    "    return lines\r\n",
    "\r\n",
    "def phoneme2mora(phonemes):\r\n",
    "    for i, phoneme in enumerate(phonemes):\r\n",
    "        if phoneme in consonants:\r\n",
    "            mora = (phoneme)\r\n",
    "\r\n",
    "    return phonemes\r\n",
    "    \r\n",
    "\r\n",
    "\r\n",
    "phoneme2mora(data['phonemes'][0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_mora(in_path):\r\n",
    "    lines = []\r\n",
    "    with open(in_path, 'r') as f:\r\n",
    "        text = f.read().split(\"\\n\")\r\n",
    "        i = 0\r\n",
    "        while True:\r\n",
    "            start_1, end_1, phoneme_1 = text[i].split(' ')\r\n",
    "            if phoneme_1 in consonants: \r\n",
    "                start_2, end_2, phoneme_2 = text.pop(i+1).split(' ')\r\n",
    "                mora =  (phoneme_1 + phoneme_2).lower()\r\n",
    "                lines.append(mora)\r\n",
    "            else:\r\n",
    "                if phoneme_1 == 'o':\r\n",
    "                    phoneme_1 = 'wo'\r\n",
    "                lines.append(phoneme_1)\r\n",
    "            if text[i] == text[-1]:\r\n",
    "                break\r\n",
    "            i += 1\r\n",
    "    return lines"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vocab = []\r\n",
    "for sentence in data['phonemes']:\r\n",
    "    for kana in sentence:\r\n",
    "        vocab.append(kana)\r\n",
    "\r\n",
    "counter = Counter(vocab)\r\n",
    "list(counter.most_common()[::-1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(counter)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor(\r\n",
    "    feature_size=1,\r\n",
    "    sampling_rate=16000,\r\n",
    "    padding_value=0.0,\r\n",
    "    do_normalize=True,\r\n",
    "    return_attention_mask=True\r\n",
    ")\r\n",
    "\r\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\")\r\n",
    "\r\n",
    "processor = Wav2Vec2Processor(\r\n",
    "    feature_extractor=feature_extractor,\r\n",
    "    tokenizer=tokenizer)\r\n",
    "\r\n",
    "processor"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\r\n",
    "    \"facebook/wav2vec2-large-xlsr-53\",\r\n",
    "    attention_dropout=0.1,\r\n",
    "    hidden_dropout=0.1,\r\n",
    "    feat_proj_dropout=0.0,\r\n",
    "    mask_time_prob=0.05,\r\n",
    "    layerdrop=0.1,\r\n",
    "    gradient_checkpointing=True,\r\n",
    "    ctc_loss_reduction=\"mean\",\r\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\r\n",
    "    vocab_size=len(processor.tokenizer)\r\n",
    ")\r\n",
    "\r\n",
    "model"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('tf-gpu': conda)"
  },
  "interpreter": {
   "hash": "b851d2923cfa3a2562599062e05fd9893d86a7c009c64d8ad3756552e4dd5f41"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}